<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.8.7">Jekyll</generator><link href="http://cppalliance.org/feed.xml" rel="self" type="application/atom+xml" /><link href="http://cppalliance.org/" rel="alternate" type="text/html" /><updated>2021-02-19T13:52:58+00:00</updated><id>http://cppalliance.org/feed.xml</id><title type="html">The C++ Alliance</title><subtitle>The C++ Alliance is dedicated to helping the C++ programming language evolve. We see it developing as an ecosystem of open source libraries and as a growing community of those who contribute to those libraries..</subtitle><entry><title type="html">Dmitry’s January Update</title><link href="http://cppalliance.org/dmitry/2021/02/15/dmitrys-january-update.html" rel="alternate" type="text/html" title="Dmitry’s January Update" /><published>2021-02-15T00:00:00+00:00</published><updated>2021-02-15T00:00:00+00:00</updated><id>http://cppalliance.org/dmitry/2021/02/15/dmitrys-january-update</id><content type="html" xml:base="http://cppalliance.org/dmitry/2021/02/15/dmitrys-january-update.html">&lt;h1 id=&quot;dmitrys-january-update&quot;&gt;Dmitry’s January Update&lt;/h1&gt;

&lt;p&gt;In January 2021 I started working on improving Boost.JSON for The C++ Alliance.
The time during this first month was mostly spent on getting familiar with the
project, particularly with its list of issues on GitHub.&lt;/p&gt;

&lt;p&gt;It turns out, half of the open issues are related to documentation. For
example, the section about conversions might need a rewrite, and related
entries in the reference need to provide more information. There should be more
examples for parsing customizations. There also needs to be a separate section
dedicated to library’s named requirements. There was also a bug in coversion
customization logic that was fixed by me this month.&lt;/p&gt;

&lt;p&gt;The next two large blocks are issues related to optimization opportunities and
dealing with floating point numbers (some issues are present in both groups).
The next group is issues related to build and continuous integration. A couple
of build bugs were fixed in January.&lt;/p&gt;

&lt;p&gt;The final group consists of feature requests, mostly for convenient ways to
access items inside &lt;code&gt;json::value&lt;/code&gt;. And this month I started implementing one
such feature – Json.Pointer support. The work is still in the early stages,
though.&lt;/p&gt;</content><author><name></name></author><category term="dmitry" /><summary type="html">Dmitry’s January Update In January 2021 I started working on improving Boost.JSON for The C++ Alliance. The time during this first month was mostly spent on getting familiar with the project, particularly with its list of issues on GitHub. It turns out, half of the open issues are related to documentation. For example, the section about conversions might need a rewrite, and related entries in the reference need to provide more information. There should be more examples for parsing customizations. There also needs to be a separate section dedicated to library’s named requirements. There was also a bug in coversion customization logic that was fixed by me this month. The next two large blocks are issues related to optimization opportunities and dealing with floating point numbers (some issues are present in both groups). The next group is issues related to build and continuous integration. A couple of build bugs were fixed in January. The final group consists of feature requests, mostly for convenient ways to access items inside json::value. And this month I started implementing one such feature – Json.Pointer support. The work is still in the early stages, though.</summary></entry><entry><title type="html">Richard’s January Update</title><link href="http://cppalliance.org/richard/2021/01/31/RichardsJanuaryUpdate.html" rel="alternate" type="text/html" title="Richard’s January Update" /><published>2021-01-31T00:00:00+00:00</published><updated>2021-01-31T00:00:00+00:00</updated><id>http://cppalliance.org/richard/2021/01/31/RichardsJanuaryUpdate</id><content type="html" xml:base="http://cppalliance.org/richard/2021/01/31/RichardsJanuaryUpdate.html">&lt;h1 id=&quot;a-year-in-the-c-alliance&quot;&gt;A year in the C++ Alliance&lt;/h1&gt;

&lt;p&gt;January marks one year since I joined the C++ Alliance and started maintaining the Boost.Beast library.&lt;/p&gt;

&lt;p&gt;It’s a pleasure to work with interesting and fun people who are passionate about writing C++, and in particular good 
C++.&lt;/p&gt;

&lt;p&gt;During the past year I have spent some time attending ISO WG21 meetings online as an alternate representative of the 
C++ Alliance. Prior to joining the organisation, during my life as a developer I always felt that the standards 
committee did the developer community a disservice. Without knowing much about the inner workings, it seemed to me that 
the committee lived in an Ivory Tower. So my intention was to see if there was a way to bring something useful to the table 
as a keen a prolific writer of C++ in the financial sector. In particular I had a personal interest in the 
standardisation of asynchronous networking, forked from the wonderful Asio library.&lt;/p&gt;

&lt;p&gt;I ended the year feeling no less jaded with the entire standards process, concluding that there is not much I can do to 
help.&lt;/p&gt;

&lt;p&gt;I feel it’s important to say that the committee is attended by very bright, passionate people who clearly enjoy the C++
language as much as I do.&lt;/p&gt;

&lt;p&gt;What I think does not work, at least from the point of view of delivering useful progress, is the process of committee 
itself. During my commercial life there has been one fundamental truth, which is that things go well when there is focus
of attention and the taking of personal responsibility. It seems to me that committees in general undermine these
important fundamentals. The upshot of this is that in my mind, C++ developers are not going to get the tools they need, 
in the timescales they need, if they wait for the slow grind of WG21’s wheels of stone.&lt;/p&gt;

&lt;p&gt;It is to me noteworthy that many of the libraries I actually use (fmt, spdlog, boost, jwt, openssl, and so on) have been 
in some way standardised or are in the process of being standardised, but always in a lesser form than the original, 
created by a small, passionate team of individuals who enjoyed autonomy and freedom of design.&lt;/p&gt;

&lt;p&gt;Even now, if a feature is available in the standard and as a 3rd party library, I will almost always choose the third 
party version. It will generally have more features and a design undamaged by a process that externalises costs.&lt;/p&gt;

&lt;p&gt;Which brings me back to my old C++ mantra, proven true for me over the past 15 years or so, &lt;em&gt;Boost is the 
Standard&lt;/em&gt;. Having said this, I must in fairness mention the wonderful fmtlib and spdlog libraries, and the gargantuan
Qt, without which a back-end developer like me would never be able to get anything to display on a screen in a cross-
platform manner.&lt;/p&gt;

&lt;p&gt;In the end I find myself in the same place I was a year ago: My view is that the only thing C++ needs is a credible 
dependency management tool. Given that, the developer community will produce and distribute all needed libraries, and 
the most popular will naturally become the standard ones.&lt;/p&gt;

&lt;h1 id=&quot;the-year-ahead&quot;&gt;The Year Ahead&lt;/h1&gt;

&lt;p&gt;Therefore, it is my intention this year to do what I can to bring more utility to the Boost ecosystem, where one 
person can make a useful impact on the lives of developers, and taking personal responsibility for libraries is the 
norm.&lt;/p&gt;

&lt;h2 id=&quot;the-big-three&quot;&gt;The Big Three&lt;/h2&gt;

&lt;p&gt;It is my view that there are a number of areas where common use cases have not been well served in Boost. These are of 
course JSON, Databases and HTTP clients.&lt;/p&gt;

&lt;h3 id=&quot;json&quot;&gt;JSON&lt;/h3&gt;

&lt;p&gt;At the end of 2020, Vinnie and the team finally brought a very credible JSON library to Boost, which I have used to
write some cryptocurrency exchange connectors. On the whole, it’s proven to be a very pleasant and 
intuitive API. In particular the methods to simultaneously query the presence of a value and return a pointer on success
with &lt;code&gt;if_contains&lt;/code&gt;, &lt;code&gt;if_string&lt;/code&gt; etc. have seen a lot of use and result in code that is readable and neat.&lt;/p&gt;

&lt;p&gt;Currently, Boost.JSON favours performance over accuracy with respect to parsing floating point numbers (sometimes a 
number is 1 ULP different to that which you’d expect from &lt;code&gt;std::strtod&lt;/code&gt;). I had a little look to see if there was 
a way to address this easily. It transpired not. Parsing decimal representations of floating point numbers into a binary
representation that will round-trip correctly is a hard problem, and beyond my level of mathematical skill. 
There is currently work underway to address this in the JSON repo.&lt;/p&gt;

&lt;p&gt;There is also work in progress to provide JSON-pointer lookups. This will be a welcome addition as it will mean I can
throw away my jury-rigged “path” code and use something robust.&lt;/p&gt;

&lt;h3 id=&quot;mysql&quot;&gt;MySQL&lt;/h3&gt;

&lt;p&gt;A very common database in use in the web world is of course MySQL. I have always found the official connectors somewhat
uninteresting, particularly as there is no asynchronous connector compatible with Asio.&lt;/p&gt;

&lt;p&gt;That was until a certain Rubén Pérez decided to write an asio-compatible mysql connector from scratch, using 
none of the code in the Oracle connector. Rubén has started the arduous journey of getting his 
&lt;a href=&quot;https://anarthal.github.io/boost-mysql/index.html&quot;&gt;library&lt;/a&gt; ready for Boost review.&lt;/p&gt;

&lt;p&gt;I have been asked to be the review manager for this work, something I am happy to do as, whether or not the admission is
ultimately accepted, I think the general principle of producing simple, self-contained libraries that meet a 
common requirement is to be encouraged.&lt;/p&gt;

&lt;p&gt;If this library is successful, I would hope that others will rise to the challenge and provide native connectors for
other common database systems.&lt;/p&gt;

&lt;h3 id=&quot;http-client&quot;&gt;HTTP Client&lt;/h3&gt;

&lt;p&gt;I mentioned in an earlier blog that an HTTP Client with a similar interface to Python’s Requests library woudl be worked 
on. As it happens, other priorities took over last year. This year I will be focussing efforts on getting this library 
in shape for a proof of concept.&lt;/p&gt;

&lt;h3 id=&quot;i-mean-big-four&quot;&gt;I mean Big Four&lt;/h3&gt;

&lt;p&gt;Redis is ubiquitous in server environments. A quick search for Redis C++ clients turned up this 
&lt;a href=&quot;https://github.com/basiliscos/cpp-bredis&quot;&gt;little gem&lt;/a&gt;. I’d like to find time to give this a try at some point.&lt;/p&gt;</content><author><name></name></author><category term="richard" /><summary type="html">A year in the C++ Alliance January marks one year since I joined the C++ Alliance and started maintaining the Boost.Beast library. It’s a pleasure to work with interesting and fun people who are passionate about writing C++, and in particular good C++. During the past year I have spent some time attending ISO WG21 meetings online as an alternate representative of the C++ Alliance. Prior to joining the organisation, during my life as a developer I always felt that the standards committee did the developer community a disservice. Without knowing much about the inner workings, it seemed to me that the committee lived in an Ivory Tower. So my intention was to see if there was a way to bring something useful to the table as a keen a prolific writer of C++ in the financial sector. In particular I had a personal interest in the standardisation of asynchronous networking, forked from the wonderful Asio library. I ended the year feeling no less jaded with the entire standards process, concluding that there is not much I can do to help. I feel it’s important to say that the committee is attended by very bright, passionate people who clearly enjoy the C++ language as much as I do. What I think does not work, at least from the point of view of delivering useful progress, is the process of committee itself. During my commercial life there has been one fundamental truth, which is that things go well when there is focus of attention and the taking of personal responsibility. It seems to me that committees in general undermine these important fundamentals. The upshot of this is that in my mind, C++ developers are not going to get the tools they need, in the timescales they need, if they wait for the slow grind of WG21’s wheels of stone. It is to me noteworthy that many of the libraries I actually use (fmt, spdlog, boost, jwt, openssl, and so on) have been in some way standardised or are in the process of being standardised, but always in a lesser form than the original, created by a small, passionate team of individuals who enjoyed autonomy and freedom of design. Even now, if a feature is available in the standard and as a 3rd party library, I will almost always choose the third party version. It will generally have more features and a design undamaged by a process that externalises costs. Which brings me back to my old C++ mantra, proven true for me over the past 15 years or so, Boost is the Standard. Having said this, I must in fairness mention the wonderful fmtlib and spdlog libraries, and the gargantuan Qt, without which a back-end developer like me would never be able to get anything to display on a screen in a cross- platform manner. In the end I find myself in the same place I was a year ago: My view is that the only thing C++ needs is a credible dependency management tool. Given that, the developer community will produce and distribute all needed libraries, and the most popular will naturally become the standard ones. The Year Ahead Therefore, it is my intention this year to do what I can to bring more utility to the Boost ecosystem, where one person can make a useful impact on the lives of developers, and taking personal responsibility for libraries is the norm. The Big Three It is my view that there are a number of areas where common use cases have not been well served in Boost. These are of course JSON, Databases and HTTP clients. JSON At the end of 2020, Vinnie and the team finally brought a very credible JSON library to Boost, which I have used to write some cryptocurrency exchange connectors. On the whole, it’s proven to be a very pleasant and intuitive API. In particular the methods to simultaneously query the presence of a value and return a pointer on success with if_contains, if_string etc. have seen a lot of use and result in code that is readable and neat. Currently, Boost.JSON favours performance over accuracy with respect to parsing floating point numbers (sometimes a number is 1 ULP different to that which you’d expect from std::strtod). I had a little look to see if there was a way to address this easily. It transpired not. Parsing decimal representations of floating point numbers into a binary representation that will round-trip correctly is a hard problem, and beyond my level of mathematical skill. There is currently work underway to address this in the JSON repo. There is also work in progress to provide JSON-pointer lookups. This will be a welcome addition as it will mean I can throw away my jury-rigged “path” code and use something robust. MySQL A very common database in use in the web world is of course MySQL. I have always found the official connectors somewhat uninteresting, particularly as there is no asynchronous connector compatible with Asio. That was until a certain Rubén Pérez decided to write an asio-compatible mysql connector from scratch, using none of the code in the Oracle connector. Rubén has started the arduous journey of getting his library ready for Boost review. I have been asked to be the review manager for this work, something I am happy to do as, whether or not the admission is ultimately accepted, I think the general principle of producing simple, self-contained libraries that meet a common requirement is to be encouraged. If this library is successful, I would hope that others will rise to the challenge and provide native connectors for other common database systems. HTTP Client I mentioned in an earlier blog that an HTTP Client with a similar interface to Python’s Requests library woudl be worked on. As it happens, other priorities took over last year. This year I will be focussing efforts on getting this library in shape for a proof of concept. I mean Big Four Redis is ubiquitous in server environments. A quick search for Redis C++ clients turned up this little gem. I’d like to find time to give this a try at some point.</summary></entry><entry><title type="html">Drone CI</title><link href="http://cppalliance.org/sam/2021/01/15/DroneCI.html" rel="alternate" type="text/html" title="Drone CI" /><published>2021-01-15T00:00:00+00:00</published><updated>2021-01-15T00:00:00+00:00</updated><id>http://cppalliance.org/sam/2021/01/15/DroneCI</id><content type="html" xml:base="http://cppalliance.org/sam/2021/01/15/DroneCI.html">&lt;h1 id=&quot;overview&quot;&gt;Overview&lt;/h1&gt;

&lt;p&gt;A message currently appears (mid-January 2021) at the top of the travis-ci.org website.&lt;/p&gt;

&lt;p&gt;“Please be aware travis-ci.org will be shutting down in several weeks, with all accounts migrating to travis-ci.com. Please stay tuned here for more information.”&lt;/p&gt;

&lt;p&gt;The transition has not been a smooth one, with long, disruptive delays occurring on existing builds, and lack of clear communication from the company. Many were unaware of the impending change. Some informative posts about the topic are &lt;a href=&quot;https://www.jeffgeerling.com/blog/2020/travis-cis-new-pricing-plan-threw-wrench-my-open-source-works&quot;&gt;Travis CI’s new pricing plan threw a wrench in my open source works&lt;/a&gt; and &lt;a href=&quot;https://travis-ci.community/t/extremely-poor-official-communication-of-the-org-shutdown/10568&quot;&gt;Extremely poor official communication of the .org shutdown&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;The C++ Alliance has decided to implement an in-house CI solution and make the new service available for Boost libraries also.&lt;/p&gt;

&lt;h1 id=&quot;selection-process&quot;&gt;Selection Process&lt;/h1&gt;

&lt;p&gt;The first step was choosing which software to use. There are truly a surprising number of alternatives. An extensive review was conducted, including many Continuous Integration services from &lt;a href=&quot;https://github.com/ligurio/awesome-ci&quot;&gt;awesome-ci&lt;/a&gt; which lists more than 50.  Coincidentally, Rene Rivera had recently done an analysis as well for &lt;a href=&quot;https://github.com/bfgroup/ci_playground&quot;&gt;ci_playground&lt;/a&gt;, and his example config files eventually became the basis for our new config files.&lt;/p&gt;

&lt;p&gt;The top choices:&lt;br /&gt;
Appveyor&lt;br /&gt;
Azure Pipelines&lt;br /&gt;
BuildBot&lt;br /&gt;
CircleCI&lt;br /&gt;
CirrusCI&lt;br /&gt;
Drone&lt;br /&gt;
Github Actions&lt;br /&gt;
Semaphore&lt;br /&gt;
Shippable&lt;br /&gt;
TeamCity&lt;/p&gt;

&lt;p&gt;From this list, Appveyor and Drone seemed the most promising to start with.  Both allow for 100% self-hosting.&lt;/p&gt;

&lt;h2 id=&quot;appveyor&quot;&gt;Appveyor&lt;/h2&gt;

&lt;p&gt;The appeal of Appveyor is that the config files are basic yaml, and everything runs in a Docker container. It sounds perfect. However, after experimentation there were a few issues.&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Appveyor was originally designed for Microsoft Windows, with .NET and Powershell being key ingredients. While it can run on Linux, it’s not a native Linux application.  Most of the CI testing done by Boost and CPPAlliance runs on Linux.&lt;/li&gt;
  &lt;li&gt;Specifically, the Docker experience on Windows is not nearly as smooth as Linux. I encountered numerous complexities when setting up Appveyor Windows Docker containers.&lt;/li&gt;
  &lt;li&gt;Bugs in their app.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Due to a combination of those reasons, Appveyor was not the best choice for this project.&lt;/p&gt;
&lt;h2 id=&quot;drone&quot;&gt;Drone&lt;/h2&gt;

&lt;p&gt;Within the first day or two experimenting with Drone, it became clear that this was an excellent CI framework:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Simple, usable UI&lt;/li&gt;
  &lt;li&gt;Easy installation&lt;/li&gt;
  &lt;li&gt;Linux and Docker native&lt;/li&gt;
  &lt;li&gt;Small number of processes to run&lt;/li&gt;
  &lt;li&gt;Integrates with PostgreSQL, MySQL, Amazon S3&lt;/li&gt;
  &lt;li&gt;Autoscale the agents&lt;/li&gt;
  &lt;li&gt;Badges on github repos&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;The main drawback with Drone is the absence of “matrix” builds. Their alternative for matrices is jsonnet or starlark, which are flexible scripting languages. I was apprehensive about this point, thinking that the end-user would prefer simple yaml files - exactly like Travis. And, in fact, that is probably the case. Basic yaml files are easier to understand. However, on balance, this was the only noticable problem with Drone, and everything else seemed to be in order. The resulting starlark files do have a matrix-like configuration where each job can be customized.&lt;/p&gt;

&lt;p&gt;To discuss &lt;a href=&quot;https://docs.bazel.build/versions/master/skylark/language.html&quot;&gt;Starlark&lt;/a&gt; for a moment - it’s a subset of python, and therefore easy to learn. &lt;a href=&quot;https://pypi.org/project/pystarlark/&quot;&gt;Why would I use Starlark instead of just Python?&lt;/a&gt; “Sandboxing. The primary reason this was written is for the “hermetic execution” feature of Starlark. Python is notoriously difficult to sandbox and there didn’t appear to be any sandboxing solutions that could run within Python to run Python or Python-like code. While Starlark isn’t exactly Python it is very very close to it. You can think of this as a secure way to run very simplistic Python functions. Note that this library itself doesn’t really provide any security guarantees and your program may crash while using it (PRs welcome). Starlark itself is providing the security guarantees.”&lt;/p&gt;

&lt;h3 id=&quot;running-a-drone-server&quot;&gt;Running a drone server&lt;/h3&gt;

&lt;p&gt;Create a script startdrone.sh with these contents:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;#!/bin/bash
  
docker run \
  --volume=/var/lib/drone:/data \
  --env=DRONE_GITHUB_CLIENT_ID= \
  --env=DRONE_GITHUB_CLIENT_SECRET= \
  --env=DRONE_RPC_SECRET= \
  --env=DRONE_TLS_AUTOCERT= \
  --env=DRONE_SERVER_HOST= \
  --env=DRONE_SERVER_PROTO= \
  --env=DRONE_CONVERT_PLUGIN_ENDPOINT= \
  --env=DRONE_CONVERT_PLUGIN_SECRET= \
  --env=DRONE_HTTP_SSL_REDIRECT= \
  --env=DRONE_HTTP_SSL_TEMPORARY_REDIRECT= \
  --env=DRONE_S3_BUCKET= \
  --env=DRONE_LOGS_PRETTY= \
  --env=AWS_ACCESS_KEY_ID= \
  --env=AWS_SECRET_ACCESS_KEY= \
  --env=AWS_DEFAULT_REGION= \
  --env=AWS_REGION= \
  --env=DRONE_DATABASE_DRIVER= \
  --env=DRONE_DATABASE_DATASOURCE= \
  --env=DRONE_USER_CREATE= \
  --env=DRONE_REPOSITORY_FILTER= \
  --env=DRONE_GITHUB_SCOPE= \
  --publish=80:80 \
  --publish=443:443 \
  --restart=always \
  --detach=true \
  --name=drone \
  drone/drone:1
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Fill in the variables. (Many of those are secure keys which shouldn’t be published on a public webpage.)&lt;br /&gt;
Then, run the script.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;./startdrone.sh  
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Drone is up and running.&lt;/p&gt;

&lt;p&gt;Next, the starlark plugin. Edit startstarlark.sh:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;#!/bin/bash
  
docker run -d \
  --volume=/var/lib/starlark:/data \
  --env= \
  --publish= \
  --env=DRONE_DEBUG= \
  --env=DRONE_SECRET= \
  --restart=always \
  --name=starlark drone/drone-convert-starlark
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;and run it:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;./startstarlark.sh
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Starlark is up and running. Finally, the autoscaler.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;#!/bin/bash
  
docker run -d \
  -v /var/lib/autoscaler:/data \
  -e DRONE_POOL_MIN= \
  -e DRONE_POOL_MAX= \
  -e DRONE_SERVER_PROTO= \
  -e DRONE_SERVER_HOST= \
  -e DRONE_SERVER_TOKEN= \
  -e DRONE_AGENT_TOKEN= \
  -e DRONE_AMAZON_REGION= \
  -e DRONE_AMAZON_SUBNET_ID= \
  -e DRONE_AMAZON_SECURITY_GROUP= \
  -e AWS_ACCESS_KEY_ID= \
  -e AWS_SECRET_ACCESS_KEY= \
  -e DRONE_CAPACITY_BUFFER= \
  -e DRONE_REAPER_INTERVAL= \
  -e DRONE_REAPER_ENABLED= \
  -e DRONE_ENABLE_REAPER= \
  -e DRONE_AMAZON_INSTANCE= \
  -e DRONE_AMAZON_VOLUME_TYPE= \
  -e DRONE_AMAZON_VOLUME_IOPS= \
  -p  \
  --restart=always \
  --name=autoscaler \
  drone/autoscaler
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Start the autoscaler.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;./startautoscaler.sh
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Windows autoscaler is still experimental. For now, both Windows and Mac servers have been installed manually and will be scaled individually. Because they are less common operating systems, with most boost builds running in Linux, the CPU load on these other machines is not as significant.&lt;/p&gt;

&lt;h1 id=&quot;configs&quot;&gt;Configs&lt;/h1&gt;

&lt;p&gt;The real complexities appear when composing the config files for each repository. After manually porting .travis.yml for &lt;a href=&quot;https://github.com/boostorg/beast&quot;&gt;https://github.com/boostorg/beast&lt;/a&gt; and &lt;a href=&quot;https://github.com/boostorg/json&quot;&gt;https://github.com/boostorg/json&lt;/a&gt;, the next step was creating a Python script which automates the entire process.&lt;/p&gt;

&lt;h2 id=&quot;drone-converter&quot;&gt;Drone Converter&lt;/h2&gt;

&lt;p&gt;A copy of the script can be viewed at &lt;a href=&quot;https://github.com/CPPAlliance/droneconverter-demo&quot;&gt;https://github.com/CPPAlliance/droneconverter-demo&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;The idea is to be able to go into any directory with a .travis.yml file, and migrate to Drone by executing a single command:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;cd boostorg/accumulators
droneconverter
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The converter ingests a source file, parses it with PyYAML, and dumps the output in Jinja2 templates. The method to write the script was by beginning with any library, such as &lt;a href=&quot;https://github.com/boostorg/array&quot;&gt;boostorg/array&lt;/a&gt;, and just get that one working. Then, move on to others, &lt;a href=&quot;https://github.com/boostorg/assign&quot;&gt;boostorg/assign&lt;/a&gt;,  &lt;a href=&quot;https://github.com/boostorg/bind&quot;&gt;boostorg/bind&lt;/a&gt;, etc. Each library contains a mix of travis jobs which are both similar and different to the previously translated libraries. Thus, each new travis file presents a new puzzle to solve, but hopefully in a generalized way that will also work for all repositories.&lt;/p&gt;

&lt;p&gt;Versions of &lt;a href=&quot;https://clang.llvm.org/&quot;&gt;clang&lt;/a&gt; ranging from 3.3 to 11 are targeted in the tests. While later releases such as clang-6 or clang-7 usually build right away without errors, the earlier versions in the 3.x series were failing to build for a variety of reasons. First of all, those versions are not available on Ubuntu 16.04 and later, which means being stuck on Ubuntu 14.04, preventing an across-the-board upgrade to a newer Ubuntu.  Then, if a standard “base” clang is simultaneously installed, such as clang-7 or 9, this seems to cause other dependent packages and libraries to be installed, which conflict with clang-3. The solution was to figure out what travis does, and copy it. Travis images have downloaded and installed clang-7 into a completely separate directory, not using the ordinary system packages. Then the extra directory /usr/local/clang-7.0.0/bin has been added to the $PATH.&lt;/p&gt;

&lt;p&gt;Some .travis.yml files have a “matrix” section. Others have “jobs”. Some .travis files place all the logic in one main “install” script. Others refer to a variety of script sections including “install”, “script”, “before_install”, “before_script”, “after_success”, which may or may not be shared between the 20 jobs contained in the file. Some job in travis were moving (with the mv command) their source directory, which is baffling and not usually permitted in Jenkins or Drone.  This must be converted to a copy command instead. “travis_wait” and “travis_retry” must be deleted, they are travis-specific.  Many travis variables such as TRAVIS_OS_NAME or TRAVIS_BRANCH need to be set and/or replaced in the resulting scripts. Environment variables in the travis file might be presented as a list, or a string, without quotes, or with single quotes, or double quotes, or single quotes embedded in double quotes, or double quotes embedded in single quotes, or missing entirely and included as a global env section at the top of the file.  The CXX variable, which determines the compiler used by boost build, isn’t always apparent and could be derived from a variety of sources, including the COMPILER variable or the list of packages. The list of these types of conversion fixes goes on and on, you can see them in the droneconverter program.&lt;/p&gt;

&lt;p&gt;Apple Macs Developer Tools depends on Xcode, with an array of possible Xcode version numbers from 6 to 12, and counting.  Catalina only runs 10 and above. High Sierra will run 7-9. None of these will operate without accepting the license agreement, however the license agreement might reset if switching to a newer version of Xcode. The presence of “Command Line Tools” seems to break some builds during testing, however everything works if “Command Line Tools” is missing and the build directly accesses Xcode in the /Applications/Xcode-11.7.app/Contents/Developer directory. On the other hand, the package manager “brew” needs “Command Line Tools” (on High Sierra) or it can’t install packages. A solution which appears to be sufficiently effective is to “mv CommandLineTools CommandLineTools.bck”, or the reverse, when needed.&lt;/p&gt;

&lt;p&gt;Drone will not start on a Mac during reboot unless the Drone user has a window console sessions running too. So, Apple is not an ideal command-line-only remote server environment.&lt;/p&gt;

&lt;h1 id=&quot;drone-deployments&quot;&gt;Drone Deployments&lt;/h1&gt;

&lt;p&gt;Pull requests with the new drone configs were rolled out to all those boost repositories with 100% travis build success rate and 100% drone build success rate, which accounts for about half of boost libraries. The other half of boost libraries are either missing a .travis.yml file, or they have a couple of travis/drone jobs which are failing. These should be addressed individually, even if only to post details about it in the pull requests. Ideally, attention should be focused on these failing tests, one by one, until the jobs attain a 100% build success rate and the badge displays green instead of red.  The &lt;a href=&quot;https://en.wikipedia.org/wiki/Long_tail&quot;&gt;long tail&lt;/a&gt; distribution of edge-cases require individual attention and rewritten tests.&lt;/p&gt;

&lt;h1 id=&quot;github-actions&quot;&gt;Github Actions&lt;/h1&gt;

&lt;p&gt;The droneconverter script is being enhanced to also generate Github Actions config files. A first draft, tested on a few boost libraries, is building Linux jobs successfully. Ongoing.&lt;/p&gt;</content><author><name></name></author><category term="sam" /><summary type="html">Overview A message currently appears (mid-January 2021) at the top of the travis-ci.org website. “Please be aware travis-ci.org will be shutting down in several weeks, with all accounts migrating to travis-ci.com. Please stay tuned here for more information.” The transition has not been a smooth one, with long, disruptive delays occurring on existing builds, and lack of clear communication from the company. Many were unaware of the impending change. Some informative posts about the topic are Travis CI’s new pricing plan threw a wrench in my open source works and Extremely poor official communication of the .org shutdown. The C++ Alliance has decided to implement an in-house CI solution and make the new service available for Boost libraries also. Selection Process The first step was choosing which software to use. There are truly a surprising number of alternatives. An extensive review was conducted, including many Continuous Integration services from awesome-ci which lists more than 50. Coincidentally, Rene Rivera had recently done an analysis as well for ci_playground, and his example config files eventually became the basis for our new config files. The top choices: Appveyor Azure Pipelines BuildBot CircleCI CirrusCI Drone Github Actions Semaphore Shippable TeamCity From this list, Appveyor and Drone seemed the most promising to start with. Both allow for 100% self-hosting. Appveyor The appeal of Appveyor is that the config files are basic yaml, and everything runs in a Docker container. It sounds perfect. However, after experimentation there were a few issues. Appveyor was originally designed for Microsoft Windows, with .NET and Powershell being key ingredients. While it can run on Linux, it’s not a native Linux application. Most of the CI testing done by Boost and CPPAlliance runs on Linux. Specifically, the Docker experience on Windows is not nearly as smooth as Linux. I encountered numerous complexities when setting up Appveyor Windows Docker containers. Bugs in their app. Due to a combination of those reasons, Appveyor was not the best choice for this project. Drone Within the first day or two experimenting with Drone, it became clear that this was an excellent CI framework: Simple, usable UI Easy installation Linux and Docker native Small number of processes to run Integrates with PostgreSQL, MySQL, Amazon S3 Autoscale the agents Badges on github repos The main drawback with Drone is the absence of “matrix” builds. Their alternative for matrices is jsonnet or starlark, which are flexible scripting languages. I was apprehensive about this point, thinking that the end-user would prefer simple yaml files - exactly like Travis. And, in fact, that is probably the case. Basic yaml files are easier to understand. However, on balance, this was the only noticable problem with Drone, and everything else seemed to be in order. The resulting starlark files do have a matrix-like configuration where each job can be customized. To discuss Starlark for a moment - it’s a subset of python, and therefore easy to learn. Why would I use Starlark instead of just Python? “Sandboxing. The primary reason this was written is for the “hermetic execution” feature of Starlark. Python is notoriously difficult to sandbox and there didn’t appear to be any sandboxing solutions that could run within Python to run Python or Python-like code. While Starlark isn’t exactly Python it is very very close to it. You can think of this as a secure way to run very simplistic Python functions. Note that this library itself doesn’t really provide any security guarantees and your program may crash while using it (PRs welcome). Starlark itself is providing the security guarantees.” Running a drone server Create a script startdrone.sh with these contents: #!/bin/bash docker run \ --volume=/var/lib/drone:/data \ --env=DRONE_GITHUB_CLIENT_ID= \ --env=DRONE_GITHUB_CLIENT_SECRET= \ --env=DRONE_RPC_SECRET= \ --env=DRONE_TLS_AUTOCERT= \ --env=DRONE_SERVER_HOST= \ --env=DRONE_SERVER_PROTO= \ --env=DRONE_CONVERT_PLUGIN_ENDPOINT= \ --env=DRONE_CONVERT_PLUGIN_SECRET= \ --env=DRONE_HTTP_SSL_REDIRECT= \ --env=DRONE_HTTP_SSL_TEMPORARY_REDIRECT= \ --env=DRONE_S3_BUCKET= \ --env=DRONE_LOGS_PRETTY= \ --env=AWS_ACCESS_KEY_ID= \ --env=AWS_SECRET_ACCESS_KEY= \ --env=AWS_DEFAULT_REGION= \ --env=AWS_REGION= \ --env=DRONE_DATABASE_DRIVER= \ --env=DRONE_DATABASE_DATASOURCE= \ --env=DRONE_USER_CREATE= \ --env=DRONE_REPOSITORY_FILTER= \ --env=DRONE_GITHUB_SCOPE= \ --publish=80:80 \ --publish=443:443 \ --restart=always \ --detach=true \ --name=drone \ drone/drone:1 Fill in the variables. (Many of those are secure keys which shouldn’t be published on a public webpage.) Then, run the script. ./startdrone.sh Drone is up and running. Next, the starlark plugin. Edit startstarlark.sh: #!/bin/bash docker run -d \ --volume=/var/lib/starlark:/data \ --env= \ --publish= \ --env=DRONE_DEBUG= \ --env=DRONE_SECRET= \ --restart=always \ --name=starlark drone/drone-convert-starlark and run it: ./startstarlark.sh Starlark is up and running. Finally, the autoscaler. #!/bin/bash docker run -d \ -v /var/lib/autoscaler:/data \ -e DRONE_POOL_MIN= \ -e DRONE_POOL_MAX= \ -e DRONE_SERVER_PROTO= \ -e DRONE_SERVER_HOST= \ -e DRONE_SERVER_TOKEN= \ -e DRONE_AGENT_TOKEN= \ -e DRONE_AMAZON_REGION= \ -e DRONE_AMAZON_SUBNET_ID= \ -e DRONE_AMAZON_SECURITY_GROUP= \ -e AWS_ACCESS_KEY_ID= \ -e AWS_SECRET_ACCESS_KEY= \ -e DRONE_CAPACITY_BUFFER= \ -e DRONE_REAPER_INTERVAL= \ -e DRONE_REAPER_ENABLED= \ -e DRONE_ENABLE_REAPER= \ -e DRONE_AMAZON_INSTANCE= \ -e DRONE_AMAZON_VOLUME_TYPE= \ -e DRONE_AMAZON_VOLUME_IOPS= \ -p \ --restart=always \ --name=autoscaler \ drone/autoscaler Start the autoscaler. ./startautoscaler.sh Windows autoscaler is still experimental. For now, both Windows and Mac servers have been installed manually and will be scaled individually. Because they are less common operating systems, with most boost builds running in Linux, the CPU load on these other machines is not as significant. Configs The real complexities appear when composing the config files for each repository. After manually porting .travis.yml for https://github.com/boostorg/beast and https://github.com/boostorg/json, the next step was creating a Python script which automates the entire process. Drone Converter A copy of the script can be viewed at https://github.com/CPPAlliance/droneconverter-demo The idea is to be able to go into any directory with a .travis.yml file, and migrate to Drone by executing a single command: cd boostorg/accumulators droneconverter The converter ingests a source file, parses it with PyYAML, and dumps the output in Jinja2 templates. The method to write the script was by beginning with any library, such as boostorg/array, and just get that one working. Then, move on to others, boostorg/assign, boostorg/bind, etc. Each library contains a mix of travis jobs which are both similar and different to the previously translated libraries. Thus, each new travis file presents a new puzzle to solve, but hopefully in a generalized way that will also work for all repositories. Versions of clang ranging from 3.3 to 11 are targeted in the tests. While later releases such as clang-6 or clang-7 usually build right away without errors, the earlier versions in the 3.x series were failing to build for a variety of reasons. First of all, those versions are not available on Ubuntu 16.04 and later, which means being stuck on Ubuntu 14.04, preventing an across-the-board upgrade to a newer Ubuntu. Then, if a standard “base” clang is simultaneously installed, such as clang-7 or 9, this seems to cause other dependent packages and libraries to be installed, which conflict with clang-3. The solution was to figure out what travis does, and copy it. Travis images have downloaded and installed clang-7 into a completely separate directory, not using the ordinary system packages. Then the extra directory /usr/local/clang-7.0.0/bin has been added to the $PATH. Some .travis.yml files have a “matrix” section. Others have “jobs”. Some .travis files place all the logic in one main “install” script. Others refer to a variety of script sections including “install”, “script”, “before_install”, “before_script”, “after_success”, which may or may not be shared between the 20 jobs contained in the file. Some job in travis were moving (with the mv command) their source directory, which is baffling and not usually permitted in Jenkins or Drone. This must be converted to a copy command instead. “travis_wait” and “travis_retry” must be deleted, they are travis-specific. Many travis variables such as TRAVIS_OS_NAME or TRAVIS_BRANCH need to be set and/or replaced in the resulting scripts. Environment variables in the travis file might be presented as a list, or a string, without quotes, or with single quotes, or double quotes, or single quotes embedded in double quotes, or double quotes embedded in single quotes, or missing entirely and included as a global env section at the top of the file. The CXX variable, which determines the compiler used by boost build, isn’t always apparent and could be derived from a variety of sources, including the COMPILER variable or the list of packages. The list of these types of conversion fixes goes on and on, you can see them in the droneconverter program. Apple Macs Developer Tools depends on Xcode, with an array of possible Xcode version numbers from 6 to 12, and counting. Catalina only runs 10 and above. High Sierra will run 7-9. None of these will operate without accepting the license agreement, however the license agreement might reset if switching to a newer version of Xcode. The presence of “Command Line Tools” seems to break some builds during testing, however everything works if “Command Line Tools” is missing and the build directly accesses Xcode in the /Applications/Xcode-11.7.app/Contents/Developer directory. On the other hand, the package manager “brew” needs “Command Line Tools” (on High Sierra) or it can’t install packages. A solution which appears to be sufficiently effective is to “mv CommandLineTools CommandLineTools.bck”, or the reverse, when needed. Drone will not start on a Mac during reboot unless the Drone user has a window console sessions running too. So, Apple is not an ideal command-line-only remote server environment. Drone Deployments Pull requests with the new drone configs were rolled out to all those boost repositories with 100% travis build success rate and 100% drone build success rate, which accounts for about half of boost libraries. The other half of boost libraries are either missing a .travis.yml file, or they have a couple of travis/drone jobs which are failing. These should be addressed individually, even if only to post details about it in the pull requests. Ideally, attention should be focused on these failing tests, one by one, until the jobs attain a 100% build success rate and the badge displays green instead of red. The long tail distribution of edge-cases require individual attention and rewritten tests. Github Actions The droneconverter script is being enhanced to also generate Github Actions config files. A first draft, tested on a few boost libraries, is building Linux jobs successfully. Ongoing.</summary></entry><entry><title type="html">Richard’s New Year Update - Reusable HTTP Connections</title><link href="http://cppalliance.org/richard/2021/01/01/RichardsNewYearUpdate.html" rel="alternate" type="text/html" title="Richard’s New Year Update - Reusable HTTP Connections" /><published>2021-01-01T00:00:00+00:00</published><updated>2021-01-01T00:00:00+00:00</updated><id>http://cppalliance.org/richard/2021/01/01/RichardsNewYearUpdate</id><content type="html" xml:base="http://cppalliance.org/richard/2021/01/01/RichardsNewYearUpdate.html">&lt;h1 id=&quot;reusable-https-connections&quot;&gt;Reusable HTTP(S) Connections&lt;/h1&gt;

&lt;p&gt;Something I am often asked by users of Boost Beast is how to code a client which effectively re-uses a pool of HTTP 
connections, in the same way a web browser does.&lt;/p&gt;

&lt;p&gt;The premise is straightforward - if our client is going to be making multiple calls to a web server (or several of them)
then it makes sense that once a connection has been used for one request, it is returned to a connection pool so that 
a subsequent request can make use of it.&lt;/p&gt;

&lt;p&gt;It also makes sense to have a limit on the number of concurrent connections that can be open against any one host.
Otherwise, if the client needs to make multiple requests at the same time, it will end up creating new connections in
parallel and lose the efficiency of re-using an existing connection.&lt;/p&gt;

&lt;p&gt;From these requirements, we can start to think about a design.&lt;/p&gt;

&lt;p&gt;Firstly, we can imagine a connection cache, with connections kept in a map keyed on host + scheme + port (we can’t 
re-use an HTTP port for an HTTPS request!).&lt;/p&gt;

&lt;p&gt;When a request needs a connection, it will either create a new one (connection limit per host not met) or will wait
for an existing connection to become available (which implies a condition variable).&lt;/p&gt;

&lt;p&gt;Once a request has a connection to use, it will send the HTTP request and wait for the response.&lt;/p&gt;

&lt;p&gt;At this stage, there is a possibility that the active connection which has been allocated could have been idle since 
the last time it was used. In TCP there is no way to atomically check whether the remote host has closed the 
connection (or died). The only way to know is to actually read from the socket with a timeout. If the remote host has 
shutdown the socket, we will be notified as soon as the RST frame arrives at our host. If the remote host has stopped
working or the network is bad, we’ll be notified by the timeout.&lt;/p&gt;

&lt;p&gt;Thus if our read operation results in an error and we have inherited the connection from the cache, we ought to re-try
by reopening the connection to the remote host and repeating the write/read operations. However, if there is an error
reported on the subsequent attempt, then we can conclude that this is a legitimate error to be reported back to the
caller.&lt;/p&gt;

&lt;p&gt;In simplified pesudocode, the operation might look something like this (assuming we report transport errors as 
exceptions):&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-cpp&quot;&gt;response 
read_write(connection&amp;amp; conn)
{
  response resp;
  
  auto retry = false;
   
  if(conn.is_initialised())
    retry = true;
  else
    conn.connect(...);
  
  for(;;)
  {
    conn.write(request);
    auto err = conn.read(resp);
    if (err)
    {
      if(!std::exchange(retry, false))
        throw system_error(err);
      request.clear();
      conn.disconnect();
    }
    else
      break;
  }

  return resp;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&quot;structuring-the-code&quot;&gt;Structuring the Code&lt;/h1&gt;

&lt;h2 id=&quot;general-principles&quot;&gt;General Principles&lt;/h2&gt;

&lt;p&gt;In my previous &lt;a href=&quot;https://cppalliance.org/richard/2020/12/22/RichardsDecemberUpdate.html&quot;&gt;blog&lt;/a&gt; I mentioned my preference
for writing the implementation of a class in such a way that it does not need to take care of its own lifetime or
mutual exclusion. These concerns are deferred to a wrapper or handle. Methods on the handle class take care of 
marshalling the call to the correct executor (or thread) and preserving the implementation’s lifetime. The 
implementation need only concern itself with the logic of handling the request.&lt;/p&gt;

&lt;p&gt;Here’s an example about which I’ll go into more detail later:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-cpp&quot;&gt;net::awaitable&amp;lt; response_type &amp;gt;
connection_cache::call(beast::http::verb   method,
                       const std::string &amp;amp; url,
                       std::string         data,
                       beast::http::fields headers,
                       request_options     options)
{
    // DRY - define an operation that performs the inner call.
    auto op = [&amp;amp;] {
        return impl_-&amp;gt;call(method,
                           url,
                           std::move(data),
                           std::move(headers),
                           std::move(options));
    };

    // deduce the current executor
    auto my_executor = co_await net::this_coro::executor;

    // either call directly or via a spawned coroutine
    co_return impl_-&amp;gt;get_executor() != my_executor
        ? co_await op()
        : co_await net::co_spawn(impl_-&amp;gt;get_executor(), op, net::use_awaitable);
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;coroutines&quot;&gt;Coroutines&lt;/h2&gt;

&lt;p&gt;In this implementation I will be providing a C++20 coroutine interface over Asio executors once again. I am using 
coroutines because they are easier to write when compared to Asio’s composed operations, but are fundamentally the 
same thing in a prettier package.&lt;/p&gt;

&lt;h2 id=&quot;mutual-exclusion&quot;&gt;Mutual Exclusion&lt;/h2&gt;

&lt;p&gt;For mutual exclusion I will be embedded an asio strand into each active object. The advantage of doing so means that no
thread of execution is ever blocked which means we can limit the number of threads in the program to the number of 
free CPUs giving us maximum throughput of work. In reality of course, one thread is more than enough computing power
for almost all asynchronous programs. It’s therefore better to think in terms of one &lt;em&gt;executor&lt;/em&gt; per program component,
with the implicit guarantee that a given executor will only perform work on one thread at a time.
Thinking this way allows us to write code in a way that is agnostic of whether the final program is compiled to be 
single or multi-threaded.&lt;/p&gt;

&lt;h2 id=&quot;but-what-about-single-threaded-programs&quot;&gt;But What About Single-threaded Programs?&lt;/h2&gt;

&lt;p&gt;In order that I don’t need to rewrite code when should I decide to make a single-threaded program multi-threaded or vice 
versa, I have a couple of little utility functions and types defined in 
&lt;a href=&quot;https://github.com/madmongo1/blog-new-year-2021/blob/master/src/config.hpp&quot;&gt;config.hpp&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Specifically:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-cpp&quot;&gt;namespace net
{
using namespace asio;

using io_executor = io_context::executor_type;

#ifdef MULTI_THREADED

using io_strand = strand&amp;lt; io_executor &amp;gt;;

inline io_strand
new_strand(io_executor const &amp;amp;src);

inline io_strand
new_strand(io_strand const &amp;amp;src);

#else

using io_strand = io_context::executor_type;

inline io_strand
new_strand(io_executor const &amp;amp;src);

#endif

inline io_executor
to_io_executor(any_io_executor const &amp;amp;src);
}   // namespace net
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Any object type in the program which &lt;em&gt;would require its own strand&lt;/em&gt; in a multi-threaded program will simply use the type
&lt;code&gt;io_strand&lt;/code&gt; whether the program is compiled for single-threaded operation or not. Any code that would notionally 
need to construct a new strand simply calls &lt;code&gt;new_strand(e)&lt;/code&gt; where &lt;code&gt;e&lt;/code&gt; is either a strand or a naked executor.&lt;/p&gt;

&lt;p&gt;Any code that needs access to the notional underlying executor would call &lt;code&gt;to_io_executor(e)&lt;/code&gt;.&lt;/p&gt;

&lt;h2 id=&quot;determinism&quot;&gt;Determinism&lt;/h2&gt;

&lt;p&gt;Since we’re using asio’s executors for scheduling, it means that we can use asio’s timer as a deterministic, ordered 
asynchronous condition variable, which means that requests waiting on the connection pool will be offered free 
connections in the order that they were requested. This guarantee is implicit in the way that the timers’ &lt;code&gt;cancel_one()&lt;/code&gt;
method is specified.&lt;/p&gt;

&lt;p&gt;As we’ll see later, asio’s timers also make it trivial to implement an asynchronous semaphore. In this case we use one
to ensure that requests are handled concurrently but no more than some upper limit at any one time.&lt;/p&gt;

&lt;h2 id=&quot;interface&quot;&gt;Interface&lt;/h2&gt;

&lt;p&gt;I’m going to create a high-level concept called a &lt;code&gt;connection_cache&lt;/code&gt;. The interface will be something like this:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-cpp&quot;&gt;struct connection_cache
{
  using response_ptr = std::unique_ptr&amp;lt; response &amp;gt;;
  
  net::awaitable&amp;lt; response_ptr &amp;gt;
  rest_call(
    verb method, 
    std::string const&amp;amp; url, 
    std::optional&amp;lt;std::string&amp;gt; data = std::nullopt,
    headers hdrs = {},
    options opts = {});
};
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;There are a few things to note here.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;The return type of the coroutine is a unique_ptr to a response. A natural question might be whether the response 
should simply be returned by value. However, in practice I have found that there are a number of practical reasons
why it’s often better to return the response as a pointer. Firstly it allows conversion to a 
&lt;code&gt;shared_ptr&amp;lt;const response&amp;gt;&lt;/code&gt; in environments where the response might be passed through a directed acyclic graph.
Secondly, would allow a further enhancement in that having finished with the response, the client could post it back
to the cache, meaning that it could be cached for re-use.&lt;/li&gt;
  &lt;li&gt;The only two required arguments are the method and url. All others can be defaulted.&lt;/li&gt;
  &lt;li&gt;An optional string may be passed which contains the payload of a POST request. This is passed by value because, 
as we’ll see later,the implementation will want to move this into the request object prior to initiating 
communications. I have chosen a string type for two reasons
    &lt;ul&gt;
      &lt;li&gt;text in the form of JSON or XML is the most common form of messaging in REST calls.&lt;/li&gt;
      &lt;li&gt;strings can contain binary data with no loss of efficiency.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;code&gt;hdrs&lt;/code&gt; is simply a list of headers which should be set in the request. Again, these are passed by value as they will
be moved into the request object.&lt;/li&gt;
  &lt;li&gt;The last parameter of the call is an as-yet undefined type called &lt;code&gt;options&lt;/code&gt;. This will allow us to add niceties like
timeout arguments, a stop_token, redirect policies, a reference to a cookie store and so on.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;When called, &lt;code&gt;rest_call&lt;/code&gt; will attempt to reuse an existing connection. If a connection is not available, it will create
a new one if we are under the connection threshold for the deduced host and if not, it will wait until a connection is
available.&lt;/p&gt;

&lt;p&gt;Furthermore, the number of concurrent requests will be throttled to some upper limit.&lt;/p&gt;

&lt;p&gt;Transport failures will be reported as an exception (of type &lt;code&gt;system_error&lt;/code&gt;) and a successful response (even if a 400 
or 500) will be returned in the &lt;code&gt;response_ptr&lt;/code&gt;. That is to say, as long as the transport layer works out, the code will 
take the non-exceptional path.&lt;/p&gt;

&lt;h2 id=&quot;implementation-details&quot;&gt;Implementation details&lt;/h2&gt;

&lt;h3 id=&quot;url-parsing&quot;&gt;URL Parsing&lt;/h3&gt;

&lt;p&gt;Among the things I am often asked about in the Beast slack channel and in the 
&lt;a href=&quot;https://github.com/boostorg/beast/issues&quot;&gt;Issue Tracker&lt;/a&gt; is why there is no URL support in Beast. 
The is that Beast is a reasonably low level library that concerns itself with the HTTP (and WebSocket) 
protocols, plus as much buffer and stream management as is necessary to implement HTTP over Asio.
The concept of a URL the subject of its own RFCs and is a higher level concern.
The C++ Alliance is working on &lt;a href=&quot;https://github.com/CPPAlliance/url&quot;&gt;Boost.URL&lt;/a&gt; but it is not ready for publishing yet.
In the meantime, I found a nifty regex on the internet that more-or-less suffices for our needs:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-cpp&quot;&gt;    std::tuple&amp;lt; connection_key, std::string &amp;gt;
    parse_url(std::string const &amp;amp;url)
    {
        static const auto url_regex = std::regex(
            R&quot;regex((http|https)://([^/ :]+):?([^/ ]*)((/?[^ #?]*)\x3f?([^ #]*)#?([^ ]*)))regex&quot;,
            std::regex_constants::icase | std::regex_constants::optimize);
        auto match = std::smatch();
        if (not std::regex_match(url, match, url_regex))
            throw system_error(net::error::invalid_argument, &quot;invalid url&quot;);

        auto &amp;amp;protocol = match[1];
        auto &amp;amp;host     = match[2];
        auto &amp;amp;port_ind = match[3];
        auto &amp;amp;target   = match[4];
        /*
        auto &amp;amp;path     = match[5];
        auto &amp;amp;query    = match[6];
        auto &amp;amp;fragment = match[7];
        */
        return std::make_tuple(
            connection_key { .hostname = host.str(),
                             .port     = deduce_port(protocol, port_ind),
                             .scheme   = deduce_scheme(protocol, port_ind) },
            target.str());
    }
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&quot;exceptions-in-asynchronous-code-considered-harmful&quot;&gt;Exceptions in Asynchronous Code Considered Harmful&lt;/h3&gt;

&lt;p&gt;I hate to admit this, because I am a huge fan of propagating errors as exceptions. This is because the combination of 
RAII and exception behaviour handling makes error handling very slick in C++. However, coroutines have two rather 
unpleasant limitations:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;You can’t call a coroutine in a destructor.&lt;/li&gt;
  &lt;li&gt;You can’t call a coroutine in an exception handling block.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;There are workarounds. Consider:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-cpp&quot;&gt;my_component::~my_component()
{
  // destructor
  net::co_spawn(get_executor(), 
  [impl = impl_]()-&amp;gt;net::awaitable&amp;lt;void&amp;gt;
  {
    co_await impl-&amp;gt;shutdown();
    // destructor of *impl happens here
  }, net::detached);
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This is the destructor of a wrapper object that contains a &lt;code&gt;shared_ptr&lt;/code&gt; to its implementation, &lt;code&gt;impl_&lt;/code&gt;. In this case
we can detect the destruction of &lt;code&gt;my_component&lt;/code&gt; and use this to spawn a new coroutine of indeterminate lifetime that
takes care of shutting down the actual implementation and then destroying it.&lt;/p&gt;

&lt;p&gt;This solves the problem of RAII but it mandates that we must author objects that will be used in coroutines as 
a handle-body pair.&lt;/p&gt;

&lt;p&gt;We can similarly get around the “no coroutine calls in exception handlers” limitation if we’re prepared to stomach code 
like this:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-cpp&quot;&gt;net::awaitable&amp;lt;void&amp;gt; 
my_coro()
{
  std::function&amp;lt;net::awaitable(void)&amp;gt; on_error;
  try {
    co_await something();
  }
  catch(...)
  {
    // set up error_handler
    on_error = [ep = std::current_exception] {
      return handler_error_coro(ep);
    };
  }
  // perhaps handle the error here
  if (on_error)
      co_await on_error();
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;I think you’ll agree that this is a revolting solution to an unforgivable omission in the language. Not only is it 
untidy, confusing, difficult to teach and error-prone, it also turns exception handling into same fiasco that is 
enforced checking of return codes.&lt;/p&gt;

&lt;p&gt;To add insult to injury, the error handling code in this function takes up 5 times as many lines as the logic!&lt;/p&gt;

&lt;p&gt;Therefore my recommendation is that in asynchronous coroutine code, it’s better to avoid exceptions and have coroutines
either return a tuple of (error_code, possible_value) or a variant containing error-or-value.&lt;/p&gt;

&lt;p&gt;For example, here is some code from the &lt;code&gt;connection_impl&lt;/code&gt; in my example project:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-cpp&quot;&gt;net::awaitable&amp;lt; std::tuple&amp;lt; error_code, response_type &amp;gt; &amp;gt;
connection_impl::rest_call(request_class const &amp;amp;  request,
                           request_options const &amp;amp;options)
{
    auto response = std::make_unique&amp;lt; response_class &amp;gt;();

    auto ec = stream_.is_open()
                  ? co_await rest_call(request, *response, options)
                  : net::error::basic_errors::not_connected;

    if (ec &amp;amp;&amp;amp; ec != net::error::operation_aborted)
    {
        ec = co_await connect(options.stop);
        if (!ec)
            ec = co_await rest_call(request, *response, options);
    }

    if (ec || response-&amp;gt;need_eof())
        stream_.close();

    co_return std::make_tuple(ec, std::move(response));
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&quot;ensuring-that-a-coroutine-executes-on-the-correct-executor&quot;&gt;Ensuring that a Coroutine Executes on the Correct Executor&lt;/h3&gt;

&lt;p&gt;In playing with asio coroutines I stumbled upon something that has become an idiom.&lt;/p&gt;

&lt;p&gt;Consider the situation where a &lt;code&gt;connection&lt;/code&gt; class is implemented in terms of a handle and body. The body contains its 
own executor. In a multi-threaded build, this executor will be a &lt;code&gt;strand&lt;/code&gt; while in a single threaded-build we would want 
it to be simply an &lt;code&gt;io_context::executor_type&lt;/code&gt; since there will be no need for any of the thread guards implicit in a 
strand.&lt;/p&gt;

&lt;p&gt;Now consider that the implementation has a member coroutine called (say) &lt;code&gt;call&lt;/code&gt;. There are two scenarios in which 
this member will be called. The first is where the caller is executing in the same executor that is associated with 
the implementation, the second is where the caller is in its own different executor.
In the latter case, we must &lt;code&gt;post&lt;/code&gt; or &lt;code&gt;spawn&lt;/code&gt; the execution of the coroutine onto the implementation’s executor in order 
to ensure that it runs in the correct sequence with respect to other coroutines initiated against it.&lt;/p&gt;

&lt;p&gt;The idiom that occurred to me originally was to recursively spawn a coroutine to ensure the call happened on the 
correct executor:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-cpp&quot;&gt;net::awaitable&amp;lt; response_type &amp;gt;
connection_cache::call(beast::http::verb   method,
                       const std::string &amp;amp; url,
                       std::string         data,
                       beast::http::fields headers,
                       request_options     options)
{
    auto my_executor = co_await net::this_coro::executor;

    if (impl_-&amp;gt;get_executor() == my_executor)
    {
        co_return co_await impl_-&amp;gt;call(method,
                                       url,
                                       std::move(data),
                                       std::move(headers),
                                       std::move(options));
    }
    else
    {
        // spawn a coroutine which recurses on the correct executor.
        // wait for this coroutine to finish
        co_return co_await net::co_spawn(
            impl_-&amp;gt;get_executor(),
            [&amp;amp;]() -&amp;gt; net::awaitable&amp;lt; response_type &amp;gt; {
                return call(method,
                            url,
                            std::move(data),
                            std::move(headers),
                            std::move(options));
            },
            net::use_awaitable);
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;However, this does have the drawback that a code analyser might see the possibility of infinite recursion.&lt;/p&gt;

&lt;p&gt;After discussing this with &lt;a href=&quot;https://github.com/chriskohlhoff/asio/&quot;&gt;Chris&lt;/a&gt;, Asio’s author, a better solution was found:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-cpp&quot;&gt;net::awaitable&amp;lt; response_type &amp;gt;
connection_cache::call(beast::http::verb   method,
                       const std::string &amp;amp; url,
                       std::string         data,
                       beast::http::fields headers,
                       request_options     options)
{
    // DRY - define an operation that performs the inner call.
    auto op = [&amp;amp;] {
        return impl_-&amp;gt;call(method,
                           url,
                           std::move(data),
                           std::move(headers),
                           std::move(options));
    };

    // deduce the current executor
    auto my_executor = co_await net::this_coro::executor;

    // either call directly or via a spawned coroutine
    co_return impl_-&amp;gt;get_executor() != my_executor
        ? co_await op()
        : co_await net::co_spawn(impl_-&amp;gt;get_executor(), op, net::use_awaitable);
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;There are a few things to note:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;All lifetimes are correct even though the &lt;code&gt;op&lt;/code&gt; takes arguments by reference. This is because whichever path we take, 
our outer coroutine will suspend on the call to &lt;code&gt;op&lt;/code&gt;.&lt;/li&gt;
  &lt;li&gt;Note that in the slow path, &lt;code&gt;op&lt;/code&gt; is captured by value in the call to &lt;code&gt;co_spawn&lt;/code&gt;. Had we written:
&lt;code&gt;: co_await net::co_spawn(impl_-&amp;gt;get_executor(), op(), net::use_awaitable);&lt;/code&gt; then &lt;code&gt;op&lt;/code&gt; would have been called &lt;em&gt;during&lt;/em&gt;
the setup of the call to &lt;code&gt;co_spawn&lt;/code&gt;, which would result in &lt;code&gt;impl_-&amp;gt;call(...)&lt;/code&gt; being called on the wrong 
executor/thread.&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;talk-is-cheap&quot;&gt;Talk is Cheap&lt;/h1&gt;

&lt;p&gt;TL;DR. Enough talk, where’s the code?&lt;/p&gt;

&lt;p&gt;It’s on github at &lt;a href=&quot;https://github.com/madmongo1/blog-new-year-2021&quot;&gt;https://github.com/madmongo1/blog-new-year-2021&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;final-notes&quot;&gt;Final Notes.&lt;/h2&gt;

&lt;p&gt;Before signing off I just wanted to cover a few of the features I have completed in this example and a few that I 
have not.&lt;/p&gt;

&lt;h3 id=&quot;whats-done&quot;&gt;What’s Done&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;There is a limit on the number of concurrent connections to a single host. Host here is defined as a tuple of the 
transport scheme (i.e. http or https), port and hostname. This is currently defaulted to two, but would be trivial to 
change.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;There is a limit on the number of concurrent requests across all hosts. This defaults to 1000. Simultaneous requests
numbering more than this will be processed through what is an asynchronous semaphore, implemented like this:&lt;/p&gt;
    &lt;pre&gt;&lt;code class=&quot;language-cpp&quot;&gt;  while (request_count_ &amp;gt;= max_concurrent_requests_)
  {
      error_code ec;
      co_await concurrent_requests_available_.async_wait(
          net::redirect_error(net::use_awaitable, ec));
  }

  ++request_count_;

  ...
  ... request takes place here
  ...
  
  if (--request_count_ &amp;lt; max_concurrent_requests_)
  {
      concurrent_requests_available_.cancel_one();
  }

&lt;/code&gt;&lt;/pre&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;whats-not-done&quot;&gt;What’s Not Done&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;HTTP 300 Redirect handling. I consider this to be a higher level concern than connection caching.&lt;/li&gt;
  &lt;li&gt;LRU Connection recycling. At the moment, the program will allow the number of connections to grow without limit if
an unlimited number of different hosts are contacted. In a production system we would need to add more active state to
each connection and have some logic to destroy old unused connections to make way for new ones.&lt;/li&gt;
  &lt;li&gt;The &lt;code&gt;stop_token&lt;/code&gt; is not executor-aware. I have left the stop_token in for exposition but it should not be activated 
by a different executor to the one where the connection to it has been created at present. If you’re interested to 
see how this will look when complete, please submit an issue against the github repo and I’ll update the code and add
a demonstration of it.&lt;/li&gt;
  &lt;li&gt;A Cookie Jar and HTTP session management. Again, these are higher level concerns. The next layer up would take care of
these plus redirect handling.&lt;/li&gt;
  &lt;li&gt;The CMakeLists project in the repo has been tested with GCC 10.2 and Clang-11 on Fedora Linux. Microsoft developers
may need to make the odd tweak to get things working. I’m more than happy to accept PRs.&lt;/li&gt;
  &lt;li&gt;Setting up of the &lt;code&gt;ssl::context&lt;/code&gt; to check peer certificates. Doing this properly is complex enough to warrant another
 blog in its own right.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Thanks for reading. I hope you’ve found blog useful. Please by all means get in touch by:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;raising an &lt;a href=&quot;https://github.com/madmongo1/blog-new-year-2021/issues&quot;&gt;issue&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Contacting me in the #beast channel of &lt;a href=&quot;https://cppalliance.org/slack/&quot;&gt;CppLang Slack&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;email &lt;a href=&quot;mailto:hodges.r@gmail.com&quot;&gt;hodges.r@gmail.com&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content><author><name></name></author><category term="richard" /><summary type="html">Reusable HTTP(S) Connections Something I am often asked by users of Boost Beast is how to code a client which effectively re-uses a pool of HTTP connections, in the same way a web browser does. The premise is straightforward - if our client is going to be making multiple calls to a web server (or several of them) then it makes sense that once a connection has been used for one request, it is returned to a connection pool so that a subsequent request can make use of it. It also makes sense to have a limit on the number of concurrent connections that can be open against any one host. Otherwise, if the client needs to make multiple requests at the same time, it will end up creating new connections in parallel and lose the efficiency of re-using an existing connection. From these requirements, we can start to think about a design. Firstly, we can imagine a connection cache, with connections kept in a map keyed on host + scheme + port (we can’t re-use an HTTP port for an HTTPS request!). When a request needs a connection, it will either create a new one (connection limit per host not met) or will wait for an existing connection to become available (which implies a condition variable). Once a request has a connection to use, it will send the HTTP request and wait for the response. At this stage, there is a possibility that the active connection which has been allocated could have been idle since the last time it was used. In TCP there is no way to atomically check whether the remote host has closed the connection (or died). The only way to know is to actually read from the socket with a timeout. If the remote host has shutdown the socket, we will be notified as soon as the RST frame arrives at our host. If the remote host has stopped working or the network is bad, we’ll be notified by the timeout. Thus if our read operation results in an error and we have inherited the connection from the cache, we ought to re-try by reopening the connection to the remote host and repeating the write/read operations. However, if there is an error reported on the subsequent attempt, then we can conclude that this is a legitimate error to be reported back to the caller. In simplified pesudocode, the operation might look something like this (assuming we report transport errors as exceptions): response read_write(connection&amp;amp; conn) { response resp; auto retry = false; if(conn.is_initialised()) retry = true; else conn.connect(...); for(;;) { conn.write(request); auto err = conn.read(resp); if (err) { if(!std::exchange(retry, false)) throw system_error(err); request.clear(); conn.disconnect(); } else break; } return resp; } Structuring the Code General Principles In my previous blog I mentioned my preference for writing the implementation of a class in such a way that it does not need to take care of its own lifetime or mutual exclusion. These concerns are deferred to a wrapper or handle. Methods on the handle class take care of marshalling the call to the correct executor (or thread) and preserving the implementation’s lifetime. The implementation need only concern itself with the logic of handling the request. Here’s an example about which I’ll go into more detail later: net::awaitable&amp;lt; response_type &amp;gt; connection_cache::call(beast::http::verb method, const std::string &amp;amp; url, std::string data, beast::http::fields headers, request_options options) { // DRY - define an operation that performs the inner call. auto op = [&amp;amp;] { return impl_-&amp;gt;call(method, url, std::move(data), std::move(headers), std::move(options)); }; // deduce the current executor auto my_executor = co_await net::this_coro::executor; // either call directly or via a spawned coroutine co_return impl_-&amp;gt;get_executor() != my_executor ? co_await op() : co_await net::co_spawn(impl_-&amp;gt;get_executor(), op, net::use_awaitable); } Coroutines In this implementation I will be providing a C++20 coroutine interface over Asio executors once again. I am using coroutines because they are easier to write when compared to Asio’s composed operations, but are fundamentally the same thing in a prettier package. Mutual Exclusion For mutual exclusion I will be embedded an asio strand into each active object. The advantage of doing so means that no thread of execution is ever blocked which means we can limit the number of threads in the program to the number of free CPUs giving us maximum throughput of work. In reality of course, one thread is more than enough computing power for almost all asynchronous programs. It’s therefore better to think in terms of one executor per program component, with the implicit guarantee that a given executor will only perform work on one thread at a time. Thinking this way allows us to write code in a way that is agnostic of whether the final program is compiled to be single or multi-threaded. But What About Single-threaded Programs? In order that I don’t need to rewrite code when should I decide to make a single-threaded program multi-threaded or vice versa, I have a couple of little utility functions and types defined in config.hpp Specifically: namespace net { using namespace asio; using io_executor = io_context::executor_type; #ifdef MULTI_THREADED using io_strand = strand&amp;lt; io_executor &amp;gt;; inline io_strand new_strand(io_executor const &amp;amp;src); inline io_strand new_strand(io_strand const &amp;amp;src); #else using io_strand = io_context::executor_type; inline io_strand new_strand(io_executor const &amp;amp;src); #endif inline io_executor to_io_executor(any_io_executor const &amp;amp;src); } // namespace net Any object type in the program which would require its own strand in a multi-threaded program will simply use the type io_strand whether the program is compiled for single-threaded operation or not. Any code that would notionally need to construct a new strand simply calls new_strand(e) where e is either a strand or a naked executor. Any code that needs access to the notional underlying executor would call to_io_executor(e). Determinism Since we’re using asio’s executors for scheduling, it means that we can use asio’s timer as a deterministic, ordered asynchronous condition variable, which means that requests waiting on the connection pool will be offered free connections in the order that they were requested. This guarantee is implicit in the way that the timers’ cancel_one() method is specified. As we’ll see later, asio’s timers also make it trivial to implement an asynchronous semaphore. In this case we use one to ensure that requests are handled concurrently but no more than some upper limit at any one time. Interface I’m going to create a high-level concept called a connection_cache. The interface will be something like this: struct connection_cache { using response_ptr = std::unique_ptr&amp;lt; response &amp;gt;; net::awaitable&amp;lt; response_ptr &amp;gt; rest_call( verb method, std::string const&amp;amp; url, std::optional&amp;lt;std::string&amp;gt; data = std::nullopt, headers hdrs = {}, options opts = {}); }; There are a few things to note here. The return type of the coroutine is a unique_ptr to a response. A natural question might be whether the response should simply be returned by value. However, in practice I have found that there are a number of practical reasons why it’s often better to return the response as a pointer. Firstly it allows conversion to a shared_ptr&amp;lt;const response&amp;gt; in environments where the response might be passed through a directed acyclic graph. Secondly, would allow a further enhancement in that having finished with the response, the client could post it back to the cache, meaning that it could be cached for re-use. The only two required arguments are the method and url. All others can be defaulted. An optional string may be passed which contains the payload of a POST request. This is passed by value because, as we’ll see later,the implementation will want to move this into the request object prior to initiating communications. I have chosen a string type for two reasons text in the form of JSON or XML is the most common form of messaging in REST calls. strings can contain binary data with no loss of efficiency. hdrs is simply a list of headers which should be set in the request. Again, these are passed by value as they will be moved into the request object. The last parameter of the call is an as-yet undefined type called options. This will allow us to add niceties like timeout arguments, a stop_token, redirect policies, a reference to a cookie store and so on. When called, rest_call will attempt to reuse an existing connection. If a connection is not available, it will create a new one if we are under the connection threshold for the deduced host and if not, it will wait until a connection is available. Furthermore, the number of concurrent requests will be throttled to some upper limit. Transport failures will be reported as an exception (of type system_error) and a successful response (even if a 400 or 500) will be returned in the response_ptr. That is to say, as long as the transport layer works out, the code will take the non-exceptional path. Implementation details URL Parsing Among the things I am often asked about in the Beast slack channel and in the Issue Tracker is why there is no URL support in Beast. The is that Beast is a reasonably low level library that concerns itself with the HTTP (and WebSocket) protocols, plus as much buffer and stream management as is necessary to implement HTTP over Asio. The concept of a URL the subject of its own RFCs and is a higher level concern. The C++ Alliance is working on Boost.URL but it is not ready for publishing yet. In the meantime, I found a nifty regex on the internet that more-or-less suffices for our needs: std::tuple&amp;lt; connection_key, std::string &amp;gt; parse_url(std::string const &amp;amp;url) { static const auto url_regex = std::regex( R&quot;regex((http|https)://([^/ :]+):?([^/ ]*)((/?[^ #?]*)\x3f?([^ #]*)#?([^ ]*)))regex&quot;, std::regex_constants::icase | std::regex_constants::optimize); auto match = std::smatch(); if (not std::regex_match(url, match, url_regex)) throw system_error(net::error::invalid_argument, &quot;invalid url&quot;); auto &amp;amp;protocol = match[1]; auto &amp;amp;host = match[2]; auto &amp;amp;port_ind = match[3]; auto &amp;amp;target = match[4]; /* auto &amp;amp;path = match[5]; auto &amp;amp;query = match[6]; auto &amp;amp;fragment = match[7]; */ return std::make_tuple( connection_key { .hostname = host.str(), .port = deduce_port(protocol, port_ind), .scheme = deduce_scheme(protocol, port_ind) }, target.str()); } Exceptions in Asynchronous Code Considered Harmful I hate to admit this, because I am a huge fan of propagating errors as exceptions. This is because the combination of RAII and exception behaviour handling makes error handling very slick in C++. However, coroutines have two rather unpleasant limitations: You can’t call a coroutine in a destructor. You can’t call a coroutine in an exception handling block. There are workarounds. Consider: my_component::~my_component() { // destructor net::co_spawn(get_executor(), [impl = impl_]()-&amp;gt;net::awaitable&amp;lt;void&amp;gt; { co_await impl-&amp;gt;shutdown(); // destructor of *impl happens here }, net::detached); } This is the destructor of a wrapper object that contains a shared_ptr to its implementation, impl_. In this case we can detect the destruction of my_component and use this to spawn a new coroutine of indeterminate lifetime that takes care of shutting down the actual implementation and then destroying it. This solves the problem of RAII but it mandates that we must author objects that will be used in coroutines as a handle-body pair. We can similarly get around the “no coroutine calls in exception handlers” limitation if we’re prepared to stomach code like this: net::awaitable&amp;lt;void&amp;gt; my_coro() { std::function&amp;lt;net::awaitable(void)&amp;gt; on_error; try { co_await something(); } catch(...) { // set up error_handler on_error = [ep = std::current_exception] { return handler_error_coro(ep); }; } // perhaps handle the error here if (on_error) co_await on_error(); } I think you’ll agree that this is a revolting solution to an unforgivable omission in the language. Not only is it untidy, confusing, difficult to teach and error-prone, it also turns exception handling into same fiasco that is enforced checking of return codes. To add insult to injury, the error handling code in this function takes up 5 times as many lines as the logic! Therefore my recommendation is that in asynchronous coroutine code, it’s better to avoid exceptions and have coroutines either return a tuple of (error_code, possible_value) or a variant containing error-or-value. For example, here is some code from the connection_impl in my example project: net::awaitable&amp;lt; std::tuple&amp;lt; error_code, response_type &amp;gt; &amp;gt; connection_impl::rest_call(request_class const &amp;amp; request, request_options const &amp;amp;options) { auto response = std::make_unique&amp;lt; response_class &amp;gt;(); auto ec = stream_.is_open() ? co_await rest_call(request, *response, options) : net::error::basic_errors::not_connected; if (ec &amp;amp;&amp;amp; ec != net::error::operation_aborted) { ec = co_await connect(options.stop); if (!ec) ec = co_await rest_call(request, *response, options); } if (ec || response-&amp;gt;need_eof()) stream_.close(); co_return std::make_tuple(ec, std::move(response)); } Ensuring that a Coroutine Executes on the Correct Executor In playing with asio coroutines I stumbled upon something that has become an idiom. Consider the situation where a connection class is implemented in terms of a handle and body. The body contains its own executor. In a multi-threaded build, this executor will be a strand while in a single threaded-build we would want it to be simply an io_context::executor_type since there will be no need for any of the thread guards implicit in a strand. Now consider that the implementation has a member coroutine called (say) call. There are two scenarios in which this member will be called. The first is where the caller is executing in the same executor that is associated with the implementation, the second is where the caller is in its own different executor. In the latter case, we must post or spawn the execution of the coroutine onto the implementation’s executor in order to ensure that it runs in the correct sequence with respect to other coroutines initiated against it. The idiom that occurred to me originally was to recursively spawn a coroutine to ensure the call happened on the correct executor: net::awaitable&amp;lt; response_type &amp;gt; connection_cache::call(beast::http::verb method, const std::string &amp;amp; url, std::string data, beast::http::fields headers, request_options options) { auto my_executor = co_await net::this_coro::executor; if (impl_-&amp;gt;get_executor() == my_executor) { co_return co_await impl_-&amp;gt;call(method, url, std::move(data), std::move(headers), std::move(options)); } else { // spawn a coroutine which recurses on the correct executor. // wait for this coroutine to finish co_return co_await net::co_spawn( impl_-&amp;gt;get_executor(), [&amp;amp;]() -&amp;gt; net::awaitable&amp;lt; response_type &amp;gt; { return call(method, url, std::move(data), std::move(headers), std::move(options)); }, net::use_awaitable); } } However, this does have the drawback that a code analyser might see the possibility of infinite recursion. After discussing this with Chris, Asio’s author, a better solution was found: net::awaitable&amp;lt; response_type &amp;gt; connection_cache::call(beast::http::verb method, const std::string &amp;amp; url, std::string data, beast::http::fields headers, request_options options) { // DRY - define an operation that performs the inner call. auto op = [&amp;amp;] { return impl_-&amp;gt;call(method, url, std::move(data), std::move(headers), std::move(options)); }; // deduce the current executor auto my_executor = co_await net::this_coro::executor; // either call directly or via a spawned coroutine co_return impl_-&amp;gt;get_executor() != my_executor ? co_await op() : co_await net::co_spawn(impl_-&amp;gt;get_executor(), op, net::use_awaitable); } There are a few things to note: All lifetimes are correct even though the op takes arguments by reference. This is because whichever path we take, our outer coroutine will suspend on the call to op. Note that in the slow path, op is captured by value in the call to co_spawn. Had we written: : co_await net::co_spawn(impl_-&amp;gt;get_executor(), op(), net::use_awaitable); then op would have been called during the setup of the call to co_spawn, which would result in impl_-&amp;gt;call(...) being called on the wrong executor/thread. Talk is Cheap TL;DR. Enough talk, where’s the code? It’s on github at https://github.com/madmongo1/blog-new-year-2021. Final Notes. Before signing off I just wanted to cover a few of the features I have completed in this example and a few that I have not. What’s Done There is a limit on the number of concurrent connections to a single host. Host here is defined as a tuple of the transport scheme (i.e. http or https), port and hostname. This is currently defaulted to two, but would be trivial to change. There is a limit on the number of concurrent requests across all hosts. This defaults to 1000. Simultaneous requests numbering more than this will be processed through what is an asynchronous semaphore, implemented like this: while (request_count_ &amp;gt;= max_concurrent_requests_) { error_code ec; co_await concurrent_requests_available_.async_wait( net::redirect_error(net::use_awaitable, ec)); } ++request_count_; ... ... request takes place here ... if (--request_count_ &amp;lt; max_concurrent_requests_) { concurrent_requests_available_.cancel_one(); } What’s Not Done HTTP 300 Redirect handling. I consider this to be a higher level concern than connection caching. LRU Connection recycling. At the moment, the program will allow the number of connections to grow without limit if an unlimited number of different hosts are contacted. In a production system we would need to add more active state to each connection and have some logic to destroy old unused connections to make way for new ones. The stop_token is not executor-aware. I have left the stop_token in for exposition but it should not be activated by a different executor to the one where the connection to it has been created at present. If you’re interested to see how this will look when complete, please submit an issue against the github repo and I’ll update the code and add a demonstration of it. A Cookie Jar and HTTP session management. Again, these are higher level concerns. The next layer up would take care of these plus redirect handling. The CMakeLists project in the repo has been tested with GCC 10.2 and Clang-11 on Fedora Linux. Microsoft developers may need to make the odd tweak to get things working. I’m more than happy to accept PRs. Setting up of the ssl::context to check peer certificates. Doing this properly is complex enough to warrant another blog in its own right. Thanks for reading. I hope you’ve found blog useful. Please by all means get in touch by: raising an issue Contacting me in the #beast channel of CppLang Slack email hodges.r@gmail.com</summary></entry><entry><title type="html">Richard’s November/December Update</title><link href="http://cppalliance.org/richard/2020/12/22/RichardsDecemberUpdate.html" rel="alternate" type="text/html" title="Richard’s November/December Update" /><published>2020-12-22T00:00:00+00:00</published><updated>2020-12-22T00:00:00+00:00</updated><id>http://cppalliance.org/richard/2020/12/22/RichardsDecemberUpdate</id><content type="html" xml:base="http://cppalliance.org/richard/2020/12/22/RichardsDecemberUpdate.html">&lt;h1 id=&quot;a-coroutine-websocket-using-boost-beast&quot;&gt;A Coroutine Websocket Using Boost Beast&lt;/h1&gt;

&lt;p&gt;This month I thought I would present a little idea that I had a few months ago.&lt;/p&gt;

&lt;p&gt;Boost.Beast is a very comprehensive and competent websocket implementation, but it is not what you might call
“straightforward” to use unless you are already wise in the ways of Asio.&lt;/p&gt;

&lt;p&gt;Beast’s documentation and design makes no apology for this. There is a disclaimer in the 
&lt;a href=&quot;https://www.boost.org/doc/libs/1_75_0/libs/beast/doc/html/beast/using_io/asio_refresher.html&quot;&gt;documentation&lt;/a&gt;:&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;To use Beast effectively, a prior understanding of Networking is required.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;This is worth taking seriously (and if you are not fully aware of how Asio works with respect to the posting of 
completion handlers onto the associated executor, this page is worth studying).&lt;/p&gt;

&lt;h2 id=&quot;the-interface&quot;&gt;The Interface&lt;/h2&gt;

&lt;p&gt;I wanted to model my websocket object’s interface roughly on the javascript websocket connection interface. There will
be a few differences of course, because the javascript version uses callbacks (or continuations) and I will be using 
C++ coroutines that execute on an Asio executor. In underlying implementation, these concepts are not actually that far
apart, since Asio awaitables are actually implemented in terms of the normal asio completion token/handler interaction.&lt;/p&gt;

&lt;p&gt;Furthermore, I want my WebSocket’s connection phase to be cancellable.&lt;/p&gt;

&lt;p&gt;My websocket interface will look something like this:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-cpp&quot;&gt;namespace websocket
{
    struct message 
    {
        bool is_binary() const;
        bool is_text() const;
        std::string_view operator*() const;
    }; 
    
    struct event 
    {
        bool is_error() const;
        bool is_message() const;
        error_code const&amp;amp; error() const; 
        message const&amp;amp; message() const;
    };
    
    struct connection
    {
        /// Schedule a frame to be sent at some point in the near future
        void 
        send(std::string_view data, bool as_text = true);

        /// Suspend and wait until either there is a message available or an error        
        net::awaitable&amp;lt;event&amp;gt; 
        consume();
        
        /// Close the websocket and suspend until it is closed.
        net::awaitable&amp;lt;void&amp;gt; 
        close(beast::websocket::close_reason = /* a sensible default */); 

        /// Send the close frame to the server but don't hang around to wait
        /// for the confirmation.
        void 
        drop(beast::websocket::close_reason = /* a sensible default */);

        /// If consume() exits with an error of beast::websocket::error::closed then this
        /// will return the reason for the closure as sent by the server.
        /// Otherwise undefined.        
        beast::websocket::close_reason
        reason() const;
    };
    
    net::awaitable&amp;lt;connection&amp;gt; 
    connect(std::string url, 
            connect_options options /* = some default */);
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The idea here is to keep the interface as lightweight and as simple as possible. The websocket connection will run on 
the executor of the coroutine that created it. Any commands sent to the websocket will be executor safe. That is, 
internally their work will be dispatched to the websocket connection’s executor. The exception to this will be the 
close_reason method, which must only be called once the connection’s consume coroutine has returned an error event.
It is a guarantee that once &lt;code&gt;consume&lt;/code&gt; returns an event that is an error, it will never return anything else, and no
other method on the connection will mutate its internal state. In this condition, it is legal to call the &lt;code&gt;reason&lt;/code&gt; 
method.&lt;/p&gt;

&lt;p&gt;A typical use would look like this:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-cpp&quot;&gt;    // default options
    auto ws = co_await websocket::connect(&quot;wss://echo.websocket.org&quot;); 

    for(;;)
    {
        auto event = co_await ws.consume();
        if (event.is_error())
            break;
        else
            on_message(std::move(event.message()));
    }    
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The above code example does not provide any means to write to the websocket. But it would be trivial to either spawn
another coroutine to handle the writer, or call a function in order to signal some orthogonal process that the websocket 
was ready.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-cpp&quot;&gt;    // default options
    auto ws = co_await websocket::connect(&quot;wss://echo.websocket.org&quot;); 
    
    on_connected(ws); // The websocket object should be shared-copyable

    for(;;)
    {
        auto event = co_await ws.consume();
        if (event.is_error()) {
            on_close();
            break;
        }
        else
            on_message(std::move(event.message()));
    }    
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Another way to visualise a websocket is exactly as javascript’s websocket connection does, using callbacks or 
continuations in order to notify user code that the websocket has received data or closed. It would be trivial to wrap
our coroutine version in order to provide this functionality. We would need to spawn a coroutine in order to run 
the &lt;code&gt;consume()&lt;/code&gt; loop and then somehow signal it to stop if the websocket was disposed of.&lt;/p&gt;

&lt;p&gt;User code might then start to look something like this:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-cpp&quot;&gt;    websocket::connect(&quot;wss://echo.websocket.org&quot;, options)
        .on_connect([](websocket::connection ws)
        {
            run_my_loop(ws);
        });
        
void run_my_loop(websocket::connection ws)
{
    bool closed = false;
    ws.on_close([&amp;amp;]{ closed = true; });
    ws.on_error([&amp;amp;]{ closed = true; });
    ws.on_message([&amp;amp;](websocket::message msg){ process_message(msg); });

    // some time later
    ws.send(&quot;Hello, World!&quot;);
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;With this style of interface we would need some means of passing the executor on which the continuations would be 
invoked. A reasonable place to do this might be the &lt;code&gt;options&lt;/code&gt; parameter.&lt;/p&gt;

&lt;p&gt;In the JavaScript style interface, it would be important to be able to detect when the websocket has gone out of scope
and ensure that it closes correctly, otherwise we’ll have a rogue resource out there with no handle by which we can 
close it. This argues that the actual &lt;code&gt;websocket::connection&lt;/code&gt; class should be a handle to an internal implementation
and that the destructor of the handle should ensure that the implementation is signalled so that it can &lt;code&gt;drop&lt;/code&gt; the 
connection and shutdown cleanly. Under the covers, we’re implementing this websocket in Boost.Beast. As with all Asio
objects, there could be (probably will be) asynchronous operations in progress at the time the websocket handle goes 
out of scope.&lt;/p&gt;

&lt;p&gt;Thinking this through, it means that:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;The implementation is going to live longer than the lifetime of the last copy of the handle owning the implementation.&lt;/li&gt;
  &lt;li&gt;There needs to be some mechanism to cancel the underlying implementation’s operations.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Coroutines can be visualised as threads of execution. In the world of threads (e.g. &lt;code&gt;std::thread&lt;/code&gt;) we have primitives
such as &lt;code&gt;std::stop_token&lt;/code&gt; and &lt;code&gt;std::condition_variable&lt;/code&gt;. The C++ Standard Library does not yet have these primitives
for coroutines. And if it did it would be questionable whether they would be suitable for networking code where 
coroutines are actually built on top of Asio composed operations. Does Asio itself provide anything we can use?&lt;/p&gt;

&lt;h2 id=&quot;asios-hidden-asynchronous-condition_variable&quot;&gt;Asio’s Hidden Asynchronous condition_variable&lt;/h2&gt;

&lt;p&gt;The answer is surprisingly, yes. But not in the form I was expecting when I asked Chris Kohlhoff (Asio’s author and 
maintainer) about it. It turns out that asio’s timer models an asynchronous version of a condition variable perfectly. 
Consider:&lt;/p&gt;

&lt;p&gt;Given:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-cpp&quot;&gt;auto t = net::steady_timer(co_await net::this_coro::executor);
t.expires_at(std::chrono::stready_clock::time_point::max());
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Then we can write:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-cpp&quot;&gt;template&amp;lt;class Pred&amp;gt;
net::awaitable&amp;lt;void&amp;gt;
wait(net::steady_timer&amp;amp; t, Pred predicate)
{
    error_code ec;
    while(!ec &amp;amp;&amp;amp; !predicate())
    {
        co_await t.async_wait(net::redirect_error(net::use_awaitable, ec));
        if (ec == net::error::operation_aborted)
            // timer cancelled
            continue;
        else
            throw std::logic_error(&quot;unexpected error&quot;);
    } 
}

void
notify(net::steady_timer&amp;amp; t)
{
    // assuming we are executing on the same executor as the wait()
    t.cancel();
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Which gives us a simple asynchronous condition_variable (this one does not implement timeouts, but it would be trivial
to extend this code to accommodate them).&lt;/p&gt;

&lt;h2 id=&quot;asynchronous-stop-token&quot;&gt;Asynchronous Stop Token&lt;/h2&gt;

&lt;p&gt;The &lt;code&gt;std::stop_token&lt;/code&gt; is a welcome addition to the standard, but it is a little heavyweight for asynchronous code that 
runs in an executor, which is already thread-safe by design. A simple in-executor stop source can be implemented 
something like this:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-cpp&quot;&gt;namespace async {
namespace detail {
struct shared_state {
    void stop()
    {
        if (!std::exchange(stopped_, true))
        {
            auto sigs = std::move(signals_);
            signals_.clear();
            for(auto&amp;amp; s : sigs)
                s();
        }
    }
    
    std::list&amp;lt;std::function&amp;lt;void()&amp;gt;&amp;gt; signals_;
    bool stopped_ =  false;
};
}
struct stop_source {
    void stop() {
        impl_-&amp;gt;stop();
    }
    std::shared_ptr&amp;lt;shared_state&amp;gt; impl_;
}

struct stop_connection
{
};

struct stop_token
{
    stop_token(stop_source&amp;amp; source)
    : impl_(source.impl_) {
    }
    
    bool 
    stopped() const { return !impl_ || impl_-&amp;gt;stopped_; }
    
    stop_connection 
    connect(std::function&amp;lt;void()&amp;gt; slot);
     
    std::shared_ptr&amp;lt;shared_state&amp;gt; impl_;
}
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The use case would look something like this:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-cpp&quot;&gt;
net::awaitable&amp;lt;void&amp;gt;
something_with_websocket(async::stop_token token)
{
    // passing the stop token allows the connect call to abort early
    // if the owner of the stop_source wants to end the use of the
    // websocket before it is connected
    auto ws = websocket::connect(&quot;wss://echo.websocket.org&quot;, 
        websocket::connect_options { .stop_token = token });

    // connect a slot to the stop token which drops the connection        
    auto stopconn = token.connect([&amp;amp;] { ws.drop(); };
    
    for(;;} {
        auto event = co_await ws.consume();
        // ...etc
    }
    
}   
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now, armed with both a &lt;code&gt;stop_token&lt;/code&gt; and a &lt;code&gt;condition_variable&lt;/code&gt;, we gain a great deal of flexibility with programs 
running on an Asio-style executor.&lt;/p&gt;

&lt;p&gt;So let’s build a little chat app to talk the the echo bot.&lt;/p&gt;

&lt;h2 id=&quot;coding-style-when-using-asio-coroutines&quot;&gt;Coding style when using Asio coroutines.&lt;/h2&gt;

&lt;p&gt;I mentioned earlier that I like to decompose objects with complex lifetimes into an impl and handle. My personal 
programming style for naming the components is as follows:&lt;/p&gt;

&lt;h3 id=&quot;the-implementation&quot;&gt;The implementation&lt;/h3&gt;

&lt;p&gt;This is the class that implements the complex functionality that we want. I generally give this class an &lt;code&gt;_impl&lt;/code&gt; suffix
and apply the following guidelines:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;The impl does not control its own lifetime.&lt;/li&gt;
  &lt;li&gt;Calls to the impl are expected to already be executing on the correct thread or strand, and in the case of 
multi-threaded code, are expected to have already taken a lock on any mutex.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;This is a personal preference which I find tends to lower the complexity of the object, since the interface functions
do not have to manage more than one concern, and deadlocks etc are not possible.&lt;/p&gt;

&lt;h3 id=&quot;the-lifetime&quot;&gt;The lifetime&lt;/h3&gt;

&lt;p&gt;When holding an object in shared_ptr, we get a chance to intercept the destruction of the last handle. At this point
we do not have to destroy the implementation, but can allow it to shut down gracefully before destruction.
In order to do this, particularly with an object that is referenced by internal coroutines, I have found that it’s 
useful to separate the public lifetime of the object, and its internal lifetime, which may be longer than the public 
one.&lt;/p&gt;

&lt;p&gt;A convenient, if not especially efficient way to do this is to hold two shared_ptr’s in the handle. One being a
shared_ptr&lt;void&gt; which has a custom destructor - the lifetime ptr, and one being a normal shared_ptr to the 
implementation which can be copied in order to extend its private lifetime while it shuts down.
It is the responsibility of the custom deleter to signal the implementation that it should start shutting down.&lt;/void&gt;&lt;/p&gt;

&lt;p&gt;In this case, the websocket connection’s public handle may look something like this:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-cpp&quot;&gt;namespace websocket {

struct connection_lifetime
{
    connection_lifetime(std::shared_ptr&amp;lt;connection_impl&amp;gt;&amp;amp;&amp;amp; adopted)
    : impl_(std::move(adopted))
    , lifetime_(new_lifetime(impl_))
    {
    }
    
    static std::shared_ptr&amp;lt;void&amp;gt;
    new_lifetime(std::shared_ptr&amp;lt;connection_impl&amp;gt; const&amp;amp; impl)
    {
        static int useful_address;
        auto deleter = [impl](int*) noexcept
        {
            net::co_spawn(impl-&amp;gt;get_executor(), 
                [impl]() -&amp;gt; net::awaitable&amp;lt;void&amp;gt;
                { 
                    co_await impl-&amp;gt;stop();
                }, net::detached);
        };
        
        return std::shared_ptr&amp;lt;void&amp;gt;(&amp;amp;useful_address, deleter);
    }
    
    std::shared_ptr&amp;lt;connection_impl&amp;gt; impl_;
    std::shared_ptr&amp;lt;void&amp;gt; lifetime_;
};

struct connection
{
    connection(connection_lifetime l);
};
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The interesting part here is in the function &lt;code&gt;new_lifetime&lt;/code&gt;. There are a few things going on here.
First, we are capturing the internal lifetime of our &lt;code&gt;connection_impl&lt;/code&gt; and storing it in the deleter of the lifetime 
pointer. This of course means that the private implementation will live at least as long as the public lifetime.
Secondly, the deleter does not actually delete anything. It merely captures a copy of the impl pointer and runs a 
coroutine on the impl to completion before releasing the impl pointer. The idea is that this coroutine will not complete
until all internal coroutines within the implementation have completed. The provides the fortunate side effect that
operations running inside the impl do not have to capture the impl’s lifetime via shared_from_this.
It turns out that this aids composability, since subordinate coroutines within the implementation can be written as free
functions, and ported to other implementations that may not involve a shared_ptr.
It also means that the impl itself can be composed, since it has no restrictions on lifetime semantics.
i.e. If I wanted to implement a JSON-RPC connection by deriving from the websocket::connection_impl, I do not have to 
be concerned about translating shared_ptrs internally in the derived class.&lt;/p&gt;

&lt;h1 id=&quot;once-its-all-put-together&quot;&gt;Once it’s all put together&lt;/h1&gt;

&lt;p&gt;Finally, having created all the primitives (which I really should start collating into a library), we can test our
little websocket chat client, which becomes a very simple program:&lt;/p&gt;

&lt;p&gt;Here’s main:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-cpp&quot;&gt;int
main()
{
    net::io_context ioctx;

    net::co_spawn(
        ioctx.get_executor(), [] { return chat(); }, net::detached);

    ioctx.run();
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;And here’s the chat() coroutine:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-cpp&quot;&gt;net::awaitable&amp;lt; void &amp;gt;
chat()
{
    // connect the websocket
    auto ws = co_await websocket::connect(&quot;wss://echo.websocket.org&quot;);

    // spawn the coroutine to read console input and send it to the websocket
    auto stop_children = async::stop_source();
    net::co_spawn(
        co_await net::this_coro::executor,
        [stop = async::stop_token(stop_children), ws]() mutable {
            return do_console(std::move(stop), std::move(ws));
        },
        net::detached);

    // read events from the websocket connection.
    for (;;)
    {
        auto event = co_await ws.consume();
        if (event.is_error())
        {
            if (event.error() == beast::websocket::error::closed)
                std::cerr &amp;lt;&amp;lt; &quot;peer closed connection: &quot; &amp;lt;&amp;lt; ws.reason()
                          &amp;lt;&amp;lt; std::endl;
            else
                std::cerr &amp;lt;&amp;lt; &quot;connection error: &quot; &amp;lt;&amp;lt; event.error() &amp;lt;&amp;lt; std::endl;
            break;
        }
        else
        {
            std::cout &amp;lt;&amp;lt; &quot;message received: &quot; &amp;lt;&amp;lt; event.message() &amp;lt;&amp;lt; std::endl;
        }
    }
    
    // at this point, the stop_source goes out of scope, 
    // which will cause the console coroutine to exit.
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;And finally, the do_console() coroutine. Note that I have used asio’s posix interface to collect console input. 
In order to run compile in a WIN32 environment, we’d need to do something different (suggestions welcome via PR!).&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-cpp&quot;&gt;net::awaitable&amp;lt; void &amp;gt;
do_console(async::stop_token stop, websocket::connection ws)
try
{
    auto console = asio::posix::stream_descriptor(
        co_await net::this_coro::executor, ::dup(STDIN_FILENO));
    auto stopconn = stop.connect([&amp;amp;] { console.cancel(); });

    std::string console_chars;
    while (!stop.stopped())
    {
        auto line_len =
            co_await net::async_read_until(console,
                                           net::dynamic_buffer(console_chars),
                                           '\n',
                                           net::use_awaitable);
        auto line = console_chars.substr(0, line_len - 1);
        console_chars.erase(0, line_len);
        std::cout &amp;lt;&amp;lt; &quot;you typed this: &quot; &amp;lt;&amp;lt; line &amp;lt;&amp;lt; std::endl;
        ws.send(line);
    }
}
catch(...) {
  // error handling here
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;If you’d like to look into the complete code, submit a PR or offer some (probably well-deserved) criticism, you will
find the &lt;a href=&quot;https://github.com/madmongo1/blog-december-2020&quot;&gt;code repository here&lt;/a&gt;.&lt;/p&gt;</content><author><name></name></author><category term="richard" /><summary type="html">A Coroutine Websocket Using Boost Beast This month I thought I would present a little idea that I had a few months ago. Boost.Beast is a very comprehensive and competent websocket implementation, but it is not what you might call “straightforward” to use unless you are already wise in the ways of Asio. Beast’s documentation and design makes no apology for this. There is a disclaimer in the documentation: To use Beast effectively, a prior understanding of Networking is required. This is worth taking seriously (and if you are not fully aware of how Asio works with respect to the posting of completion handlers onto the associated executor, this page is worth studying). The Interface I wanted to model my websocket object’s interface roughly on the javascript websocket connection interface. There will be a few differences of course, because the javascript version uses callbacks (or continuations) and I will be using C++ coroutines that execute on an Asio executor. In underlying implementation, these concepts are not actually that far apart, since Asio awaitables are actually implemented in terms of the normal asio completion token/handler interaction. Furthermore, I want my WebSocket’s connection phase to be cancellable. My websocket interface will look something like this: namespace websocket { struct message { bool is_binary() const; bool is_text() const; std::string_view operator*() const; }; struct event { bool is_error() const; bool is_message() const; error_code const&amp;amp; error() const; message const&amp;amp; message() const; }; struct connection { /// Schedule a frame to be sent at some point in the near future void send(std::string_view data, bool as_text = true); /// Suspend and wait until either there is a message available or an error net::awaitable&amp;lt;event&amp;gt; consume(); /// Close the websocket and suspend until it is closed. net::awaitable&amp;lt;void&amp;gt; close(beast::websocket::close_reason = /* a sensible default */); /// Send the close frame to the server but don't hang around to wait /// for the confirmation. void drop(beast::websocket::close_reason = /* a sensible default */); /// If consume() exits with an error of beast::websocket::error::closed then this /// will return the reason for the closure as sent by the server. /// Otherwise undefined. beast::websocket::close_reason reason() const; }; net::awaitable&amp;lt;connection&amp;gt; connect(std::string url, connect_options options /* = some default */); } The idea here is to keep the interface as lightweight and as simple as possible. The websocket connection will run on the executor of the coroutine that created it. Any commands sent to the websocket will be executor safe. That is, internally their work will be dispatched to the websocket connection’s executor. The exception to this will be the close_reason method, which must only be called once the connection’s consume coroutine has returned an error event. It is a guarantee that once consume returns an event that is an error, it will never return anything else, and no other method on the connection will mutate its internal state. In this condition, it is legal to call the reason method. A typical use would look like this: // default options auto ws = co_await websocket::connect(&quot;wss://echo.websocket.org&quot;); for(;;) { auto event = co_await ws.consume(); if (event.is_error()) break; else on_message(std::move(event.message())); } The above code example does not provide any means to write to the websocket. But it would be trivial to either spawn another coroutine to handle the writer, or call a function in order to signal some orthogonal process that the websocket was ready. // default options auto ws = co_await websocket::connect(&quot;wss://echo.websocket.org&quot;); on_connected(ws); // The websocket object should be shared-copyable for(;;) { auto event = co_await ws.consume(); if (event.is_error()) { on_close(); break; } else on_message(std::move(event.message())); } Another way to visualise a websocket is exactly as javascript’s websocket connection does, using callbacks or continuations in order to notify user code that the websocket has received data or closed. It would be trivial to wrap our coroutine version in order to provide this functionality. We would need to spawn a coroutine in order to run the consume() loop and then somehow signal it to stop if the websocket was disposed of. User code might then start to look something like this: websocket::connect(&quot;wss://echo.websocket.org&quot;, options) .on_connect([](websocket::connection ws) { run_my_loop(ws); }); void run_my_loop(websocket::connection ws) { bool closed = false; ws.on_close([&amp;amp;]{ closed = true; }); ws.on_error([&amp;amp;]{ closed = true; }); ws.on_message([&amp;amp;](websocket::message msg){ process_message(msg); }); // some time later ws.send(&quot;Hello, World!&quot;); } With this style of interface we would need some means of passing the executor on which the continuations would be invoked. A reasonable place to do this might be the options parameter. In the JavaScript style interface, it would be important to be able to detect when the websocket has gone out of scope and ensure that it closes correctly, otherwise we’ll have a rogue resource out there with no handle by which we can close it. This argues that the actual websocket::connection class should be a handle to an internal implementation and that the destructor of the handle should ensure that the implementation is signalled so that it can drop the connection and shutdown cleanly. Under the covers, we’re implementing this websocket in Boost.Beast. As with all Asio objects, there could be (probably will be) asynchronous operations in progress at the time the websocket handle goes out of scope. Thinking this through, it means that: The implementation is going to live longer than the lifetime of the last copy of the handle owning the implementation. There needs to be some mechanism to cancel the underlying implementation’s operations. Coroutines can be visualised as threads of execution. In the world of threads (e.g. std::thread) we have primitives such as std::stop_token and std::condition_variable. The C++ Standard Library does not yet have these primitives for coroutines. And if it did it would be questionable whether they would be suitable for networking code where coroutines are actually built on top of Asio composed operations. Does Asio itself provide anything we can use? Asio’s Hidden Asynchronous condition_variable The answer is surprisingly, yes. But not in the form I was expecting when I asked Chris Kohlhoff (Asio’s author and maintainer) about it. It turns out that asio’s timer models an asynchronous version of a condition variable perfectly. Consider: Given: auto t = net::steady_timer(co_await net::this_coro::executor); t.expires_at(std::chrono::stready_clock::time_point::max()); Then we can write: template&amp;lt;class Pred&amp;gt; net::awaitable&amp;lt;void&amp;gt; wait(net::steady_timer&amp;amp; t, Pred predicate) { error_code ec; while(!ec &amp;amp;&amp;amp; !predicate()) { co_await t.async_wait(net::redirect_error(net::use_awaitable, ec)); if (ec == net::error::operation_aborted) // timer cancelled continue; else throw std::logic_error(&quot;unexpected error&quot;); } } void notify(net::steady_timer&amp;amp; t) { // assuming we are executing on the same executor as the wait() t.cancel(); } Which gives us a simple asynchronous condition_variable (this one does not implement timeouts, but it would be trivial to extend this code to accommodate them). Asynchronous Stop Token The std::stop_token is a welcome addition to the standard, but it is a little heavyweight for asynchronous code that runs in an executor, which is already thread-safe by design. A simple in-executor stop source can be implemented something like this: namespace async { namespace detail { struct shared_state { void stop() { if (!std::exchange(stopped_, true)) { auto sigs = std::move(signals_); signals_.clear(); for(auto&amp;amp; s : sigs) s(); } } std::list&amp;lt;std::function&amp;lt;void()&amp;gt;&amp;gt; signals_; bool stopped_ = false; }; } struct stop_source { void stop() { impl_-&amp;gt;stop(); } std::shared_ptr&amp;lt;shared_state&amp;gt; impl_; } struct stop_connection { }; struct stop_token { stop_token(stop_source&amp;amp; source) : impl_(source.impl_) { } bool stopped() const { return !impl_ || impl_-&amp;gt;stopped_; } stop_connection connect(std::function&amp;lt;void()&amp;gt; slot); std::shared_ptr&amp;lt;shared_state&amp;gt; impl_; } } The use case would look something like this: net::awaitable&amp;lt;void&amp;gt; something_with_websocket(async::stop_token token) { // passing the stop token allows the connect call to abort early // if the owner of the stop_source wants to end the use of the // websocket before it is connected auto ws = websocket::connect(&quot;wss://echo.websocket.org&quot;, websocket::connect_options { .stop_token = token }); // connect a slot to the stop token which drops the connection auto stopconn = token.connect([&amp;amp;] { ws.drop(); }; for(;;} { auto event = co_await ws.consume(); // ...etc } } Now, armed with both a stop_token and a condition_variable, we gain a great deal of flexibility with programs running on an Asio-style executor. So let’s build a little chat app to talk the the echo bot. Coding style when using Asio coroutines. I mentioned earlier that I like to decompose objects with complex lifetimes into an impl and handle. My personal programming style for naming the components is as follows: The implementation This is the class that implements the complex functionality that we want. I generally give this class an _impl suffix and apply the following guidelines: The impl does not control its own lifetime. Calls to the impl are expected to already be executing on the correct thread or strand, and in the case of multi-threaded code, are expected to have already taken a lock on any mutex. This is a personal preference which I find tends to lower the complexity of the object, since the interface functions do not have to manage more than one concern, and deadlocks etc are not possible. The lifetime When holding an object in shared_ptr, we get a chance to intercept the destruction of the last handle. At this point we do not have to destroy the implementation, but can allow it to shut down gracefully before destruction. In order to do this, particularly with an object that is referenced by internal coroutines, I have found that it’s useful to separate the public lifetime of the object, and its internal lifetime, which may be longer than the public one. A convenient, if not especially efficient way to do this is to hold two shared_ptr’s in the handle. One being a shared_ptr which has a custom destructor - the lifetime ptr, and one being a normal shared_ptr to the implementation which can be copied in order to extend its private lifetime while it shuts down. It is the responsibility of the custom deleter to signal the implementation that it should start shutting down. In this case, the websocket connection’s public handle may look something like this: namespace websocket { struct connection_lifetime { connection_lifetime(std::shared_ptr&amp;lt;connection_impl&amp;gt;&amp;amp;&amp;amp; adopted) : impl_(std::move(adopted)) , lifetime_(new_lifetime(impl_)) { } static std::shared_ptr&amp;lt;void&amp;gt; new_lifetime(std::shared_ptr&amp;lt;connection_impl&amp;gt; const&amp;amp; impl) { static int useful_address; auto deleter = [impl](int*) noexcept { net::co_spawn(impl-&amp;gt;get_executor(), [impl]() -&amp;gt; net::awaitable&amp;lt;void&amp;gt; { co_await impl-&amp;gt;stop(); }, net::detached); }; return std::shared_ptr&amp;lt;void&amp;gt;(&amp;amp;useful_address, deleter); } std::shared_ptr&amp;lt;connection_impl&amp;gt; impl_; std::shared_ptr&amp;lt;void&amp;gt; lifetime_; }; struct connection { connection(connection_lifetime l); }; } The interesting part here is in the function new_lifetime. There are a few things going on here. First, we are capturing the internal lifetime of our connection_impl and storing it in the deleter of the lifetime pointer. This of course means that the private implementation will live at least as long as the public lifetime. Secondly, the deleter does not actually delete anything. It merely captures a copy of the impl pointer and runs a coroutine on the impl to completion before releasing the impl pointer. The idea is that this coroutine will not complete until all internal coroutines within the implementation have completed. The provides the fortunate side effect that operations running inside the impl do not have to capture the impl’s lifetime via shared_from_this. It turns out that this aids composability, since subordinate coroutines within the implementation can be written as free functions, and ported to other implementations that may not involve a shared_ptr. It also means that the impl itself can be composed, since it has no restrictions on lifetime semantics. i.e. If I wanted to implement a JSON-RPC connection by deriving from the websocket::connection_impl, I do not have to be concerned about translating shared_ptrs internally in the derived class. Once it’s all put together Finally, having created all the primitives (which I really should start collating into a library), we can test our little websocket chat client, which becomes a very simple program: Here’s main: int main() { net::io_context ioctx; net::co_spawn( ioctx.get_executor(), [] { return chat(); }, net::detached); ioctx.run(); } And here’s the chat() coroutine: net::awaitable&amp;lt; void &amp;gt; chat() { // connect the websocket auto ws = co_await websocket::connect(&quot;wss://echo.websocket.org&quot;); // spawn the coroutine to read console input and send it to the websocket auto stop_children = async::stop_source(); net::co_spawn( co_await net::this_coro::executor, [stop = async::stop_token(stop_children), ws]() mutable { return do_console(std::move(stop), std::move(ws)); }, net::detached); // read events from the websocket connection. for (;;) { auto event = co_await ws.consume(); if (event.is_error()) { if (event.error() == beast::websocket::error::closed) std::cerr &amp;lt;&amp;lt; &quot;peer closed connection: &quot; &amp;lt;&amp;lt; ws.reason() &amp;lt;&amp;lt; std::endl; else std::cerr &amp;lt;&amp;lt; &quot;connection error: &quot; &amp;lt;&amp;lt; event.error() &amp;lt;&amp;lt; std::endl; break; } else { std::cout &amp;lt;&amp;lt; &quot;message received: &quot; &amp;lt;&amp;lt; event.message() &amp;lt;&amp;lt; std::endl; } } // at this point, the stop_source goes out of scope, // which will cause the console coroutine to exit. } And finally, the do_console() coroutine. Note that I have used asio’s posix interface to collect console input. In order to run compile in a WIN32 environment, we’d need to do something different (suggestions welcome via PR!). net::awaitable&amp;lt; void &amp;gt; do_console(async::stop_token stop, websocket::connection ws) try { auto console = asio::posix::stream_descriptor( co_await net::this_coro::executor, ::dup(STDIN_FILENO)); auto stopconn = stop.connect([&amp;amp;] { console.cancel(); }); std::string console_chars; while (!stop.stopped()) { auto line_len = co_await net::async_read_until(console, net::dynamic_buffer(console_chars), '\n', net::use_awaitable); auto line = console_chars.substr(0, line_len - 1); console_chars.erase(0, line_len); std::cout &amp;lt;&amp;lt; &quot;you typed this: &quot; &amp;lt;&amp;lt; line &amp;lt;&amp;lt; std::endl; ws.send(line); } } catch(...) { // error handling here } If you’d like to look into the complete code, submit a PR or offer some (probably well-deserved) criticism, you will find the code repository here.</summary></entry><entry><title type="html">Richard’s October Update</title><link href="http://cppalliance.org/richard/2020/10/31/RichardsOctoberUpdate.html" rel="alternate" type="text/html" title="Richard’s October Update" /><published>2020-10-31T00:00:00+00:00</published><updated>2020-10-31T00:00:00+00:00</updated><id>http://cppalliance.org/richard/2020/10/31/RichardsOctoberUpdate</id><content type="html" xml:base="http://cppalliance.org/richard/2020/10/31/RichardsOctoberUpdate.html">&lt;h1 id=&quot;asio-coroutines-in-qt-applications&quot;&gt;Asio Coroutines in Qt applications!&lt;/h1&gt;

&lt;p&gt;I started this train of thought when I wanted to hook up some back-end style code that I had written to a gui front end.
One way to do this would be to have a web front end subscribing to a back-end service, but I am no expert in modern web 
technologies so rather than spend time learning something that wasn’t C++ I decided to reach for the 
popular-but-so-far-unused-by-me C++ GUI framework, Qt.&lt;/p&gt;

&lt;p&gt;The challenge was how to hook up Qt, which is an event driven framework to a service written with Asio C++ coroutines.&lt;/p&gt;

&lt;p&gt;In the end it turned out to be easier than I had expected. Here’s how.&lt;/p&gt;

&lt;h2 id=&quot;a-simple-executor&quot;&gt;A simple Executor&lt;/h2&gt;

&lt;p&gt;As mentioned in a previous blog, Asio comes with a full implementation of the 
&lt;a href=&quot;http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2020/p0443r12.html&quot;&gt;Unified Executors proposal&lt;/a&gt;. Asio coroutines
are designed to be initiated and continued within an executor’s execution context. So let’s build an executor that will
perform work in a Qt UI thread.&lt;/p&gt;

&lt;p&gt;The executor I am going to build will have to invoke completion handlers to Asio IO objects, so we need to make it 
compatible with &lt;code&gt;asio::any_io_executor&lt;/code&gt;. This means it needs to have an associated 
&lt;a href=&quot;https://www.boost.org/doc/libs/1_74_0/doc/html/boost_asio/reference/execution_context.html&quot;&gt;execution context&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;The execution context is going to ultimately perform work on a Qt Application, so it makes sense to capture a reference
to the Application. Although Qt defines the macro &lt;code&gt;qApp&lt;/code&gt; to resolve to a pointer to the “current” application, for 
testing and sanity purposes I prefer that all services I write allow dependency injection, so I’ll arrange things so 
that the execution_context’s constructor takes an optional pointer to an application. In addition, it will be convenient
when writing components to not have to specifically create and pass an an execution context to windows within the Qt 
application so it makes sense to be able to provide access to a default context which references the default application.
Here’s a first cut:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-cpp&quot;&gt;struct qt_execution_context : net::execution_context
    , boost::noncopyable
{
    qt_execution_context(QApplication *app = qApp)
        : app_(app)
    {
        instance_ = this;
    }

    template&amp;lt;class F&amp;gt;
    void
    post(F f)
    {
        // todo
    }

    static qt_execution_context &amp;amp;
    singleton()
    {
        assert(instance_);
        return *instance_;
    }

private:
    static qt_execution_context *instance_;
    QApplication *app_;
};
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This class will provide two services. The first is to provide the asio service infrastructure so that we can create 
timers, sockets etc that use executors associated with this context and the second is to allow the executor to actually
dispatch work in a Qt application. This is the purpose of the &lt;code&gt;post&lt;/code&gt; method.&lt;/p&gt;

&lt;p&gt;Now a Qt application is itself a kind of execution context - in that it dispatches QEvent objects to be handled by 
children of the application. We can use this infrastructure to ensure that work dispatched by this execution context
actually takes place on the correct thread and at the correct time.&lt;/p&gt;

&lt;p&gt;In order for us to dispatch work to the application, we need to wrap our function into a QEvent:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-cpp&quot;&gt;class qt_work_event_base : public QEvent
{
public:
    qt_work_event_base()
        : QEvent(generated_type())
    {
    }

    virtual void
    invoke() = 0;

    static QEvent::Type
    generated_type()
    {
        static int event_type = QEvent::registerEventType();
        return static_cast&amp;lt;QEvent::Type&amp;gt;(event_type);
    }
};

template&amp;lt;class F&amp;gt;
struct basic_qt_work_event : qt_work_event_base
{
    basic_qt_work_event(F f)
        : f_(std::move(f))
    {}

    void
    invoke() override
    {
        f_();
    }

private:
    F f_;
};
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As opposed to using a &lt;code&gt;std::function&lt;/code&gt;, the &lt;code&gt;basic_qt_work_event&lt;/code&gt; allows us to wrap a move-only function object, which is 
important when that object is actually an Asio completion handler. Completion handlers benefit from being move-only as 
it means they can carry move-only state. This makes them more versatile, and can often lead to improvements in 
execution performance.&lt;/p&gt;

&lt;p&gt;Now we just need to fill out the code for &lt;code&gt;qt_execution_context::post&lt;/code&gt; and provide a mechanism in the Qt application to
detect and dispatch these messages:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-cpp&quot;&gt;    template&amp;lt;class F&amp;gt;
    void
    post(F f)
    {
        // c++20 auto template deduction
        auto event = new basic_qt_work_event(std::move(f));
        QApplication::postEvent(app_, event);
    }
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&quot;language-cpp&quot;&gt;class qt_net_application : public QApplication
{
    using QApplication::QApplication;

protected:
    bool
    event(QEvent *event) override;
};

bool
qt_net_application::event(QEvent *event)
{
    if (event-&amp;gt;type() == qt_work_event_base::generated_type())
    {
        auto p = static_cast&amp;lt;qt_work_event_base*&amp;gt;(event);
        p-&amp;gt;accept();
        p-&amp;gt;invoke();
        return true;
    }
    else
    {
        return QApplication::event(event);
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Note that I have seen on stack overflow the technique of invoking a function object in the destructor of the 
&lt;code&gt;QEvent&lt;/code&gt;-derived event. This would mean no necessity of custom event handling in the &lt;code&gt;QApplication&lt;/code&gt; but there are two
problems that I can see with this approach:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;I don’t know enough about Qt to know that this is safe and correct, and&lt;/li&gt;
  &lt;li&gt;Executors-TS executors can be destroyed while there are still un-invoked handlers within them. The correct behaviour
is to destroy these handlers without invoking them. If we put invocation code in the destructors, they will actually
mass-invoke when the executor is destroyed, leading most probably to annihilation of our program by segfault.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;However, that being done, we can now write the executor to meet the minimal expectations of an asio executor which can 
be used in an &lt;code&gt;any_io_executor&lt;/code&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-cpp&quot;&gt;struct qt_executor
{
    qt_executor(qt_execution_context &amp;amp;context = qt_execution_context::singleton()) noexcept
        : context_(std::addressof(context))
    {
    }

    qt_execution_context &amp;amp;query(net::execution::context_t) const noexcept
    {
        return *context_;
    }

    static constexpr net::execution::blocking_t
    query(net::execution::blocking_t) noexcept
    {
        return net::execution::blocking.never;
    }

    static constexpr net::execution::relationship_t
    query(net::execution::relationship_t) noexcept
    {
        return net::execution::relationship.fork;
    }

    static constexpr net::execution::outstanding_work_t
    query(net::execution::outstanding_work_t) noexcept
    {
        return net::execution::outstanding_work.tracked;
    }

    template &amp;lt; typename OtherAllocator &amp;gt;
    static constexpr auto query(
    net::execution::allocator_t&amp;lt; OtherAllocator &amp;gt;) noexcept
    {
        return std::allocator&amp;lt;void&amp;gt;();
    }

    static constexpr auto
    query(net::execution::allocator_t&amp;lt; void &amp;gt;) noexcept
    {
        return std::allocator&amp;lt;void&amp;gt;();
    }

    template&amp;lt;class F&amp;gt;
    void
    execute(F f) const
    {
        context_-&amp;gt;post(std::move(f));
    }

    bool
    operator==(qt_executor const &amp;amp;other) const noexcept
    {
        return context_ == other.context_;
    }

    bool
    operator!=(qt_executor const &amp;amp;other) const noexcept
    {
        return !(*this == other);
    }

private:
    qt_execution_context *context_;
};


static_assert(net::execution::is_executor_v&amp;lt;qt_executor&amp;gt;);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now all that remains is to write a subclass of some Qt Widget so that we can dispatch some work against it.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-cpp&quot;&gt;class test_widget : public QTextEdit
{
    Q_OBJECT
public:
    using QTextEdit::QTextEdit;

private:
    void
    showEvent(QShowEvent *event) override;

    void
    hideEvent(QHideEvent *event) override;

    net::awaitable&amp;lt;void&amp;gt;
    run_demo();
};

void
test_widget::showEvent(QShowEvent *event)
{
    net::co_spawn(
    qt_executor(), [this] {
        return run_demo();
    },
    net::detached);

    QTextEdit::showEvent(event);
}

void
test_widget::hideEvent(QHideEvent *event)
{
    QWidget::hideEvent(event);
}

net::awaitable&amp;lt;void&amp;gt;
test_widget::run_demo()
{
    using namespace std::literals;

    auto timer = net::high_resolution_timer(co_await net::this_coro::executor);

    for (int i = 0; i &amp;lt; 10; ++i)
    {
        timer.expires_after(1s);
        co_await timer.async_wait(net::use_awaitable);
        this-&amp;gt;setText(QString::fromStdString(std::to_string(i + 1) + &quot; seconds&quot;));
    }
    co_return;
}

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Here is the code for &lt;a href=&quot;https://github.com/madmongo1/blog-october-2020/tree/stage-1&quot;&gt;stage 1&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;And here is a screenshot of the app running:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/posts/richard/2020-october/stage-1.png&quot; alt=&quot;app running&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;all-very-well&quot;&gt;All very well…&lt;/h2&gt;

&lt;p&gt;OK, so we have a coroutine running in a Qt application. This is nice because it allows us to express an event-driven 
system in terms of procedural expression of code in a coroutine.&lt;/p&gt;

&lt;p&gt;But what if the user closes the window before the coroutine completes?&lt;/p&gt;

&lt;p&gt;This application has created the window on the stack, but in a larger application, there will be multiple windows and 
they may open and close at any time. It is not unusual in Qt to delete a closed window. If the coroutine continues to 
run once the windows that’s hosting it is deleted, we are sure to get a segfault.&lt;/p&gt;

&lt;p&gt;One answer to this is to maintain a sentinel in the Qt widget implementation, which prevents the continuation of the 
coroutine if destroyed. A &lt;code&gt;std::shared_ptr/weak_ptr&lt;/code&gt; pair would seem like a sensible solution. Let’s create an updated
version of the executor:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-cpp&quot;&gt;struct qt_guarded_executor
{
    qt_guarded_executor(std::weak_ptr&amp;lt;void&amp;gt; guard,
                        qt_execution_context &amp;amp;context
                        = qt_execution_context::singleton()) noexcept
        : context_(std::addressof(context))
        , guard_(std::move(guard))
    {}

    qt_execution_context &amp;amp;query(net::execution::context_t) const noexcept
    {
        return *context_;
    }

    static constexpr net::execution::blocking_t
    query(net::execution::blocking_t) noexcept
    {
        return net::execution::blocking.never;
    }

    static constexpr net::execution::relationship_t
    query(net::execution::relationship_t) noexcept
    {
        return net::execution::relationship.fork;
    }

    static constexpr net::execution::outstanding_work_t
    query(net::execution::outstanding_work_t) noexcept
    {
        return net::execution::outstanding_work.tracked;
    }

    template&amp;lt;typename OtherAllocator&amp;gt;
    static constexpr auto
    query(net::execution::allocator_t&amp;lt;OtherAllocator&amp;gt;) noexcept
    {
        return std::allocator&amp;lt;void&amp;gt;();
    }

    static constexpr auto query(net::execution::allocator_t&amp;lt;void&amp;gt;) noexcept
    {
        return std::allocator&amp;lt;void&amp;gt;();
    }

    template&amp;lt;class F&amp;gt;
    void
    execute(F f) const
    {
        if (auto lock1 = guard_.lock())
        {
            context_-&amp;gt;post([guard = guard_, f = std::move(f)]() mutable {
                if (auto lock2 = guard.lock())
                    f();
            });
        }
    }

    bool
    operator==(qt_guarded_executor const &amp;amp;other) const noexcept
    {
        return context_ == other.context_ &amp;amp;&amp;amp; !guard_.owner_before(other.guard_)
            &amp;amp;&amp;amp; !other.guard_.owner_before(guard_);
    }

    bool
    operator!=(qt_guarded_executor const &amp;amp;other) const noexcept
    {
        return !(*this == other);
    }

private:
    qt_execution_context *context_;
    std::weak_ptr&amp;lt;void&amp;gt; guard_;
};
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now we’ll make a little boilerplate class that we can use as a base class in any executor-enabled object in Qt:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-cpp&quot;&gt;struct has_guarded_executor
{
    using executor_type = qt_guarded_executor;

    has_guarded_executor(qt_execution_context &amp;amp;ctx
                         = qt_execution_context::singleton())
        : context_(std::addressof(ctx))
    {
        new_guard();
    }

    void
    new_guard()
    {
        static int x = 0;
        guard_ = std::shared_ptr&amp;lt;int&amp;gt;(std::addressof(x),
                                      // no-op deleter
                                      [](auto *) {});
    }

    void
    reset_guard()
    {
        guard_.reset();
    }

    executor_type
    get_executor() const
    {
        return qt_guarded_executor(guard_, *context_);
    }

private:
    qt_execution_context *context_;
    std::shared_ptr&amp;lt;void&amp;gt; guard_;
};
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;And we can modify the &lt;code&gt;test_widget&lt;/code&gt; to use it:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-cpp&quot;&gt;class test_widget
    : public QTextEdit
    , public has_guarded_executor
{
    ...
};

void
test_widget::showEvent(QShowEvent *event)
{
    // stop all existing coroutines and create a new guard
    new_guard();

    // start our coroutine
    net::co_spawn(
        get_executor(), [this] { return run_demo(); }, net::detached);

    QTextEdit::showEvent(event);
}

void
test_widget::hideEvent(QHideEvent *event)
{
    // stop all coroutines
    reset_guard();
    QWidget::hideEvent(event);
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now we’ll update the application to allow the creation and deletion of our widget. For this I’ll use the QMdiWindow
and add a menu with an action to create new widgets.&lt;/p&gt;

&lt;p&gt;We are now able to create and destroy widgets at will, with no segfaults.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/posts/richard/2020-october/stage-2.png&quot; alt=&quot;MDI app running&quot; /&gt;&lt;/p&gt;

&lt;p&gt;If you look at the code, you’ll also see that I’ve wired up a rudimentary signal/slot device to allow the coroutine to 
be cancelled early.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-cpp&quot;&gt;    // test_widget.hpp

    void
    listen_for_stop(std::function&amp;lt;void()&amp;gt; slot);

    void
    stop_all();

    std::vector&amp;lt;std::function&amp;lt;void()&amp;gt;&amp;gt; stop_signals_;
    bool stopped_ = false;

    // test_widget.cpp

    void
    test_widget::listen_for_stop(std::function&amp;lt;void()&amp;gt; slot)
    {
        if (stopped_)
            return slot();
    
        stop_signals_.push_back(std::move(slot));
    }
    
    void
    test_widget::stop_all()
    {
        stopped_ = true;
        auto copy = std::exchange(stop_signals_, {});
        for (auto &amp;amp;slot : copy) slot();
    }
    
    void
    test_widget::closeEvent(QCloseEvent *event)
    {
        stop_all();
        QWidget::closeEvent(event);
    }

    net::awaitable&amp;lt;void&amp;gt;
    test_widget::run_demo()
    {
        using namespace std::literals;
    
        auto timer = net::high_resolution_timer(co_await net::this_coro::executor);
    
        auto done = false;
    
        listen_for_stop([&amp;amp;] {
            done = true;
            timer.cancel();
        });
    
        while (!done)
        {
            for (int i = 0; i &amp;lt; 10; ++i)
            {
                timer.expires_after(1s);
                auto ec = boost::system::error_code();
                co_await timer.async_wait(
                    net::redirect_error(net::use_awaitable, ec));
                if (ec)
                {
                    done = true;
                    break;
                }
                this-&amp;gt;setText(
                    QString::fromStdString(std::to_string(i + 1) + &quot; seconds&quot;));
            }
    
            for (int i = 10; i--;)
            {
                timer.expires_after(250ms);
                auto ec = boost::system::error_code();
                co_await timer.async_wait(
                    net::redirect_error(net::use_awaitable, ec));
                if (ec)
                {
                    done = true;
                    break;
                }
                this-&amp;gt;setText(QString::fromStdString(std::to_string(i)));
            }
        }
        co_return;
    }

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Apparently I am told that it’s been a long-believed myth that Asio “doesn’t do cancellation”. This is of course, 
nonsense.&lt;/p&gt;

&lt;p&gt;Here’s the code for &lt;a href=&quot;https://github.com/madmongo1/blog-october-2020/tree/stage-2&quot;&gt;stage 2&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;state-of-the-art&quot;&gt;State of the Art&lt;/h2&gt;

&lt;p&gt;It’s worth mentioning that I wrote and tested this demo using clang-9 and the libc++ version of the standard library. 
I have also successfully tested clang-11 with coroutines (and concepts). As I understand it, recent versions of 
Visual Studio support both well. GCC 10 - although advertising support for coroutines - has given me trouble, exhibiting 
segfaults at run time.&lt;/p&gt;

&lt;p&gt;Apple Clang, of course, is as always well behind the curve with no support for coroutines. If you want to try this code 
on a mac, it’s entirely possible as long as you ditch the Apple compiler and use the homebrew’s clang:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;brew install llvm
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Clang will then be available in &lt;code&gt;/usr/local/opt/bin&lt;/code&gt; and you will need to set your &lt;code&gt;CMAKE_CXX_COMPILER&lt;/code&gt; CMake variable
appropriately. For completeness, it’s worth mentioning that I also installed Qt5 using homebrew. You will need to
set &lt;code&gt;Qt5_DIR&lt;/code&gt;. Something like this:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt; cmake -H. -Bmy_build_dir -DCMAKE_CXX_COMPILER=/usr/local/opt/llvm/clang++ -DQt5_DIR=/usr/local/opt/qt5/lib/cmake/Qt5
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&quot;going-further&quot;&gt;Going further&lt;/h3&gt;

&lt;p&gt;Ok, so what if we want our Qt application to interact with some asio-based service running in another thread?&lt;/p&gt;

&lt;p&gt;For this I’m going to create a few boilerplate classes. The reason is that we’re going to have multiple threads running
and each thread is going to be executing multiple coroutines. Each coroutine has an associated executor and that 
executor is dispatching completion handlers (which for our purposes advance the progress of the coroutines) in one of
the threads assigned to it.&lt;/p&gt;

&lt;p&gt;It is important that coroutines are able to synchronise with each other, similar to the way that threads synchronise
with each other.&lt;/p&gt;

&lt;p&gt;In fact, it’s reasonable to use the mental model that a coroutine is a kind of “thread”.&lt;/p&gt;

&lt;p&gt;In standard C++, we have the class &lt;code&gt;std::condition_variable&lt;/code&gt; which we can wait on for some condition to be fulfilled.
If we were to produce a similar class for coroutines, then coroutines could co_await on each other. This could form the 
basis of an asynchronous event queue.&lt;/p&gt;

&lt;p&gt;First the condition_variable, implemented in terms of cancellation of an Asio timer to indicate readiness (thanks
to Chris Kohlhoff - the author of Asio - for suggesting this and saving me having reach for another library or worse, 
write my own awaitable type!):&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-cpp&quot;&gt;struct async_condition_variable
{
private:
    using timer_type = net::high_resolution_timer;

public:
    using clock_type = timer_type::clock_type;
    using duration = timer_type::duration;
    using time_point = timer_type::time_point;
    using executor_type = timer_type::executor_type;

    /// Constructor
    /// @param exec is the executor to associate with the internal timer.
    explicit inline async_condition_variable(net::any_io_executor exec);

    template&amp;lt;class Pred&amp;gt;
    [[nodiscard]]
    auto
    wait(Pred pred) -&amp;gt; net::awaitable&amp;lt;void&amp;gt;;

    template&amp;lt;class Pred&amp;gt;
    [[nodiscard]]
    auto
    wait_until(Pred pred, time_point limit) -&amp;gt; net::awaitable&amp;lt;std::cv_status&amp;gt;;

    template&amp;lt;class Pred&amp;gt;
    [[nodiscard]]
    auto
    wait_for(Pred pred, duration d) -&amp;gt; net::awaitable&amp;lt;std::cv_status&amp;gt;;

    auto
    get_executor() noexcept -&amp;gt; executor_type
    {
        return timer_.get_executor();
    }

    inline void
    notify_one();

    inline void
    notify_all();

    /// Put the condition into a stop state so that all future awaits fail.
    inline void
    stop();

    auto
    error() const -&amp;gt; error_code const &amp;amp;
    {
        return error_;
    }

    void
    reset()
    {
        error_ = {};
    }

private:
    timer_type timer_;
    error_code error_;
    std::multiset&amp;lt;timer_type::time_point&amp;gt; wait_times_;
};

template&amp;lt;class Pred&amp;gt;
auto
async_condition_variable::wait_until(Pred pred, time_point limit)
    -&amp;gt; net::awaitable&amp;lt;std::cv_status&amp;gt;
{
    assert(co_await net::this_coro::executor == timer_.get_executor());

    while (not error_ and not pred())
    {
        if (auto now = clock_type::now(); now &amp;gt;= limit)
            co_return std::cv_status::timeout;

        // insert our expiry time into the set and remember where it is
        auto where = wait_times_.insert(limit);

        // find the nearest expiry time and set the timeout for that one
        auto when = *wait_times_.begin();
        if (timer_.expiry() != when)
            timer_.expires_at(when);

        // wait for timeout or cancellation
        error_code ec;
        co_await timer_.async_wait(net::redirect_error(net::use_awaitable, ec));

        // remove our expiry time from the set
        wait_times_.erase(where);

        // any error other than operation_aborted is unexpected
        if (ec and ec != net::error::operation_aborted)
            if (not error_)
                error_ = ec;
    }

    if (error_)
        throw system_error(error_);

    co_return std::cv_status::no_timeout;
}

template&amp;lt;class Pred&amp;gt;
auto
async_condition_variable::wait(Pred pred) -&amp;gt; net::awaitable&amp;lt;void&amp;gt;
{
    auto stat = co_await wait_until(std::move(pred), time_point::max());
    boost::ignore_unused(stat);
    co_return;
}

template&amp;lt;class Pred&amp;gt;
auto
async_condition_variable::wait_for(Pred pred, duration d)
    -&amp;gt; net::awaitable&amp;lt;std::cv_status&amp;gt;
{
    return wait_until(std::move(pred), clock_type::now() + d);
}

async_condition_variable::async_condition_variable(net::any_io_executor exec)
    : timer_(std::move(exec))
    , error_()
{}

void
async_condition_variable::notify_one()
{
    timer_.cancel_one();
}

void
async_condition_variable::notify_all()
{
    timer_.cancel();
}

void
async_condition_variable::stop()
{
    error_ = net::error::operation_aborted;
    notify_all();
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;For our purposes this one is a little too all-singing and all-dancing as it allows for timed waits from multiple
coroutines. This is not needed in our example, but I happened to have the code handy from previous experiments.
You will notice that I have marked the coroutines as &lt;code&gt;[[nodiscard]]&lt;/code&gt;. This is to ensure that I don’t forget to 
&lt;code&gt;co_await&lt;/code&gt; them at the call site. I can’t tell you how many times I have done that and then wondered why my program
mysteriously freezes mid run.&lt;/p&gt;

&lt;p&gt;Having built the condition_variable, we now need some kind of waitable queue. I have implemented this in terms of some
shared state which contains an  &lt;code&gt;async_condition_variable&lt;/code&gt; and some kind of queue. I have made the implementation of the 
queue a template function (another over-complication for our purposes). The template represents the strategy for 
accumulating messages before they have been consumed by the client. The strategy I have used here is a FIFO, which means 
that every message posted will be consumed in the order in which they were posted. But it could just as easily be a 
priority queue, or a latch - i.e. only storing the most recent message.&lt;/p&gt;

&lt;p&gt;The code to describe this machinery is a little long to put inline, but by all means look at the code:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/madmongo1/blog-october-2020/blob/stage-3/src/basic_connection.hpp&quot;&gt;basic_connection&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/madmongo1/blog-october-2020/blob/stage-3/src/basic_distributor.hpp&quot;&gt;basic_distributor&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/madmongo1/blog-october-2020/blob/stage-3/src/basic_shared_state.hpp&quot;&gt;basic_shared_state&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The next piece of machinery we need is the actual service that will be delivering messages. The code is more-or-less
a copy/paste of the code that was in our widget because it’s doing the same job - delivering messages, but this time
via the basic_distributor.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/madmongo1/blog-october-2020/blob/stage-3/src/message_service.hpp&quot;&gt;message_service.hpp&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/madmongo1/blog-october-2020/blob/stage-3/src/message_service.cpp&quot;&gt;message_service.cpp&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Note that the message_service class is a pimpl. Although it uses a shared_ptr to hold the impl’s lifetime, it is itself
non-copyable. When the message_service is destroyed, it will signal its impl to stop. The impl will last a little longer
than the handle, while it shuts itself down.&lt;/p&gt;

&lt;p&gt;The main coroutine on the impl is called &lt;code&gt;run()&lt;/code&gt; and it is initiated when the impl is created:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-cpp&quot;&gt;message_service::message_service(const executor_type &amp;amp;exec)
    : exec_(exec)
    , impl_(std::make_shared&amp;lt;message_service_impl&amp;gt;(exec_))
{
    net::co_spawn(
        impl_-&amp;gt;get_executor(),
        [impl = impl_]() -&amp;gt; net::awaitable&amp;lt;void&amp;gt; { co_await impl-&amp;gt;run(); },
        net::detached);
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Note that the &lt;code&gt;impl&lt;/code&gt; shared_ptr has been captured in the lambda. Normally we’d need to be careful here because the
lambda is just a class who’s &lt;code&gt;operator()&lt;/code&gt; happens to be a coroutine. This means that the actual coroutine can outlive the
lambda that initiated it, which means that &lt;code&gt;impl&lt;/code&gt; could be destroyed before the coroutine finishes. For this reason
it’s generally safer to pass the impl to the coroutine as an argument, so that it gets decay_copied into the 
coroutine state.
However, in this case we’re safe. &lt;code&gt;net::co_spawn&lt;/code&gt; will actually copy the lambda object before invoking it, guaranteeing&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;with asio at least - that the impl will survive the execution of the coroutine.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;And here’s the &lt;code&gt;run()&lt;/code&gt; coroutine:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-cpp&quot;&gt;net::awaitable&amp;lt;void&amp;gt;
message_service_impl::run()
{
    using namespace std::literals;

    auto timer
        = net::high_resolution_timer(co_await net::this_coro::executor);

    auto done = false;

    listen_for_stop([&amp;amp;] {
      done = true;
      timer.cancel();
    });

    while (!done)
    {
        for (int i = 0; i &amp;lt; 10 &amp;amp;&amp;amp; !done; ++i)
        {
            timer.expires_after(1s);
            auto ec = boost::system::error_code();
            co_await timer.async_wait(
                net::redirect_error(net::use_awaitable, ec));
            if (ec)
                break;
            message_dist_.notify_value(std::to_string(i + 1) + &quot; seconds&quot;);
        }

        for (int i = 10; i-- &amp;amp;&amp;amp; !done;)
        {
            timer.expires_after(250ms);
            auto ec = boost::system::error_code();
            co_await timer.async_wait(
                net::redirect_error(net::use_awaitable, ec));
            if (ec)
                break;
            message_dist_.notify_value(std::to_string(i));
        }
    }
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Notice the &lt;code&gt;done&lt;/code&gt; machinery allowing detection of a stop event. Remember that a stop event can arrive at any time. The
first this coroutine will hear of it is when one of the timer &lt;code&gt;async_wait&lt;/code&gt; calls is canceled. Note that the lambda
passed to &lt;code&gt;listen_for_stop&lt;/code&gt; &lt;em&gt;is not actually part of the coroutine&lt;/em&gt;. It is a separate function that just happens to 
refer to the same state that the coroutine refers to. The communication between the two is via the timer cancellation
and the &lt;code&gt;done&lt;/code&gt; flag. This communication is guaranteed not to race because both the coroutine and the lambda are executed 
by the same &lt;code&gt;strand&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Finally we need to modify the widget:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-cpp&quot;&gt;net::awaitable&amp;lt;void&amp;gt;
test_widget::run_demo()
{
    using namespace std::literals;

    auto service = message_service(ioexec_);
    auto conn = co_await service.connect();

    auto done = false;

    listen_for_stop([&amp;amp;] {
        done = true;
        conn.disconnect();
        service.reset();
    });

    while (!done)
    {
        auto message = co_await conn.consume();
        this-&amp;gt;setText(QString::fromStdString(message));
    }
    co_return;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This coroutine will exit via exception when the distributor feeding the connection is destroyed. This will happen when
the impl of the service is destroyed.&lt;/p&gt;

&lt;p&gt;Here is the final code for &lt;a href=&quot;https://github.com/madmongo1/blog-october-2020/tree/stage-3&quot;&gt;stage 3&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;I’ve covered quite a few topics here and I hope this has been useful and interesting for people interested in exploring
coroutines and the think-async mindset.&lt;/p&gt;

&lt;p&gt;There are a number of things I have not covered, the most important of which is improving the (currently very basic)
&lt;code&gt;qt_guarded_executor&lt;/code&gt; to improve its performance. At the present time, whether you call &lt;code&gt;dispatch&lt;/code&gt; or &lt;code&gt;post&lt;/code&gt; referencing
this executor type, a post will actually be performed. Perhaps next month I’ll revisit and add the extra machinery to
allow &lt;code&gt;net::dispatch(e, f)&lt;/code&gt; to offer straight-through execution if we’re already on the correct Qt thread.&lt;/p&gt;

&lt;p&gt;If you have any questions or suggestions I’m happy to hear them. You can generally find me in the &lt;code&gt;#beast&lt;/code&gt; channel 
on &lt;a href=&quot;https://cppalliance.org/slack/&quot;&gt;cpplang slack&lt;/a&gt; or if you prefer you can either email &lt;a href=&quot;mailto:hodges.r@gmail.com&quot;&gt;me&lt;/a&gt;
or create an issue on &lt;a href=&quot;https://github.com/madmongo1/blog-october-2020/issues&quot;&gt;this repo&lt;/a&gt;.&lt;/p&gt;</content><author><name></name></author><category term="richard" /><summary type="html">Asio Coroutines in Qt applications! I started this train of thought when I wanted to hook up some back-end style code that I had written to a gui front end. One way to do this would be to have a web front end subscribing to a back-end service, but I am no expert in modern web technologies so rather than spend time learning something that wasn’t C++ I decided to reach for the popular-but-so-far-unused-by-me C++ GUI framework, Qt. The challenge was how to hook up Qt, which is an event driven framework to a service written with Asio C++ coroutines. In the end it turned out to be easier than I had expected. Here’s how. A simple Executor As mentioned in a previous blog, Asio comes with a full implementation of the Unified Executors proposal. Asio coroutines are designed to be initiated and continued within an executor’s execution context. So let’s build an executor that will perform work in a Qt UI thread. The executor I am going to build will have to invoke completion handlers to Asio IO objects, so we need to make it compatible with asio::any_io_executor. This means it needs to have an associated execution context. The execution context is going to ultimately perform work on a Qt Application, so it makes sense to capture a reference to the Application. Although Qt defines the macro qApp to resolve to a pointer to the “current” application, for testing and sanity purposes I prefer that all services I write allow dependency injection, so I’ll arrange things so that the execution_context’s constructor takes an optional pointer to an application. In addition, it will be convenient when writing components to not have to specifically create and pass an an execution context to windows within the Qt application so it makes sense to be able to provide access to a default context which references the default application. Here’s a first cut: struct qt_execution_context : net::execution_context , boost::noncopyable { qt_execution_context(QApplication *app = qApp) : app_(app) { instance_ = this; } template&amp;lt;class F&amp;gt; void post(F f) { // todo } static qt_execution_context &amp;amp; singleton() { assert(instance_); return *instance_; } private: static qt_execution_context *instance_; QApplication *app_; }; This class will provide two services. The first is to provide the asio service infrastructure so that we can create timers, sockets etc that use executors associated with this context and the second is to allow the executor to actually dispatch work in a Qt application. This is the purpose of the post method. Now a Qt application is itself a kind of execution context - in that it dispatches QEvent objects to be handled by children of the application. We can use this infrastructure to ensure that work dispatched by this execution context actually takes place on the correct thread and at the correct time. In order for us to dispatch work to the application, we need to wrap our function into a QEvent: class qt_work_event_base : public QEvent { public: qt_work_event_base() : QEvent(generated_type()) { } virtual void invoke() = 0; static QEvent::Type generated_type() { static int event_type = QEvent::registerEventType(); return static_cast&amp;lt;QEvent::Type&amp;gt;(event_type); } }; template&amp;lt;class F&amp;gt; struct basic_qt_work_event : qt_work_event_base { basic_qt_work_event(F f) : f_(std::move(f)) {} void invoke() override { f_(); } private: F f_; }; As opposed to using a std::function, the basic_qt_work_event allows us to wrap a move-only function object, which is important when that object is actually an Asio completion handler. Completion handlers benefit from being move-only as it means they can carry move-only state. This makes them more versatile, and can often lead to improvements in execution performance. Now we just need to fill out the code for qt_execution_context::post and provide a mechanism in the Qt application to detect and dispatch these messages: template&amp;lt;class F&amp;gt; void post(F f) { // c++20 auto template deduction auto event = new basic_qt_work_event(std::move(f)); QApplication::postEvent(app_, event); } class qt_net_application : public QApplication { using QApplication::QApplication; protected: bool event(QEvent *event) override; }; bool qt_net_application::event(QEvent *event) { if (event-&amp;gt;type() == qt_work_event_base::generated_type()) { auto p = static_cast&amp;lt;qt_work_event_base*&amp;gt;(event); p-&amp;gt;accept(); p-&amp;gt;invoke(); return true; } else { return QApplication::event(event); } } Note that I have seen on stack overflow the technique of invoking a function object in the destructor of the QEvent-derived event. This would mean no necessity of custom event handling in the QApplication but there are two problems that I can see with this approach: I don’t know enough about Qt to know that this is safe and correct, and Executors-TS executors can be destroyed while there are still un-invoked handlers within them. The correct behaviour is to destroy these handlers without invoking them. If we put invocation code in the destructors, they will actually mass-invoke when the executor is destroyed, leading most probably to annihilation of our program by segfault. However, that being done, we can now write the executor to meet the minimal expectations of an asio executor which can be used in an any_io_executor. struct qt_executor { qt_executor(qt_execution_context &amp;amp;context = qt_execution_context::singleton()) noexcept : context_(std::addressof(context)) { } qt_execution_context &amp;amp;query(net::execution::context_t) const noexcept { return *context_; } static constexpr net::execution::blocking_t query(net::execution::blocking_t) noexcept { return net::execution::blocking.never; } static constexpr net::execution::relationship_t query(net::execution::relationship_t) noexcept { return net::execution::relationship.fork; } static constexpr net::execution::outstanding_work_t query(net::execution::outstanding_work_t) noexcept { return net::execution::outstanding_work.tracked; } template &amp;lt; typename OtherAllocator &amp;gt; static constexpr auto query( net::execution::allocator_t&amp;lt; OtherAllocator &amp;gt;) noexcept { return std::allocator&amp;lt;void&amp;gt;(); } static constexpr auto query(net::execution::allocator_t&amp;lt; void &amp;gt;) noexcept { return std::allocator&amp;lt;void&amp;gt;(); } template&amp;lt;class F&amp;gt; void execute(F f) const { context_-&amp;gt;post(std::move(f)); } bool operator==(qt_executor const &amp;amp;other) const noexcept { return context_ == other.context_; } bool operator!=(qt_executor const &amp;amp;other) const noexcept { return !(*this == other); } private: qt_execution_context *context_; }; static_assert(net::execution::is_executor_v&amp;lt;qt_executor&amp;gt;); Now all that remains is to write a subclass of some Qt Widget so that we can dispatch some work against it. class test_widget : public QTextEdit { Q_OBJECT public: using QTextEdit::QTextEdit; private: void showEvent(QShowEvent *event) override; void hideEvent(QHideEvent *event) override; net::awaitable&amp;lt;void&amp;gt; run_demo(); }; void test_widget::showEvent(QShowEvent *event) { net::co_spawn( qt_executor(), [this] { return run_demo(); }, net::detached); QTextEdit::showEvent(event); } void test_widget::hideEvent(QHideEvent *event) { QWidget::hideEvent(event); } net::awaitable&amp;lt;void&amp;gt; test_widget::run_demo() { using namespace std::literals; auto timer = net::high_resolution_timer(co_await net::this_coro::executor); for (int i = 0; i &amp;lt; 10; ++i) { timer.expires_after(1s); co_await timer.async_wait(net::use_awaitable); this-&amp;gt;setText(QString::fromStdString(std::to_string(i + 1) + &quot; seconds&quot;)); } co_return; } Here is the code for stage 1 And here is a screenshot of the app running: All very well… OK, so we have a coroutine running in a Qt application. This is nice because it allows us to express an event-driven system in terms of procedural expression of code in a coroutine. But what if the user closes the window before the coroutine completes? This application has created the window on the stack, but in a larger application, there will be multiple windows and they may open and close at any time. It is not unusual in Qt to delete a closed window. If the coroutine continues to run once the windows that’s hosting it is deleted, we are sure to get a segfault. One answer to this is to maintain a sentinel in the Qt widget implementation, which prevents the continuation of the coroutine if destroyed. A std::shared_ptr/weak_ptr pair would seem like a sensible solution. Let’s create an updated version of the executor: struct qt_guarded_executor { qt_guarded_executor(std::weak_ptr&amp;lt;void&amp;gt; guard, qt_execution_context &amp;amp;context = qt_execution_context::singleton()) noexcept : context_(std::addressof(context)) , guard_(std::move(guard)) {} qt_execution_context &amp;amp;query(net::execution::context_t) const noexcept { return *context_; } static constexpr net::execution::blocking_t query(net::execution::blocking_t) noexcept { return net::execution::blocking.never; } static constexpr net::execution::relationship_t query(net::execution::relationship_t) noexcept { return net::execution::relationship.fork; } static constexpr net::execution::outstanding_work_t query(net::execution::outstanding_work_t) noexcept { return net::execution::outstanding_work.tracked; } template&amp;lt;typename OtherAllocator&amp;gt; static constexpr auto query(net::execution::allocator_t&amp;lt;OtherAllocator&amp;gt;) noexcept { return std::allocator&amp;lt;void&amp;gt;(); } static constexpr auto query(net::execution::allocator_t&amp;lt;void&amp;gt;) noexcept { return std::allocator&amp;lt;void&amp;gt;(); } template&amp;lt;class F&amp;gt; void execute(F f) const { if (auto lock1 = guard_.lock()) { context_-&amp;gt;post([guard = guard_, f = std::move(f)]() mutable { if (auto lock2 = guard.lock()) f(); }); } } bool operator==(qt_guarded_executor const &amp;amp;other) const noexcept { return context_ == other.context_ &amp;amp;&amp;amp; !guard_.owner_before(other.guard_) &amp;amp;&amp;amp; !other.guard_.owner_before(guard_); } bool operator!=(qt_guarded_executor const &amp;amp;other) const noexcept { return !(*this == other); } private: qt_execution_context *context_; std::weak_ptr&amp;lt;void&amp;gt; guard_; }; Now we’ll make a little boilerplate class that we can use as a base class in any executor-enabled object in Qt: struct has_guarded_executor { using executor_type = qt_guarded_executor; has_guarded_executor(qt_execution_context &amp;amp;ctx = qt_execution_context::singleton()) : context_(std::addressof(ctx)) { new_guard(); } void new_guard() { static int x = 0; guard_ = std::shared_ptr&amp;lt;int&amp;gt;(std::addressof(x), // no-op deleter [](auto *) {}); } void reset_guard() { guard_.reset(); } executor_type get_executor() const { return qt_guarded_executor(guard_, *context_); } private: qt_execution_context *context_; std::shared_ptr&amp;lt;void&amp;gt; guard_; }; And we can modify the test_widget to use it: class test_widget : public QTextEdit , public has_guarded_executor { ... }; void test_widget::showEvent(QShowEvent *event) { // stop all existing coroutines and create a new guard new_guard(); // start our coroutine net::co_spawn( get_executor(), [this] { return run_demo(); }, net::detached); QTextEdit::showEvent(event); } void test_widget::hideEvent(QHideEvent *event) { // stop all coroutines reset_guard(); QWidget::hideEvent(event); } Now we’ll update the application to allow the creation and deletion of our widget. For this I’ll use the QMdiWindow and add a menu with an action to create new widgets. We are now able to create and destroy widgets at will, with no segfaults. If you look at the code, you’ll also see that I’ve wired up a rudimentary signal/slot device to allow the coroutine to be cancelled early. // test_widget.hpp void listen_for_stop(std::function&amp;lt;void()&amp;gt; slot); void stop_all(); std::vector&amp;lt;std::function&amp;lt;void()&amp;gt;&amp;gt; stop_signals_; bool stopped_ = false; // test_widget.cpp void test_widget::listen_for_stop(std::function&amp;lt;void()&amp;gt; slot) { if (stopped_) return slot(); stop_signals_.push_back(std::move(slot)); } void test_widget::stop_all() { stopped_ = true; auto copy = std::exchange(stop_signals_, {}); for (auto &amp;amp;slot : copy) slot(); } void test_widget::closeEvent(QCloseEvent *event) { stop_all(); QWidget::closeEvent(event); } net::awaitable&amp;lt;void&amp;gt; test_widget::run_demo() { using namespace std::literals; auto timer = net::high_resolution_timer(co_await net::this_coro::executor); auto done = false; listen_for_stop([&amp;amp;] { done = true; timer.cancel(); }); while (!done) { for (int i = 0; i &amp;lt; 10; ++i) { timer.expires_after(1s); auto ec = boost::system::error_code(); co_await timer.async_wait( net::redirect_error(net::use_awaitable, ec)); if (ec) { done = true; break; } this-&amp;gt;setText( QString::fromStdString(std::to_string(i + 1) + &quot; seconds&quot;)); } for (int i = 10; i--;) { timer.expires_after(250ms); auto ec = boost::system::error_code(); co_await timer.async_wait( net::redirect_error(net::use_awaitable, ec)); if (ec) { done = true; break; } this-&amp;gt;setText(QString::fromStdString(std::to_string(i))); } } co_return; } Apparently I am told that it’s been a long-believed myth that Asio “doesn’t do cancellation”. This is of course, nonsense. Here’s the code for stage 2 State of the Art It’s worth mentioning that I wrote and tested this demo using clang-9 and the libc++ version of the standard library. I have also successfully tested clang-11 with coroutines (and concepts). As I understand it, recent versions of Visual Studio support both well. GCC 10 - although advertising support for coroutines - has given me trouble, exhibiting segfaults at run time. Apple Clang, of course, is as always well behind the curve with no support for coroutines. If you want to try this code on a mac, it’s entirely possible as long as you ditch the Apple compiler and use the homebrew’s clang: brew install llvm Clang will then be available in /usr/local/opt/bin and you will need to set your CMAKE_CXX_COMPILER CMake variable appropriately. For completeness, it’s worth mentioning that I also installed Qt5 using homebrew. You will need to set Qt5_DIR. Something like this: cmake -H. -Bmy_build_dir -DCMAKE_CXX_COMPILER=/usr/local/opt/llvm/clang++ -DQt5_DIR=/usr/local/opt/qt5/lib/cmake/Qt5 Going further Ok, so what if we want our Qt application to interact with some asio-based service running in another thread? For this I’m going to create a few boilerplate classes. The reason is that we’re going to have multiple threads running and each thread is going to be executing multiple coroutines. Each coroutine has an associated executor and that executor is dispatching completion handlers (which for our purposes advance the progress of the coroutines) in one of the threads assigned to it. It is important that coroutines are able to synchronise with each other, similar to the way that threads synchronise with each other. In fact, it’s reasonable to use the mental model that a coroutine is a kind of “thread”. In standard C++, we have the class std::condition_variable which we can wait on for some condition to be fulfilled. If we were to produce a similar class for coroutines, then coroutines could co_await on each other. This could form the basis of an asynchronous event queue. First the condition_variable, implemented in terms of cancellation of an Asio timer to indicate readiness (thanks to Chris Kohlhoff - the author of Asio - for suggesting this and saving me having reach for another library or worse, write my own awaitable type!): struct async_condition_variable { private: using timer_type = net::high_resolution_timer; public: using clock_type = timer_type::clock_type; using duration = timer_type::duration; using time_point = timer_type::time_point; using executor_type = timer_type::executor_type; /// Constructor /// @param exec is the executor to associate with the internal timer. explicit inline async_condition_variable(net::any_io_executor exec); template&amp;lt;class Pred&amp;gt; [[nodiscard]] auto wait(Pred pred) -&amp;gt; net::awaitable&amp;lt;void&amp;gt;; template&amp;lt;class Pred&amp;gt; [[nodiscard]] auto wait_until(Pred pred, time_point limit) -&amp;gt; net::awaitable&amp;lt;std::cv_status&amp;gt;; template&amp;lt;class Pred&amp;gt; [[nodiscard]] auto wait_for(Pred pred, duration d) -&amp;gt; net::awaitable&amp;lt;std::cv_status&amp;gt;; auto get_executor() noexcept -&amp;gt; executor_type { return timer_.get_executor(); } inline void notify_one(); inline void notify_all(); /// Put the condition into a stop state so that all future awaits fail. inline void stop(); auto error() const -&amp;gt; error_code const &amp;amp; { return error_; } void reset() { error_ = {}; } private: timer_type timer_; error_code error_; std::multiset&amp;lt;timer_type::time_point&amp;gt; wait_times_; }; template&amp;lt;class Pred&amp;gt; auto async_condition_variable::wait_until(Pred pred, time_point limit) -&amp;gt; net::awaitable&amp;lt;std::cv_status&amp;gt; { assert(co_await net::this_coro::executor == timer_.get_executor()); while (not error_ and not pred()) { if (auto now = clock_type::now(); now &amp;gt;= limit) co_return std::cv_status::timeout; // insert our expiry time into the set and remember where it is auto where = wait_times_.insert(limit); // find the nearest expiry time and set the timeout for that one auto when = *wait_times_.begin(); if (timer_.expiry() != when) timer_.expires_at(when); // wait for timeout or cancellation error_code ec; co_await timer_.async_wait(net::redirect_error(net::use_awaitable, ec)); // remove our expiry time from the set wait_times_.erase(where); // any error other than operation_aborted is unexpected if (ec and ec != net::error::operation_aborted) if (not error_) error_ = ec; } if (error_) throw system_error(error_); co_return std::cv_status::no_timeout; } template&amp;lt;class Pred&amp;gt; auto async_condition_variable::wait(Pred pred) -&amp;gt; net::awaitable&amp;lt;void&amp;gt; { auto stat = co_await wait_until(std::move(pred), time_point::max()); boost::ignore_unused(stat); co_return; } template&amp;lt;class Pred&amp;gt; auto async_condition_variable::wait_for(Pred pred, duration d) -&amp;gt; net::awaitable&amp;lt;std::cv_status&amp;gt; { return wait_until(std::move(pred), clock_type::now() + d); } async_condition_variable::async_condition_variable(net::any_io_executor exec) : timer_(std::move(exec)) , error_() {} void async_condition_variable::notify_one() { timer_.cancel_one(); } void async_condition_variable::notify_all() { timer_.cancel(); } void async_condition_variable::stop() { error_ = net::error::operation_aborted; notify_all(); } For our purposes this one is a little too all-singing and all-dancing as it allows for timed waits from multiple coroutines. This is not needed in our example, but I happened to have the code handy from previous experiments. You will notice that I have marked the coroutines as [[nodiscard]]. This is to ensure that I don’t forget to co_await them at the call site. I can’t tell you how many times I have done that and then wondered why my program mysteriously freezes mid run. Having built the condition_variable, we now need some kind of waitable queue. I have implemented this in terms of some shared state which contains an async_condition_variable and some kind of queue. I have made the implementation of the queue a template function (another over-complication for our purposes). The template represents the strategy for accumulating messages before they have been consumed by the client. The strategy I have used here is a FIFO, which means that every message posted will be consumed in the order in which they were posted. But it could just as easily be a priority queue, or a latch - i.e. only storing the most recent message. The code to describe this machinery is a little long to put inline, but by all means look at the code: basic_connection basic_distributor basic_shared_state The next piece of machinery we need is the actual service that will be delivering messages. The code is more-or-less a copy/paste of the code that was in our widget because it’s doing the same job - delivering messages, but this time via the basic_distributor. message_service.hpp message_service.cpp Note that the message_service class is a pimpl. Although it uses a shared_ptr to hold the impl’s lifetime, it is itself non-copyable. When the message_service is destroyed, it will signal its impl to stop. The impl will last a little longer than the handle, while it shuts itself down. The main coroutine on the impl is called run() and it is initiated when the impl is created: message_service::message_service(const executor_type &amp;amp;exec) : exec_(exec) , impl_(std::make_shared&amp;lt;message_service_impl&amp;gt;(exec_)) { net::co_spawn( impl_-&amp;gt;get_executor(), [impl = impl_]() -&amp;gt; net::awaitable&amp;lt;void&amp;gt; { co_await impl-&amp;gt;run(); }, net::detached); } Note that the impl shared_ptr has been captured in the lambda. Normally we’d need to be careful here because the lambda is just a class who’s operator() happens to be a coroutine. This means that the actual coroutine can outlive the lambda that initiated it, which means that impl could be destroyed before the coroutine finishes. For this reason it’s generally safer to pass the impl to the coroutine as an argument, so that it gets decay_copied into the coroutine state. However, in this case we’re safe. net::co_spawn will actually copy the lambda object before invoking it, guaranteeing with asio at least - that the impl will survive the execution of the coroutine. And here’s the run() coroutine: net::awaitable&amp;lt;void&amp;gt; message_service_impl::run() { using namespace std::literals; auto timer = net::high_resolution_timer(co_await net::this_coro::executor); auto done = false; listen_for_stop([&amp;amp;] { done = true; timer.cancel(); }); while (!done) { for (int i = 0; i &amp;lt; 10 &amp;amp;&amp;amp; !done; ++i) { timer.expires_after(1s); auto ec = boost::system::error_code(); co_await timer.async_wait( net::redirect_error(net::use_awaitable, ec)); if (ec) break; message_dist_.notify_value(std::to_string(i + 1) + &quot; seconds&quot;); } for (int i = 10; i-- &amp;amp;&amp;amp; !done;) { timer.expires_after(250ms); auto ec = boost::system::error_code(); co_await timer.async_wait( net::redirect_error(net::use_awaitable, ec)); if (ec) break; message_dist_.notify_value(std::to_string(i)); } } } Notice the done machinery allowing detection of a stop event. Remember that a stop event can arrive at any time. The first this coroutine will hear of it is when one of the timer async_wait calls is canceled. Note that the lambda passed to listen_for_stop is not actually part of the coroutine. It is a separate function that just happens to refer to the same state that the coroutine refers to. The communication between the two is via the timer cancellation and the done flag. This communication is guaranteed not to race because both the coroutine and the lambda are executed by the same strand. Finally we need to modify the widget: net::awaitable&amp;lt;void&amp;gt; test_widget::run_demo() { using namespace std::literals; auto service = message_service(ioexec_); auto conn = co_await service.connect(); auto done = false; listen_for_stop([&amp;amp;] { done = true; conn.disconnect(); service.reset(); }); while (!done) { auto message = co_await conn.consume(); this-&amp;gt;setText(QString::fromStdString(message)); } co_return; } This coroutine will exit via exception when the distributor feeding the connection is destroyed. This will happen when the impl of the service is destroyed. Here is the final code for stage 3. I’ve covered quite a few topics here and I hope this has been useful and interesting for people interested in exploring coroutines and the think-async mindset. There are a number of things I have not covered, the most important of which is improving the (currently very basic) qt_guarded_executor to improve its performance. At the present time, whether you call dispatch or post referencing this executor type, a post will actually be performed. Perhaps next month I’ll revisit and add the extra machinery to allow net::dispatch(e, f) to offer straight-through execution if we’re already on the correct Qt thread. If you have any questions or suggestions I’m happy to hear them. You can generally find me in the #beast channel on cpplang slack or if you prefer you can either email me or create an issue on this repo.</summary></entry><entry><title type="html">Richard’s September Update</title><link href="http://cppalliance.org/richard/2020/09/30/RichardsSeptemberUpdate.html" rel="alternate" type="text/html" title="Richard’s September Update" /><published>2020-09-30T00:00:00+00:00</published><updated>2020-09-30T00:00:00+00:00</updated><id>http://cppalliance.org/richard/2020/09/30/RichardsSeptemberUpdate</id><content type="html" xml:base="http://cppalliance.org/richard/2020/09/30/RichardsSeptemberUpdate.html">&lt;h1 id=&quot;cancellation-in-beastasio-and-better-compile-performance-with-beastwebsocket&quot;&gt;Cancellation in Beast/Asio and Better Compile Performance with Beast.Websocket&lt;/h1&gt;

&lt;p&gt;This month I will be discussing two issues. One of interest to many people who come to us with questions on the 
&lt;a href=&quot;https://github.com/boostorg/beast/issues&quot;&gt;Github Issue Tracker&lt;/a&gt; and the #beast channel of 
&lt;a href=&quot;https://cppalliance.org/slack/&quot;&gt;Cpplang Slack&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;compile-times-and-separation-of-concerns&quot;&gt;Compile Times and Separation of Concerns&lt;/h2&gt;

&lt;p&gt;A common complaint about Boost.Beast is that compilation units that use the &lt;code&gt;websocket::stream&lt;/code&gt; template class
often take a long time to compile, and that because websocket::stream is a template, this compilation overhead can
become viral in an application.&lt;/p&gt;

&lt;p&gt;This is a valid complaint and we believe there are some reasonable tradeoffs we can make by refactoring the websocket
stream to use fewer templates internally. Vinnie has started work to express the WebSocket’s 
intermediate completion handlers, buffer sequence and executor in terms of a polymorphic object. This would mean a 
few indirect jumps in the compiled code but would significantly reduce the number of internal template expansions.
In the scheme of things, we don’t believe that the virtual function calls will materially affect runtime performance.
The branch is &lt;a href=&quot;https://github.com/vinniefalco/beast/tree/async-an&quot;&gt;here&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;I will be continuing work in this area in the coming days.&lt;/p&gt;

&lt;p&gt;In the meantime, our general response is to suggest that users create a base class to handle the transport, and 
communicate important events such as frame received, connection state and the close notification to a derived 
application-layer class through a private polymorphic interface.&lt;/p&gt;

&lt;p&gt;In this way, the websocket transport compilation unit may take a while to compile, but it needs to be done only once
since the transport layer will rarely change during the development life of an application. Whenever there is a change
to the application layer, the transport layer is not affected so websocket-related code is not affected.&lt;/p&gt;

&lt;p&gt;This approach has a number of benefits. Not least of which is that developing another client implementation over 
a different websocket connection in the same application becomes trivial.&lt;/p&gt;

&lt;p&gt;Another benefit is that the application can be designed such that application-level concerns are agnostic of the 
transport mechanism. Such as when the server can be accessed by multiple means - WSS, WS, long poll, direct connection, 
unix sockets and so on.&lt;/p&gt;

&lt;p&gt;In this blog I will present a simplified implementation of this idea. My thanks to the cpplang Slack user &lt;code&gt;@elegracer&lt;/code&gt;
who most recently asked for guidance on reducing compile times. It was (his/her? Slack is silent on the matter) question
which prompted me to finally conjure up a demo. &lt;code&gt;@elegracer&lt;/code&gt;’s problem was needing to connect to multiple cryptocurrency
exchanges in the same app over websocket. In this particular example I’ll demonstrate a simplified connection to
the public FMex market data feed since that was the subject of the original question.&lt;/p&gt;

&lt;h2 id=&quot;correct-cancellation&quot;&gt;Correct Cancellation&lt;/h2&gt;

&lt;p&gt;Our examples in the Beast Repository are rudimentary and don’t cover the issue of graceful shutdown of an application
in response to a SIGINT (i.e. the user pressing ctrl-c). It is common for simple programs to exit suddenly in response
to this signal, which is the default behaviour. For many applications, this is perfectly fine but not all. We may want 
active objects in the program to write data to disk, we may want to ensure that the underlying websocket is 
shut down cleanly and we may want to give the user an opportunity to prevent the shutdown.&lt;/p&gt;

&lt;p&gt;I will further annotate the example by providing this ability to prevent the shutdown. The user will have to confirm the 
first SIGINT with another within 5 seconds to confirm.&lt;/p&gt;

&lt;h1 id=&quot;designing-the-application&quot;&gt;Designing the application&lt;/h1&gt;

&lt;p&gt;When I write IO applications involving Asio and Beast, I prefer to create an “application” object. This has the 
responsibility of monitoring signals and starting the initial connection objects. It also provides the communication
between the two.&lt;/p&gt;

&lt;p&gt;The construction and configuration of the &lt;code&gt;io_context&lt;/code&gt; and &lt;code&gt;ssl::context&lt;/code&gt; stay in &lt;code&gt;main()&lt;/code&gt;. The executor and ssl context
are passed to the application by reference as dependencies. The application can then pass on these refrences as 
required. It is also worth mentioning that I don’t pass the io_context’s executor as a polymorphic &lt;code&gt;any_io_executor&lt;/code&gt; 
type at this stage. The reason is that I may want in future to upgrade my program to be multi-threaded. If I do this, 
then each individual io_enabled object such as a connection or the application will need to have its &lt;em&gt;own&lt;/em&gt; strand.
Getting the strand out of an any_io_executor is not possible in the general case as it will have been type-erased, so 
for top level objects I pass the executor as &lt;code&gt;io_context::executor_type&lt;/code&gt;. It is then up to each object to create its own
strand internally which will have the type &lt;code&gt;strand&amp;lt;io_context::executor_type&amp;gt;&lt;/code&gt;. The &lt;code&gt;strand&lt;/code&gt; type provides the method
&lt;code&gt;get_inner_executor&lt;/code&gt; which allows the application to extract the underlying &lt;code&gt;io_context::executor_type&lt;/code&gt; and pass it to
the constructor of any subordinate but otherwise self-contained io objects. The subordinates can then build their own
strands from this.&lt;/p&gt;

&lt;h2 id=&quot;step-1---a-simple-application-framework-that-supports-ctrl-c&quot;&gt;Step 1 - A Simple Application Framework That Supports ctrl-c&lt;/h2&gt;

&lt;p&gt;OK, let’s get started and build the framework. Here’s a link to 
&lt;a href=&quot;https://github.com/test-scenarios/boost_beast_websocket_echo/tree/blog-2020-09-step-1/pre-cxx20/blog-2020-09&quot;&gt;step 1&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;code&gt;ssl.hpp&lt;/code&gt; and &lt;code&gt;net.hpp&lt;/code&gt; simply configure the project to use boost.asio. The idea of these little configuration headers
is that they could be generated by the cmake project if necessary to allow the option of upgrading to std networking
if it ever arrives.&lt;/p&gt;

&lt;p&gt;As a matter of style, I like to ensure that no names are created in the global namespace other than &lt;code&gt;main&lt;/code&gt;. This saves
headaches that could occur if I wrote code on one platform, but then happened to port it to another where the name
was already in use by the native system libraries.&lt;/p&gt;

&lt;p&gt;&lt;code&gt;main.cpp&lt;/code&gt; simply creates the io execution context and a default ssl context, creates the application, starts it and
runs the io context.&lt;/p&gt;

&lt;p&gt;At the moment, the only interesting part of our program is the &lt;code&gt;signit_state&lt;/code&gt;. This is a state machine which handles the
behaviour of the program when a &lt;code&gt;SIGINT&lt;/code&gt; is received. Our state machine is doing something a little fancy. Here is the 
state diagram:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/posts/richard/2020-09-sigint-state.png&quot; alt=&quot;sigint_state&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Rather than reproduce the code here, please refer to 
&lt;a href=&quot;https://github.com/test-scenarios/boost_beast_websocket_echo/tree/blog-2020-09-step-1/pre-cxx20/blog-2020-09&quot;&gt;step 1&lt;/a&gt; 
to see the source code.&lt;/p&gt;

&lt;p&gt;At this point the program will run and successfully handle ctrl-c:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ ./blog_2020_09 
Application starting
Press ctrl-c to interrupt.
^CInterrupt detected. Press ctrl-c again within 5 seconds to exit
Interrupt unconfirmed. Ignoring
^CInterrupt detected. Press ctrl-c again within 5 seconds to exit
^CInterrupt confirmed. Shutting down
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;step-2---connecting-to-an-exchange&quot;&gt;Step 2 - Connecting to an Exchange&lt;/h2&gt;

&lt;p&gt;Now we need to create our WebSocket transport class and our FMex exchange protocol class that will derive from it.
For now we won’t worry about cancellation - we’ll retrofit that in Step 3.&lt;/p&gt;

&lt;p&gt;Here is the code for 
&lt;a href=&quot;https://github.com/test-scenarios/boost_beast_websocket_echo/tree/blog-2020-09-step-2/pre-cxx20/blog-2020-09&quot;&gt;step 2&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;This section introduces two new main classes - the &lt;code&gt;wss_transport&lt;/code&gt; and the &lt;code&gt;fmex_connection&lt;/code&gt;. In addition, the connection
phase of the wss_transport is expressed as a composed operation for exposition purposes (and in my opinion it actually 
makes the code easier to read than continuation-passing style code)&lt;/p&gt;

&lt;p&gt;Here is the implementation of the connect coroutine:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-cpp&quot;&gt;    struct wss_transport::connect_op : asio::coroutine
    {
        using executor_type = wss_transport::executor_type;
        using websock       = wss_transport::websock;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here we define the &lt;em&gt;implementation&lt;/em&gt; of the coroutine - this is an object which will not be moved for the duration of the
execution of the coroutine. This address stability is important because intermediate asynchronous operations will rely
on knowing the address of the resolver (and later perhaps other io objects).&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-cpp&quot;&gt;        struct impl_data
        {
            impl_data(websock &amp;amp;   ws,
                      std::string host,
                      std::string port,
                      std::string target)
            : ws(ws)
            , resolver(ws.get_executor())
            , host(host)
            , port(port)
            , target(target)
            {
            }

            layer_0 &amp;amp;
            tcp_layer() const
            {
                return ws.next_layer().next_layer();
            }

            layer_1 &amp;amp;
            ssl_layer() const
            {
                return ws.next_layer();
            }

            websock &amp;amp;                            ws;
            net::ip::tcp::resolver               resolver;
            net::ip::tcp::resolver::results_type endpoints;
            std::string                          host, port, target;
        };
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The constructor merely forwards the arguments to the construction of the &lt;code&gt;impl_data&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-cpp&quot;&gt;        connect_op(websock &amp;amp;   ws,
                   std::string host,
                   std::string port,
                   std::string target)
        : impl_(std::make_unique&amp;lt; impl_data &amp;gt;(ws, host, port, target))
        {
        }

&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This coroutine is both a composed operation and a completion handler for sub-operations. This means it must have an
&lt;code&gt;operator()&lt;/code&gt; interface matching the requirements of each sub-operation. During the lifetime of this coroutine we 
will be using the resolver and calling &lt;code&gt;async_connect&lt;/code&gt; on the &lt;code&gt;tcp_stream&lt;/code&gt;. We therefore provide conforming member
functions which store or ignore the and forward the &lt;code&gt;error_code&lt;/code&gt; to the main implementation of the coroutine.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-cpp&quot;&gt;        template &amp;lt; class Self &amp;gt;
        void
        operator()(Self &amp;amp;                               self,
                   error_code                           ec,
                   net::ip::tcp::resolver::results_type results)
        {
            impl_-&amp;gt;endpoints = results;
            (*this)(self, ec);
        }

        template &amp;lt; class Self &amp;gt;
        void
        operator()(Self &amp;amp;self, error_code ec, net::ip::tcp::endpoint const &amp;amp;)
        {
            (*this)(self, ec);
        }
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here is the main implementation of the coroutine. Note that the last two parameters provide defaults. This is in order
to allow this member function to match the completion handler signatures of:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;code&gt;void()&lt;/code&gt; - invoked during async_compose in order to start the coroutine.&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;void(error_code)&lt;/code&gt; - invoked by the two functions above and by the async handshakes.&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;void(error_code, std::size_t)&lt;/code&gt; - invoked by operations such as async_read and async_write although not strictly 
necessary here.
    &lt;pre&gt;&lt;code class=&quot;language-cpp&quot;&gt;      template &amp;lt; class Self &amp;gt;
      void operator()(Self &amp;amp;self, error_code ec = {}, std::size_t = 0)
      {
&lt;/code&gt;&lt;/pre&gt;
    &lt;p&gt;Note that here we are checking the error code before re-entering the coroutine. This is a shortcut which allows us to
omit error checking after each sub-operation. This check will happen on every attempt to re-enter the coroutine, 
including the first entry (at which time &lt;code&gt;ec&lt;/code&gt; is guaranteed to be default constructed).&lt;/p&gt;
    &lt;pre&gt;&lt;code class=&quot;language-cpp&quot;&gt;          if (ec)
              return self.complete(ec);

          auto &amp;amp;impl = *impl_;
&lt;/code&gt;&lt;/pre&gt;
    &lt;p&gt;Note the use of the asio yield and unyield headers to create the fake ‘keywords’ &lt;code&gt;reenter&lt;/code&gt; and &lt;code&gt;yield&lt;/code&gt; in avery limited
scope.&lt;/p&gt;
    &lt;pre&gt;&lt;code class=&quot;language-cpp&quot;&gt;#include &amp;lt;boost/asio/yield.hpp&amp;gt;
          reenter(*this)
          {
              yield impl.resolver.async_resolve(
                  impl.host, impl.port, std::move(self));

              impl.tcp_layer().expires_after(15s);
              yield impl.tcp_layer().async_connect(impl.endpoints,
                                                   std::move(self));

              if (!SSL_set_tlsext_host_name(impl.ssl_layer().native_handle(),
                                            impl.host.c_str()))
                  return self.complete(
                      error_code(static_cast&amp;lt; int &amp;gt;(::ERR_get_error()),
                                 net::error::get_ssl_category()));

              impl.tcp_layer().expires_after(15s);
              yield impl.ssl_layer().async_handshake(ssl::stream_base::client,
                                                     std::move(self));

              impl.tcp_layer().expires_after(15s);
              yield impl.ws.async_handshake(
                  impl.host, impl.target, std::move(self));
&lt;/code&gt;&lt;/pre&gt;
    &lt;p&gt;If the coroutine is re-entered here, it must be because there was no error (if there was an error, it would have been
caught by the pre-reentry error check above). Since execution has resumed here in the completion handler of the 
&lt;code&gt;async_handshake&lt;/code&gt; initiating function, we are guaranteed to be executing in the correct executor. Therefore we can
simply call &lt;code&gt;complete&lt;/code&gt; directly without needing to post to an executor. Note that the &lt;code&gt;async_compose&lt;/code&gt; call which will
encapsulate the use of this class embeds this object into a wrapper which provides the &lt;code&gt;executor_type&lt;/code&gt; and 
&lt;code&gt;get_executor()&lt;/code&gt; mechanism which asio uses to determine on which executor to invoke completion handlers.&lt;/p&gt;
    &lt;pre&gt;&lt;code class=&quot;language-cpp&quot;&gt;              impl.tcp_layer().expires_never();
              yield self.complete(ec);
          }
#include &amp;lt;boost/asio/unyield.hpp&amp;gt;
      }

      std::unique_ptr&amp;lt; impl_data &amp;gt; impl_;
  };
&lt;/code&gt;&lt;/pre&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The &lt;code&gt;wss_connection&lt;/code&gt; class provides the bare bones required to connect a websocket and maintain the connection. It 
provides a protected interface so that derived classes can send text frames and it will call private virtual functions
in order to notify the derived class of:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;transport up (websocket connection established).&lt;/li&gt;
  &lt;li&gt;frame received.&lt;/li&gt;
  &lt;li&gt;connection error (either during connection or operation).&lt;/li&gt;
  &lt;li&gt;websocket close - the server has requested or agreed to a graceful shutdown.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Connection errors will only be notified once, and once a connection error has been indicated, no other event will reach
the derived class.&lt;/p&gt;

&lt;p&gt;One of the many areas that trips up asio/beast beginners is that care must be taken to ensure that only one &lt;code&gt;async_write&lt;/code&gt;
is in progress at a time on the WebSocket (or indeed any async io object). For this reason we implement a simple 
transmit queue state which can be considered to be an orthogonal region (parallel task) to the read state.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-cpp&quot;&gt;        // send_state - data to control sending data

        std::deque&amp;lt;std::string&amp;gt; send_queue_;
        enum send_state
        {
            not_sending,
            sending
        } send_state_ = not_sending;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;You will note that I have used a &lt;code&gt;std::deque&lt;/code&gt; to hold the pending messages. Although a deque has theoretically better
complexity when inserting or removing items at the ends than a vector, this is not the reason for choosing this data
structure. The actual reason is that items in a deque are guaranteed to have a stable address, even when other items
are added or removed. This is useful as it means we don’t have to move frames out of the transmit queue in order to
send them. Remember that during an &lt;code&gt;async_write&lt;/code&gt;, the data to which the supplied buffer sequence refers must have a 
stable address.&lt;/p&gt;

&lt;p&gt;Here are the functions that deal with the send state transitions.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-cpp&quot;&gt;    void
    wss_transport::send_text_frame(std::string frame)
    {
        if (state_ != connected)
            return;

        send_queue_.push_back(std::move(frame));
        start_sending();
    }

    void
    wss_transport::start_sending()
    {
        if (state_ == connected &amp;amp;&amp;amp; send_state_ == not_sending &amp;amp;&amp;amp;
            !send_queue_.empty())
        {
            send_state_ = sending;
            websock_.async_write(net::buffer(send_queue_.front()),
                                 [this](error_code const &amp;amp;ec, std::size_t bt) {
                                     handle_send(ec, bt);
                                 });
        }
    }

    void
    wss_transport::handle_send(const error_code &amp;amp;ec, std::size_t)
    {
        send_state_ = not_sending;

        send_queue_.pop_front();

        if (ec)
            event_transport_error(ec);
        else
            start_sending();
    }
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Finally, we can implement our specific exchange protocol on top of the &lt;code&gt;wss_connection&lt;/code&gt;. In this case, FMex eschews 
the ping/pong built into websockets and requires a json ping/pong to be initiated by the client.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-cpp&quot;&gt;    void
    fmex_connection::ping_enter_state()
    {
        BOOST_ASSERT(ping_state_ == ping_not_started);
        ping_enter_wait();
    }

    void
    fmex_connection::ping_enter_wait()
    {
        ping_state_ = ping_wait;

        ping_timer_.expires_after(5s);

        ping_timer_.async_wait([this](error_code const &amp;amp;ec) {
            if (!ec)
                ping_event_timeout();
        });
    }

    void
    fmex_connection::ping_event_timeout()
    {
        ping_state_ = ping_waiting_pong;

        auto  frame = json::value();
        auto &amp;amp;o     = frame.emplace_object();
        o[&quot;cmd&quot;]    = &quot;ping&quot;;
        o[&quot;id&quot;]     = &quot;my_ping_ident&quot;;
        o[&quot;args&quot;].emplace_array().push_back(timestamp());
        send_text_frame(json::serialize(frame));
    }

    void
    fmex_connection::ping_event_pong(json::value const &amp;amp;frame)
    {
        ping_enter_wait();
    }
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Note that since we have implemented frame transmission in the base class in terms of a queue, the fmex class has no
need to worry about ensuring the one-write-at-a-time rule. The base class handles it. This makes the application 
developer’s life easy.&lt;/p&gt;

&lt;p&gt;Finally, we implement &lt;code&gt;on_text_frame&lt;/code&gt; and write a little message parser and switch. Note that this function may throw.
The base class will catch any exceptions thrown here and ensure that the &lt;code&gt;on_transport_error&lt;/code&gt; event will be called at
the appropriate time. Thus again, the application developer’s life is improved as he doesn’t need to worry about
handling exceptions in an asynchronous environment.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-cpp&quot;&gt;    void
    fmex_connection::on_text_frame(std::string_view frame)
    try
    {
        auto jframe =
            json::parse(json::string_view(frame.data(), frame.size()));

        // dispatch on frame type

        auto &amp;amp;type = jframe.as_object().at(&quot;type&quot;);
        if (type == &quot;hello&quot;)
        {
            on_hello();
        }
        else if (type == &quot;ping&quot;)
        {
            ping_event_pong(jframe);
        }
        else if (type.as_string().starts_with(&quot;ticker.&quot;))
        {
            fmt::print(stdout,
                       &quot;fmex: tick {} : {}\n&quot;,
                       type.as_string().subview(7),
                       jframe.as_object().at(&quot;ticker&quot;));
        }
    }
    catch (...)
    {
        fmt::print(stderr, &quot;text frame is not json : {}\n&quot;, frame);
        throw;
    }
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Compiling and running the program produces output similar to this:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Application starting
Press ctrl-c to interrupt.
fmex: initiating connection
fmex: transport up
fmex: hello
fmex: tick btcusd_p : [1.0879E4,1.407E3,1.0879E4,2.28836E5,1.08795E4,1.13E2,1.0701E4,1.0939E4,1.0663E4,2.51888975E8,2.3378048830533768E4]
fmex: tick btcusd_p : [1.08795E4,1E0,1.0879E4,3.79531E5,1.08795E4,3.518E3,1.0701E4,1.0939E4,1.0663E4,2.51888976E8,2.3378048922449758E4]
fmex: tick btcusd_p : [1.0879E4,2E0,1.0879E4,3.7747E5,1.08795E4,7.575E3,1.0701E4,1.0939E4,1.0663E4,2.51888978E8,2.3378049106290182E4]
fmex: tick btcusd_p : [1.0879E4,2E0,1.0879E4,3.77468E5,1.08795E4,9.229E3,1.0701E4,1.0939E4,1.0663E4,2.5188898E8,2.337804929013061E4]
fmex: tick btcusd_p : [1.0879E4,1E0,1.0879E4,1.0039E4,1.08795E4,2.54203E5,1.0701E4,1.0939E4,1.0663E4,2.51888981E8,2.3378049382050827E4]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Note however, that although pressing ctrl-c is noticed by the application, the fmex feed does not shut down in response.
This is because we have not wired up a mechanism to communicate the &lt;code&gt;stop()&lt;/code&gt; event to the implementation of the 
connection:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ ./blog_2020_09 
Application starting
Press ctrl-c to interrupt.
fmex: initiating connection
fmex: transport up
fmex: hello
fmex: tick btcusd_p : [1.0859E4,1E0,1.0859E4,6.8663E4,1.08595E4,4.1457E4,1.07125E4,1.0939E4,1.0667E4,2.58585817E8,2.3968266005011003E4]
^CInterrupt detected. Press ctrl-c again within 5 seconds to exit
fmex: tick btcusd_p : [1.08595E4,2E0,1.0859E4,5.9942E4,1.08595E4,4.3727E4,1.07125E4,1.0939E4,1.0667E4,2.58585819E8,2.3968266189181537E4]
^CInterrupt confirmed. Shutting down
fmex: tick btcusd_p : [1.08595E4,2E0,1.0859E4,5.9932E4,1.08595E4,4.0933E4,1.07125E4,1.0939E4,1.0667E4,2.58585821E8,2.396826637335208E4]
fmex: tick btcusd_p : [1.0859E4,1E0,1.0859E4,6.2722E4,1.08595E4,4.0943E4,1.07125E4,1.0939E4,1.0667E4,2.58585823E8,2.3968266557531104E4]
fmex: tick btcusd_p : [1.08595E4,1.58E2,1.0859E4,6.2732E4,1.08595E4,3.7953E4,1.07125E4,1.0939E4,1.0667E4,2.58585981E8,2.3968281107003917E4]
^Z
[1]+  Stopped                 ./blog_2020_09
$ kill %1

[1]+  Stopped                 ./blog_2020_09
$ 
[1]+  Terminated              ./blog_2020_09
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;step-3---re-enabling-cancellation&quot;&gt;Step 3 - Re-Enabling Cancellation&lt;/h2&gt;

&lt;p&gt;You will remember from step 1 that we created a little class called &lt;code&gt;sigint_state&lt;/code&gt; which notices that the application 
has received a sigint and checks for a confirming sigint before taking action. We also added a slot to this to pass the 
signal to the fmex connection:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-cpp&quot;&gt;            fmex_connection_.start();
            sigint_state_.add_slot([this]{
                fmex_connection_.stop();
            });
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;But we didn’t put any code in &lt;code&gt;wss_transport::stop&lt;/code&gt;. Now all we have to do is provide a function object within 
&lt;code&gt;wss_transport&lt;/code&gt; that we can adjust whenever the current state changes:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-cpp&quot;&gt;        // stop signal
        std::function&amp;lt;void()&amp;gt; stop_signal_;
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&quot;language-cpp&quot;&gt;    void
    wss_transport::stop()
    {
        net::dispatch(get_executor(), [this] {
            if (auto sig = boost::exchange(stop_signal_, nullptr))
                sig();
        });
    }
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We will also need to provide a way for the connect operation to respond to the stop signal (the user might press
ctrl-c while resolving for example).&lt;/p&gt;

&lt;p&gt;The way I have done this here is a simple approach, merely pass a reference to the &lt;code&gt;wss_transport&lt;/code&gt; into the composed
operation so that the operation can modify the function directly. There are other more scalable ways to do this, but
this is good enough for now.&lt;/p&gt;

&lt;p&gt;The body of the coroutine then becomes:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-cpp&quot;&gt;            auto &amp;amp;impl = *impl_;

            if(ec)
                impl.error = ec;

            if (impl.error)
                return self.complete(impl.error);

#include &amp;lt;boost/asio/yield.hpp&amp;gt;
            reenter(*this)
            {
                transport_-&amp;gt;stop_signal_ = [&amp;amp;impl] {
                    impl.resolver.cancel();
                    impl.error = net::error::operation_aborted;
                };
                yield impl.resolver.async_resolve(
                    impl.host, impl.port, std::move(self));

                //

                transport_-&amp;gt;stop_signal_ = [&amp;amp;impl] {
                    impl.tcp_layer().cancel();
                    impl.error = net::error::operation_aborted;
                };

                impl.tcp_layer().expires_after(15s);
                yield impl.tcp_layer().async_connect(impl.endpoints,
                                                     std::move(self));

                //

                if (!SSL_set_tlsext_host_name(impl.ssl_layer().native_handle(),
                                              impl.host.c_str()))
                    return self.complete(
                        error_code(static_cast&amp;lt; int &amp;gt;(::ERR_get_error()),
                                   net::error::get_ssl_category()));

                //

                impl.tcp_layer().expires_after(15s);
                yield impl.ssl_layer().async_handshake(ssl::stream_base::client,
                                                       std::move(self));

                //

                impl.tcp_layer().expires_after(15s);
                yield impl.ws.async_handshake(
                    impl.host, impl.target, std::move(self));

                //

                transport_-&amp;gt;stop_signal_ = nullptr;
                impl.tcp_layer().expires_never();
                yield self.complete(impl.error);
            }
#include &amp;lt;boost/asio/unyield.hpp&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The final source code for 
&lt;a href=&quot;https://github.com/test-scenarios/boost_beast_websocket_echo/tree/blog-2020-09-step-3/pre-cxx20/blog-2020-09&quot;&gt;step 3 is here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Stopping the program while connecting:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ ./blog_2020_09 
Application starting
Press ctrl-c to interrupt.
fmex: initiating connection
^CInterrupt detected. Press ctrl-c again within 5 seconds to exit
^CInterrupt confirmed. Shutting down
fmex: transport error : system : 125 : Operation canceled
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;And stopping the program while connected:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ ./blog_2020_09 
Application starting
Press ctrl-c to interrupt.
fmex: initiating connection
fmex: transport up
fmex: hello
^CInterrupt detected. Press ctrl-c again within 5 seconds to exit
fmex: tick btcusd_p : [1.0882E4,1E0,1.0882E4,3.75594E5,1.08825E4,5.103E3,1.07295E4,1.0939E4,1.06785E4,2.58278146E8,2.3907706652603207E4]
^CInterrupt confirmed. Shutting down
closing websocket
fmex: closed
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&quot;future-development&quot;&gt;Future development&lt;/h1&gt;

&lt;p&gt;Next month I’ll refactor the application to use C++20 coroutines and we can see whether this makes developing
event based systems easier and/or more maintainable.&lt;/p&gt;

&lt;p&gt;Thanks for reading.&lt;/p&gt;</content><author><name></name></author><category term="richard" /><summary type="html">Cancellation in Beast/Asio and Better Compile Performance with Beast.Websocket This month I will be discussing two issues. One of interest to many people who come to us with questions on the Github Issue Tracker and the #beast channel of Cpplang Slack. Compile Times and Separation of Concerns A common complaint about Boost.Beast is that compilation units that use the websocket::stream template class often take a long time to compile, and that because websocket::stream is a template, this compilation overhead can become viral in an application. This is a valid complaint and we believe there are some reasonable tradeoffs we can make by refactoring the websocket stream to use fewer templates internally. Vinnie has started work to express the WebSocket’s intermediate completion handlers, buffer sequence and executor in terms of a polymorphic object. This would mean a few indirect jumps in the compiled code but would significantly reduce the number of internal template expansions. In the scheme of things, we don’t believe that the virtual function calls will materially affect runtime performance. The branch is here I will be continuing work in this area in the coming days. In the meantime, our general response is to suggest that users create a base class to handle the transport, and communicate important events such as frame received, connection state and the close notification to a derived application-layer class through a private polymorphic interface. In this way, the websocket transport compilation unit may take a while to compile, but it needs to be done only once since the transport layer will rarely change during the development life of an application. Whenever there is a change to the application layer, the transport layer is not affected so websocket-related code is not affected. This approach has a number of benefits. Not least of which is that developing another client implementation over a different websocket connection in the same application becomes trivial. Another benefit is that the application can be designed such that application-level concerns are agnostic of the transport mechanism. Such as when the server can be accessed by multiple means - WSS, WS, long poll, direct connection, unix sockets and so on. In this blog I will present a simplified implementation of this idea. My thanks to the cpplang Slack user @elegracer who most recently asked for guidance on reducing compile times. It was (his/her? Slack is silent on the matter) question which prompted me to finally conjure up a demo. @elegracer’s problem was needing to connect to multiple cryptocurrency exchanges in the same app over websocket. In this particular example I’ll demonstrate a simplified connection to the public FMex market data feed since that was the subject of the original question. Correct Cancellation Our examples in the Beast Repository are rudimentary and don’t cover the issue of graceful shutdown of an application in response to a SIGINT (i.e. the user pressing ctrl-c). It is common for simple programs to exit suddenly in response to this signal, which is the default behaviour. For many applications, this is perfectly fine but not all. We may want active objects in the program to write data to disk, we may want to ensure that the underlying websocket is shut down cleanly and we may want to give the user an opportunity to prevent the shutdown. I will further annotate the example by providing this ability to prevent the shutdown. The user will have to confirm the first SIGINT with another within 5 seconds to confirm. Designing the application When I write IO applications involving Asio and Beast, I prefer to create an “application” object. This has the responsibility of monitoring signals and starting the initial connection objects. It also provides the communication between the two. The construction and configuration of the io_context and ssl::context stay in main(). The executor and ssl context are passed to the application by reference as dependencies. The application can then pass on these refrences as required. It is also worth mentioning that I don’t pass the io_context’s executor as a polymorphic any_io_executor type at this stage. The reason is that I may want in future to upgrade my program to be multi-threaded. If I do this, then each individual io_enabled object such as a connection or the application will need to have its own strand. Getting the strand out of an any_io_executor is not possible in the general case as it will have been type-erased, so for top level objects I pass the executor as io_context::executor_type. It is then up to each object to create its own strand internally which will have the type strand&amp;lt;io_context::executor_type&amp;gt;. The strand type provides the method get_inner_executor which allows the application to extract the underlying io_context::executor_type and pass it to the constructor of any subordinate but otherwise self-contained io objects. The subordinates can then build their own strands from this. Step 1 - A Simple Application Framework That Supports ctrl-c OK, let’s get started and build the framework. Here’s a link to step 1. ssl.hpp and net.hpp simply configure the project to use boost.asio. The idea of these little configuration headers is that they could be generated by the cmake project if necessary to allow the option of upgrading to std networking if it ever arrives. As a matter of style, I like to ensure that no names are created in the global namespace other than main. This saves headaches that could occur if I wrote code on one platform, but then happened to port it to another where the name was already in use by the native system libraries. main.cpp simply creates the io execution context and a default ssl context, creates the application, starts it and runs the io context. At the moment, the only interesting part of our program is the signit_state. This is a state machine which handles the behaviour of the program when a SIGINT is received. Our state machine is doing something a little fancy. Here is the state diagram: Rather than reproduce the code here, please refer to step 1 to see the source code. At this point the program will run and successfully handle ctrl-c: $ ./blog_2020_09 Application starting Press ctrl-c to interrupt. ^CInterrupt detected. Press ctrl-c again within 5 seconds to exit Interrupt unconfirmed. Ignoring ^CInterrupt detected. Press ctrl-c again within 5 seconds to exit ^CInterrupt confirmed. Shutting down Step 2 - Connecting to an Exchange Now we need to create our WebSocket transport class and our FMex exchange protocol class that will derive from it. For now we won’t worry about cancellation - we’ll retrofit that in Step 3. Here is the code for step 2. This section introduces two new main classes - the wss_transport and the fmex_connection. In addition, the connection phase of the wss_transport is expressed as a composed operation for exposition purposes (and in my opinion it actually makes the code easier to read than continuation-passing style code) Here is the implementation of the connect coroutine: struct wss_transport::connect_op : asio::coroutine { using executor_type = wss_transport::executor_type; using websock = wss_transport::websock; Here we define the implementation of the coroutine - this is an object which will not be moved for the duration of the execution of the coroutine. This address stability is important because intermediate asynchronous operations will rely on knowing the address of the resolver (and later perhaps other io objects). struct impl_data { impl_data(websock &amp;amp; ws, std::string host, std::string port, std::string target) : ws(ws) , resolver(ws.get_executor()) , host(host) , port(port) , target(target) { } layer_0 &amp;amp; tcp_layer() const { return ws.next_layer().next_layer(); } layer_1 &amp;amp; ssl_layer() const { return ws.next_layer(); } websock &amp;amp; ws; net::ip::tcp::resolver resolver; net::ip::tcp::resolver::results_type endpoints; std::string host, port, target; }; The constructor merely forwards the arguments to the construction of the impl_data. connect_op(websock &amp;amp; ws, std::string host, std::string port, std::string target) : impl_(std::make_unique&amp;lt; impl_data &amp;gt;(ws, host, port, target)) { } This coroutine is both a composed operation and a completion handler for sub-operations. This means it must have an operator() interface matching the requirements of each sub-operation. During the lifetime of this coroutine we will be using the resolver and calling async_connect on the tcp_stream. We therefore provide conforming member functions which store or ignore the and forward the error_code to the main implementation of the coroutine. template &amp;lt; class Self &amp;gt; void operator()(Self &amp;amp; self, error_code ec, net::ip::tcp::resolver::results_type results) { impl_-&amp;gt;endpoints = results; (*this)(self, ec); } template &amp;lt; class Self &amp;gt; void operator()(Self &amp;amp;self, error_code ec, net::ip::tcp::endpoint const &amp;amp;) { (*this)(self, ec); } Here is the main implementation of the coroutine. Note that the last two parameters provide defaults. This is in order to allow this member function to match the completion handler signatures of: void() - invoked during async_compose in order to start the coroutine. void(error_code) - invoked by the two functions above and by the async handshakes. void(error_code, std::size_t) - invoked by operations such as async_read and async_write although not strictly necessary here. template &amp;lt; class Self &amp;gt; void operator()(Self &amp;amp;self, error_code ec = {}, std::size_t = 0) { Note that here we are checking the error code before re-entering the coroutine. This is a shortcut which allows us to omit error checking after each sub-operation. This check will happen on every attempt to re-enter the coroutine, including the first entry (at which time ec is guaranteed to be default constructed). if (ec) return self.complete(ec); auto &amp;amp;impl = *impl_; Note the use of the asio yield and unyield headers to create the fake ‘keywords’ reenter and yield in avery limited scope. #include &amp;lt;boost/asio/yield.hpp&amp;gt; reenter(*this) { yield impl.resolver.async_resolve( impl.host, impl.port, std::move(self)); impl.tcp_layer().expires_after(15s); yield impl.tcp_layer().async_connect(impl.endpoints, std::move(self)); if (!SSL_set_tlsext_host_name(impl.ssl_layer().native_handle(), impl.host.c_str())) return self.complete( error_code(static_cast&amp;lt; int &amp;gt;(::ERR_get_error()), net::error::get_ssl_category())); impl.tcp_layer().expires_after(15s); yield impl.ssl_layer().async_handshake(ssl::stream_base::client, std::move(self)); impl.tcp_layer().expires_after(15s); yield impl.ws.async_handshake( impl.host, impl.target, std::move(self)); If the coroutine is re-entered here, it must be because there was no error (if there was an error, it would have been caught by the pre-reentry error check above). Since execution has resumed here in the completion handler of the async_handshake initiating function, we are guaranteed to be executing in the correct executor. Therefore we can simply call complete directly without needing to post to an executor. Note that the async_compose call which will encapsulate the use of this class embeds this object into a wrapper which provides the executor_type and get_executor() mechanism which asio uses to determine on which executor to invoke completion handlers. impl.tcp_layer().expires_never(); yield self.complete(ec); } #include &amp;lt;boost/asio/unyield.hpp&amp;gt; } std::unique_ptr&amp;lt; impl_data &amp;gt; impl_; }; The wss_connection class provides the bare bones required to connect a websocket and maintain the connection. It provides a protected interface so that derived classes can send text frames and it will call private virtual functions in order to notify the derived class of: transport up (websocket connection established). frame received. connection error (either during connection or operation). websocket close - the server has requested or agreed to a graceful shutdown. Connection errors will only be notified once, and once a connection error has been indicated, no other event will reach the derived class. One of the many areas that trips up asio/beast beginners is that care must be taken to ensure that only one async_write is in progress at a time on the WebSocket (or indeed any async io object). For this reason we implement a simple transmit queue state which can be considered to be an orthogonal region (parallel task) to the read state. // send_state - data to control sending data std::deque&amp;lt;std::string&amp;gt; send_queue_; enum send_state { not_sending, sending } send_state_ = not_sending; You will note that I have used a std::deque to hold the pending messages. Although a deque has theoretically better complexity when inserting or removing items at the ends than a vector, this is not the reason for choosing this data structure. The actual reason is that items in a deque are guaranteed to have a stable address, even when other items are added or removed. This is useful as it means we don’t have to move frames out of the transmit queue in order to send them. Remember that during an async_write, the data to which the supplied buffer sequence refers must have a stable address. Here are the functions that deal with the send state transitions. void wss_transport::send_text_frame(std::string frame) { if (state_ != connected) return; send_queue_.push_back(std::move(frame)); start_sending(); } void wss_transport::start_sending() { if (state_ == connected &amp;amp;&amp;amp; send_state_ == not_sending &amp;amp;&amp;amp; !send_queue_.empty()) { send_state_ = sending; websock_.async_write(net::buffer(send_queue_.front()), [this](error_code const &amp;amp;ec, std::size_t bt) { handle_send(ec, bt); }); } } void wss_transport::handle_send(const error_code &amp;amp;ec, std::size_t) { send_state_ = not_sending; send_queue_.pop_front(); if (ec) event_transport_error(ec); else start_sending(); } Finally, we can implement our specific exchange protocol on top of the wss_connection. In this case, FMex eschews the ping/pong built into websockets and requires a json ping/pong to be initiated by the client. void fmex_connection::ping_enter_state() { BOOST_ASSERT(ping_state_ == ping_not_started); ping_enter_wait(); } void fmex_connection::ping_enter_wait() { ping_state_ = ping_wait; ping_timer_.expires_after(5s); ping_timer_.async_wait([this](error_code const &amp;amp;ec) { if (!ec) ping_event_timeout(); }); } void fmex_connection::ping_event_timeout() { ping_state_ = ping_waiting_pong; auto frame = json::value(); auto &amp;amp;o = frame.emplace_object(); o[&quot;cmd&quot;] = &quot;ping&quot;; o[&quot;id&quot;] = &quot;my_ping_ident&quot;; o[&quot;args&quot;].emplace_array().push_back(timestamp()); send_text_frame(json::serialize(frame)); } void fmex_connection::ping_event_pong(json::value const &amp;amp;frame) { ping_enter_wait(); } Note that since we have implemented frame transmission in the base class in terms of a queue, the fmex class has no need to worry about ensuring the one-write-at-a-time rule. The base class handles it. This makes the application developer’s life easy. Finally, we implement on_text_frame and write a little message parser and switch. Note that this function may throw. The base class will catch any exceptions thrown here and ensure that the on_transport_error event will be called at the appropriate time. Thus again, the application developer’s life is improved as he doesn’t need to worry about handling exceptions in an asynchronous environment. void fmex_connection::on_text_frame(std::string_view frame) try { auto jframe = json::parse(json::string_view(frame.data(), frame.size())); // dispatch on frame type auto &amp;amp;type = jframe.as_object().at(&quot;type&quot;); if (type == &quot;hello&quot;) { on_hello(); } else if (type == &quot;ping&quot;) { ping_event_pong(jframe); } else if (type.as_string().starts_with(&quot;ticker.&quot;)) { fmt::print(stdout, &quot;fmex: tick {} : {}\n&quot;, type.as_string().subview(7), jframe.as_object().at(&quot;ticker&quot;)); } } catch (...) { fmt::print(stderr, &quot;text frame is not json : {}\n&quot;, frame); throw; } Compiling and running the program produces output similar to this: Application starting Press ctrl-c to interrupt. fmex: initiating connection fmex: transport up fmex: hello fmex: tick btcusd_p : [1.0879E4,1.407E3,1.0879E4,2.28836E5,1.08795E4,1.13E2,1.0701E4,1.0939E4,1.0663E4,2.51888975E8,2.3378048830533768E4] fmex: tick btcusd_p : [1.08795E4,1E0,1.0879E4,3.79531E5,1.08795E4,3.518E3,1.0701E4,1.0939E4,1.0663E4,2.51888976E8,2.3378048922449758E4] fmex: tick btcusd_p : [1.0879E4,2E0,1.0879E4,3.7747E5,1.08795E4,7.575E3,1.0701E4,1.0939E4,1.0663E4,2.51888978E8,2.3378049106290182E4] fmex: tick btcusd_p : [1.0879E4,2E0,1.0879E4,3.77468E5,1.08795E4,9.229E3,1.0701E4,1.0939E4,1.0663E4,2.5188898E8,2.337804929013061E4] fmex: tick btcusd_p : [1.0879E4,1E0,1.0879E4,1.0039E4,1.08795E4,2.54203E5,1.0701E4,1.0939E4,1.0663E4,2.51888981E8,2.3378049382050827E4] Note however, that although pressing ctrl-c is noticed by the application, the fmex feed does not shut down in response. This is because we have not wired up a mechanism to communicate the stop() event to the implementation of the connection: $ ./blog_2020_09 Application starting Press ctrl-c to interrupt. fmex: initiating connection fmex: transport up fmex: hello fmex: tick btcusd_p : [1.0859E4,1E0,1.0859E4,6.8663E4,1.08595E4,4.1457E4,1.07125E4,1.0939E4,1.0667E4,2.58585817E8,2.3968266005011003E4] ^CInterrupt detected. Press ctrl-c again within 5 seconds to exit fmex: tick btcusd_p : [1.08595E4,2E0,1.0859E4,5.9942E4,1.08595E4,4.3727E4,1.07125E4,1.0939E4,1.0667E4,2.58585819E8,2.3968266189181537E4] ^CInterrupt confirmed. Shutting down fmex: tick btcusd_p : [1.08595E4,2E0,1.0859E4,5.9932E4,1.08595E4,4.0933E4,1.07125E4,1.0939E4,1.0667E4,2.58585821E8,2.396826637335208E4] fmex: tick btcusd_p : [1.0859E4,1E0,1.0859E4,6.2722E4,1.08595E4,4.0943E4,1.07125E4,1.0939E4,1.0667E4,2.58585823E8,2.3968266557531104E4] fmex: tick btcusd_p : [1.08595E4,1.58E2,1.0859E4,6.2732E4,1.08595E4,3.7953E4,1.07125E4,1.0939E4,1.0667E4,2.58585981E8,2.3968281107003917E4] ^Z [1]+ Stopped ./blog_2020_09 $ kill %1 [1]+ Stopped ./blog_2020_09 $ [1]+ Terminated ./blog_2020_09 Step 3 - Re-Enabling Cancellation You will remember from step 1 that we created a little class called sigint_state which notices that the application has received a sigint and checks for a confirming sigint before taking action. We also added a slot to this to pass the signal to the fmex connection: fmex_connection_.start(); sigint_state_.add_slot([this]{ fmex_connection_.stop(); }); But we didn’t put any code in wss_transport::stop. Now all we have to do is provide a function object within wss_transport that we can adjust whenever the current state changes: // stop signal std::function&amp;lt;void()&amp;gt; stop_signal_; void wss_transport::stop() { net::dispatch(get_executor(), [this] { if (auto sig = boost::exchange(stop_signal_, nullptr)) sig(); }); } We will also need to provide a way for the connect operation to respond to the stop signal (the user might press ctrl-c while resolving for example). The way I have done this here is a simple approach, merely pass a reference to the wss_transport into the composed operation so that the operation can modify the function directly. There are other more scalable ways to do this, but this is good enough for now. The body of the coroutine then becomes: auto &amp;amp;impl = *impl_; if(ec) impl.error = ec; if (impl.error) return self.complete(impl.error); #include &amp;lt;boost/asio/yield.hpp&amp;gt; reenter(*this) { transport_-&amp;gt;stop_signal_ = [&amp;amp;impl] { impl.resolver.cancel(); impl.error = net::error::operation_aborted; }; yield impl.resolver.async_resolve( impl.host, impl.port, std::move(self)); // transport_-&amp;gt;stop_signal_ = [&amp;amp;impl] { impl.tcp_layer().cancel(); impl.error = net::error::operation_aborted; }; impl.tcp_layer().expires_after(15s); yield impl.tcp_layer().async_connect(impl.endpoints, std::move(self)); // if (!SSL_set_tlsext_host_name(impl.ssl_layer().native_handle(), impl.host.c_str())) return self.complete( error_code(static_cast&amp;lt; int &amp;gt;(::ERR_get_error()), net::error::get_ssl_category())); // impl.tcp_layer().expires_after(15s); yield impl.ssl_layer().async_handshake(ssl::stream_base::client, std::move(self)); // impl.tcp_layer().expires_after(15s); yield impl.ws.async_handshake( impl.host, impl.target, std::move(self)); // transport_-&amp;gt;stop_signal_ = nullptr; impl.tcp_layer().expires_never(); yield self.complete(impl.error); } #include &amp;lt;boost/asio/unyield.hpp&amp;gt; The final source code for step 3 is here. Stopping the program while connecting: $ ./blog_2020_09 Application starting Press ctrl-c to interrupt. fmex: initiating connection ^CInterrupt detected. Press ctrl-c again within 5 seconds to exit ^CInterrupt confirmed. Shutting down fmex: transport error : system : 125 : Operation canceled And stopping the program while connected: $ ./blog_2020_09 Application starting Press ctrl-c to interrupt. fmex: initiating connection fmex: transport up fmex: hello ^CInterrupt detected. Press ctrl-c again within 5 seconds to exit fmex: tick btcusd_p : [1.0882E4,1E0,1.0882E4,3.75594E5,1.08825E4,5.103E3,1.07295E4,1.0939E4,1.06785E4,2.58278146E8,2.3907706652603207E4] ^CInterrupt confirmed. Shutting down closing websocket fmex: closed Future development Next month I’ll refactor the application to use C++20 coroutines and we can see whether this makes developing event based systems easier and/or more maintainable. Thanks for reading.</summary></entry><entry><title type="html">Krystian’s September Update</title><link href="http://cppalliance.org/krystian/2020/09/29/KrystiansSeptemberUpdate.html" rel="alternate" type="text/html" title="Krystian’s September Update" /><published>2020-09-29T00:00:00+00:00</published><updated>2020-09-29T00:00:00+00:00</updated><id>http://cppalliance.org/krystian/2020/09/29/KrystiansSeptemberUpdate</id><content type="html" xml:base="http://cppalliance.org/krystian/2020/09/29/KrystiansSeptemberUpdate.html">&lt;h1 id=&quot;reviewing-the-review&quot;&gt;Reviewing the review&lt;/h1&gt;

&lt;p&gt;The review period for Boost.JSON has come and gone, and we got some great feedback on the design of the library. Glancing over the results, it appears that the general mood was to accept the library. This doesn’t mean that there weren’t any problem areas – most notably the documentation, which often did contain the information people wanted, but it was difficult to find.&lt;/p&gt;

&lt;p&gt;Other points of contention were the use of a push parser as opposed to a pull parser, the use of &lt;code&gt;double&lt;/code&gt;, &lt;code&gt;uint64_t&lt;/code&gt;, and &lt;code&gt;int64_t&lt;/code&gt; without allowing for users to change them, and the value conversion interface. Overall some very good points were made, and I’d like to thank everyone for participating in the review.&lt;/p&gt;

&lt;h1 id=&quot;customizing-the-build&quot;&gt;Customizing the build&lt;/h1&gt;

&lt;p&gt;I put a bit of work into improving our CI matrix, as it had several redundant configurations and did not test newer compiler versions (e.g. GCC 10, clang 11), nor did we have any 32-bit jobs. The most difficult thing about working on the build matrix is balancing how exhaustive it is with the turnaround time – sure, we could add 60 configurations that test x86, x86-64, and ARM on every major compiler version released since 2011, but the turnaround would be abysmal.&lt;/p&gt;

&lt;p&gt;To alleviate this, I only added 32-bit jobs for the sanitizers that use a recent version of GCC. It’s a less common configuration in the days of 64-bit universality, and if 64 bit works then it’s highly likely that 32 bit will “just work” as well.&lt;/p&gt;

&lt;p&gt;Here’s a table of the new Travis configurations that will be added:&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;Compiler&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;Library&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;C++ Standard&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;Variant&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;OS&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;Architecture&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;Job&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;—&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;—&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;—&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Boost&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Linux (Xenial)&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;x86-64&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Documentation&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;gcc 8.4.0&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;libstdc++&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;11&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Boost&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Linux (Xenial)&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;x86-64&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Coverage&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;clang 6.0.1&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;libstdc++&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;11, 14&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Boost&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Linux (Xenial)&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;x86-64&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Valgrind&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;clang 11.0.0&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;libstdc++&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;17&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Boost&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Linux (Xenial)&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;x86-64&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Address Sanitizer&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;clang 11.0.0&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;libstdc++&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;17&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Boost&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Linux (Xenial)&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;x86-64&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;UB Sanitizer&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;msvc 14.1&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;MS STL&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;11, 14, 17&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Boost&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Windows&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;x86-64&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;—&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;msvc 14.1&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;MS STL&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;17, 2a&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Standalone&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Windows&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;x86-64&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;—&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;msvc 14.2&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;MS STL&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;17, 2a&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Boost&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Windows&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;x86-64&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;—&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;msvc 14.2&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;MS STL&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;17, 2a&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Standalone&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Windows&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;x86-64&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;—&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;icc 2021.1&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;libstdc++&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;11, 14, 17&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Boost&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Linux (Bionic)&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;x86-64&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;—&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;gcc 4.8.5&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;libstdc++&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;11&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Boost&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Linux (Trusty)&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;x86-64&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;—&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;gcc 4.9.4&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;libstdc++&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;11&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Boost&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Linux (Trusty)&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;x86-64&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;—&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;gcc 5.5.0&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;libstdc++&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;11&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Boost&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Linux (Xenial)&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;x86-64&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;—&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;gcc 6.5.0&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;libstdc++&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;11, 14&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Boost&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Linux (Xenial)&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;x86-64&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;—&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;gcc 7.5.0&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;libstdc++&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;14, 17&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Boost&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Linux (Xenial)&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;x86-64&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;—&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;gcc 8.4.0&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;libstdc++&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;17, 2a&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Boost&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Linux (Xenial)&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;x86-64&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;—&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;gcc 9.3.0&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;libstdc++&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;17, 2a&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Boost&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Linux (Xenial)&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;x86-64&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;—&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;gcc 9.3.0&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;libstdc++&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;17, 2a&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Standalone&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Linux (Xenial)&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;x86-64&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;—&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;gcc 10.2.0&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;libstdc++&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;17, 2a&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Boost&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Linux (Focal)&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;x86-64&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;—&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;gcc 10.2.0&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;libstdc++&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;17, 2a&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Standalone&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Linux (Focal)&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;x86-64&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;—&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;gcc (trunk)&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;libstdc++&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;17, 2a&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Boost&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Linux (Focal)&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;x86-64&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;—&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;gcc (trunk)&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;libstdc++&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;17, 2a&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Standalone&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Linux (Focal)&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;x86-64&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;—&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;clang 3.8.0&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;libstdc++&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;11&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Boost&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Linux (Trusty)&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;x86-64&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;—&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;clang 4.0.0&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;libstdc++&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;11, 14&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Boost&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Linux (Xenial)&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;x86-64&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;—&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;clang 5.0.2&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;libstdc++&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;11, 14&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Boost&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Linux (Xenial)&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;x86-64&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;—&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;clang 6.0.1&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;libstdc++&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;14, 17&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Boost&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Linux (Xenial)&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;x86-64&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;—&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;clang 7.0.1&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;libstdc++&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;17, 2a&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Boost&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Linux (Xenial)&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;x86-64&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;—&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;clang 9.0.1&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;libstdc++&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;17, 2a&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Boost&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Linux (Xenial)&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;x86-64&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;—&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;clang 9.0.1&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;libstdc++&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;17, 2a&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Standalone&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Linux (Xenial)&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;x86-64&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;—&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;clang 10.0.1&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;libstdc++&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;17, 2a&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Boost&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Linux (Xenial)&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;x86-64&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;—&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;clang 10.0.1&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;libstdc++&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;17, 2a&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Standalone&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Linux (Xenial)&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;x86-64&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;—&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;clang 11.0.0&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;libstdc++&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;17, 2a&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Boost&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Linux (Xenial)&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;x86-64&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;—&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;clang 11.0.0&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;libstdc++&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;17, 2a&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Standalone&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Linux (Xenial)&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;x86-64&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;—&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;clang (trunk)&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;libstdc++&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;17, 2a&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Boost&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Linux (Xenial)&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;x86-64&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;—&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;clang (trunk)&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;libstdc++&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;17, 2a&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Standalone&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Linux (Xenial)&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;x86-64&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;—&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;I think it strikes a good balance between exhaustiveness and turnaround time, and we now test the most recent compiler versions to make sure they won’t cause problems on the cutting edge.&lt;/p&gt;

&lt;h1 id=&quot;binary-size&quot;&gt;Binary size&lt;/h1&gt;

&lt;p&gt;It doesn’t matter how good a library is if it’s too big to use within your environment. As with all things in computer science, there is a trade-off between size and speed; seldom can you have both. We have been exploring options to reduce the size of the binary, and this mostly involved removing a lot of the pre-written tables we have (such as the ever-controversial jump table), since it allows the compiler to take into account the specific options it was past and optimize for those constraints (i.e. size and speed) rather than hard-coding in a set configuration as we did with the jump tables.&lt;/p&gt;

&lt;p&gt;Peter Dimov also helped out by transitioning our compile-time system of generating unique parse functions for each permutation of extensions to a runtime system, which drastically decreases the binary size without affecting performance too much.&lt;/p&gt;

&lt;p&gt;I must admit I’m not the biggest fan of these changes, but it’s important to support the use of Boost.JSON in embedded environments. As Peter has said time and time again: don’t overfit for a particular use-case or configuration.&lt;/p&gt;

&lt;p&gt;Another place with room for improvement is with string to float-point conversions. Right now we calculate a mantissa and base-10 exponent, then lookup the value in a massive table that contains pre-calculated powers of 10 from 1e-308 to 1e+308. As you can surmise, this takes up a substantial amount of space (8 bytes * 618 elements = 4.95 kb).&lt;/p&gt;

&lt;p&gt;Here is a boiled down version of how we currently perform the conversion:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-cpp&quot;&gt;double calculate_float(
    std::uint64_t mantissa, 
    std::uint32_t exponent, 
    bool sign)
{
    constexpr static double table[618] = 
    { 
        1e-308, 1e-307, 
        ..., 
        1e307, 1e308 
    };
    double power;
    if(exponent &amp;lt; -308 || exponent &amp;gt; 308)
        power = std::pow(10.0, exponent);
    else
        power = table[exponent + 308]
    double result = mantissa * power;
    return sign ? -result : result;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;To further reduce the size of the binary, Peter suggested that we instead calculate &lt;code&gt;power&lt;/code&gt; as &lt;code&gt;10^floor(exponent / 8) * 10^(exponent mod 8)&lt;/code&gt;. Yes, the division operations there might look expensive, but any decent optimizing compiler will transform &lt;code&gt;exponent / 8&lt;/code&gt; to &lt;code&gt;exponent &amp;gt;&amp;gt; 3&lt;/code&gt;, and &lt;code&gt;exponent mod 8&lt;/code&gt; to &lt;code&gt;exponent &amp;amp; 7&lt;/code&gt;. This does introduce another multiplication instruction, but at the same time, it makes our table 8 times smaller. In theory, the slight drop in performance is worth the significant reduction in binary size.&lt;/p&gt;</content><author><name></name></author><category term="krystian" /><summary type="html">Reviewing the review The review period for Boost.JSON has come and gone, and we got some great feedback on the design of the library. Glancing over the results, it appears that the general mood was to accept the library. This doesn’t mean that there weren’t any problem areas – most notably the documentation, which often did contain the information people wanted, but it was difficult to find. Other points of contention were the use of a push parser as opposed to a pull parser, the use of double, uint64_t, and int64_t without allowing for users to change them, and the value conversion interface. Overall some very good points were made, and I’d like to thank everyone for participating in the review. Customizing the build I put a bit of work into improving our CI matrix, as it had several redundant configurations and did not test newer compiler versions (e.g. GCC 10, clang 11), nor did we have any 32-bit jobs. The most difficult thing about working on the build matrix is balancing how exhaustive it is with the turnaround time – sure, we could add 60 configurations that test x86, x86-64, and ARM on every major compiler version released since 2011, but the turnaround would be abysmal. To alleviate this, I only added 32-bit jobs for the sanitizers that use a recent version of GCC. It’s a less common configuration in the days of 64-bit universality, and if 64 bit works then it’s highly likely that 32 bit will “just work” as well. Here’s a table of the new Travis configurations that will be added: Compiler Library C++ Standard Variant OS Architecture Job — — — Boost Linux (Xenial) x86-64 Documentation gcc 8.4.0 libstdc++ 11 Boost Linux (Xenial) x86-64 Coverage clang 6.0.1 libstdc++ 11, 14 Boost Linux (Xenial) x86-64 Valgrind clang 11.0.0 libstdc++ 17 Boost Linux (Xenial) x86-64 Address Sanitizer clang 11.0.0 libstdc++ 17 Boost Linux (Xenial) x86-64 UB Sanitizer msvc 14.1 MS STL 11, 14, 17 Boost Windows x86-64 — msvc 14.1 MS STL 17, 2a Standalone Windows x86-64 — msvc 14.2 MS STL 17, 2a Boost Windows x86-64 — msvc 14.2 MS STL 17, 2a Standalone Windows x86-64 — icc 2021.1 libstdc++ 11, 14, 17 Boost Linux (Bionic) x86-64 — gcc 4.8.5 libstdc++ 11 Boost Linux (Trusty) x86-64 — gcc 4.9.4 libstdc++ 11 Boost Linux (Trusty) x86-64 — gcc 5.5.0 libstdc++ 11 Boost Linux (Xenial) x86-64 — gcc 6.5.0 libstdc++ 11, 14 Boost Linux (Xenial) x86-64 — gcc 7.5.0 libstdc++ 14, 17 Boost Linux (Xenial) x86-64 — gcc 8.4.0 libstdc++ 17, 2a Boost Linux (Xenial) x86-64 — gcc 9.3.0 libstdc++ 17, 2a Boost Linux (Xenial) x86-64 — gcc 9.3.0 libstdc++ 17, 2a Standalone Linux (Xenial) x86-64 — gcc 10.2.0 libstdc++ 17, 2a Boost Linux (Focal) x86-64 — gcc 10.2.0 libstdc++ 17, 2a Standalone Linux (Focal) x86-64 — gcc (trunk) libstdc++ 17, 2a Boost Linux (Focal) x86-64 — gcc (trunk) libstdc++ 17, 2a Standalone Linux (Focal) x86-64 — clang 3.8.0 libstdc++ 11 Boost Linux (Trusty) x86-64 — clang 4.0.0 libstdc++ 11, 14 Boost Linux (Xenial) x86-64 — clang 5.0.2 libstdc++ 11, 14 Boost Linux (Xenial) x86-64 — clang 6.0.1 libstdc++ 14, 17 Boost Linux (Xenial) x86-64 — clang 7.0.1 libstdc++ 17, 2a Boost Linux (Xenial) x86-64 — clang 9.0.1 libstdc++ 17, 2a Boost Linux (Xenial) x86-64 — clang 9.0.1 libstdc++ 17, 2a Standalone Linux (Xenial) x86-64 — clang 10.0.1 libstdc++ 17, 2a Boost Linux (Xenial) x86-64 — clang 10.0.1 libstdc++ 17, 2a Standalone Linux (Xenial) x86-64 — clang 11.0.0 libstdc++ 17, 2a Boost Linux (Xenial) x86-64 — clang 11.0.0 libstdc++ 17, 2a Standalone Linux (Xenial) x86-64 — clang (trunk) libstdc++ 17, 2a Boost Linux (Xenial) x86-64 — clang (trunk) libstdc++ 17, 2a Standalone Linux (Xenial) x86-64 — I think it strikes a good balance between exhaustiveness and turnaround time, and we now test the most recent compiler versions to make sure they won’t cause problems on the cutting edge. Binary size It doesn’t matter how good a library is if it’s too big to use within your environment. As with all things in computer science, there is a trade-off between size and speed; seldom can you have both. We have been exploring options to reduce the size of the binary, and this mostly involved removing a lot of the pre-written tables we have (such as the ever-controversial jump table), since it allows the compiler to take into account the specific options it was past and optimize for those constraints (i.e. size and speed) rather than hard-coding in a set configuration as we did with the jump tables. Peter Dimov also helped out by transitioning our compile-time system of generating unique parse functions for each permutation of extensions to a runtime system, which drastically decreases the binary size without affecting performance too much. I must admit I’m not the biggest fan of these changes, but it’s important to support the use of Boost.JSON in embedded environments. As Peter has said time and time again: don’t overfit for a particular use-case or configuration. Another place with room for improvement is with string to float-point conversions. Right now we calculate a mantissa and base-10 exponent, then lookup the value in a massive table that contains pre-calculated powers of 10 from 1e-308 to 1e+308. As you can surmise, this takes up a substantial amount of space (8 bytes * 618 elements = 4.95 kb). Here is a boiled down version of how we currently perform the conversion: double calculate_float( std::uint64_t mantissa, std::uint32_t exponent, bool sign) { constexpr static double table[618] = { 1e-308, 1e-307, ..., 1e307, 1e308 }; double power; if(exponent &amp;lt; -308 || exponent &amp;gt; 308) power = std::pow(10.0, exponent); else power = table[exponent + 308] double result = mantissa * power; return sign ? -result : result; } To further reduce the size of the binary, Peter suggested that we instead calculate power as 10^floor(exponent / 8) * 10^(exponent mod 8). Yes, the division operations there might look expensive, but any decent optimizing compiler will transform exponent / 8 to exponent &amp;gt;&amp;gt; 3, and exponent mod 8 to exponent &amp;amp; 7. This does introduce another multiplication instruction, but at the same time, it makes our table 8 times smaller. In theory, the slight drop in performance is worth the significant reduction in binary size.</summary></entry><entry><title type="html">Krystian’s August Update</title><link href="http://cppalliance.org/krystian/2020/09/06/KrystiansAugustUpdate.html" rel="alternate" type="text/html" title="Krystian’s August Update" /><published>2020-09-06T00:00:00+00:00</published><updated>2020-09-06T00:00:00+00:00</updated><id>http://cppalliance.org/krystian/2020/09/06/KrystiansAugustUpdate</id><content type="html" xml:base="http://cppalliance.org/krystian/2020/09/06/KrystiansAugustUpdate.html">&lt;h1 id=&quot;boostjson&quot;&gt;Boost.JSON&lt;/h1&gt;

&lt;p&gt;Boost.JSON is officially scheduled for review! It starts on September 14th, so there isn’t much time left to finish up polishing the library – but it looks like we will make the deadline.&lt;/p&gt;

&lt;h2 id=&quot;optimize-optimize-optimize&quot;&gt;Optimize, optimize, optimize&lt;/h2&gt;

&lt;p&gt;Boost.JSON’s performance has significantly increased in the past month. The change to the parsing functions where we pass and return &lt;code&gt;const char*&lt;/code&gt; instead of &lt;code&gt;result&lt;/code&gt; (detailed in my last post) was merged, bringing large gains across the board. After this, my work on optimizing &lt;code&gt;basic_parser&lt;/code&gt; was complete (for now…), save for a few more minor changes:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;The handler is stored as the first data member as opposed to passing a reference to each parse function. This means that the &lt;code&gt;this&lt;/code&gt; pointer for &lt;code&gt;basic_parser&lt;/code&gt; is the &lt;code&gt;this&lt;/code&gt; pointer for the handler, which eliminates some register spills.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The parser’s depth (i.e. nesting level of objects/arrays) is now tracked as &lt;code&gt;max_depth - actual_depth&lt;/code&gt;, meaning that we don’t have to read &lt;code&gt;max_depth&lt;/code&gt; from memory each time a structure is parsed.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;code&gt;parse_string&lt;/code&gt; was split into two functions: &lt;code&gt;parse_unescaped&lt;/code&gt; and &lt;code&gt;parse_escaped&lt;/code&gt;. The former is much cheaper to call as it doesn’t have to store the string within a local buffer, and since unescaped strings are vastly more common in JSON documents, this increases performance considerably.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;the-dom-parser&quot;&gt;The DOM parser&lt;/h3&gt;

&lt;p&gt;Our old implementation of &lt;code&gt;parser&lt;/code&gt; was pretty wasteful. It stored state information (such as whether we were parsing an object or array), keys, and values, all on one stack. This proved to be quite a pain when it came to unwinding it and also required us to align the stack when pushing arrays and objects.&lt;/p&gt;

&lt;p&gt;Several months ago, Vinnie and I tried to figure out how to make the homogeneous but came to a dead end. I decided to revisit the idea, and after some experimentation, it became apparent that there was a &lt;em&gt;lot&lt;/em&gt; of redundancy in the implementation. For example, &lt;code&gt;basic_parser&lt;/code&gt; already keeps track of the current object/array/string/key size, so there is no reason to so within &lt;code&gt;parser&lt;/code&gt;. The state information we were tracking was also not needed – &lt;code&gt;basic_parser&lt;/code&gt; already checks the syntactic correctness of the input. That left one more thing: strings and keys.&lt;/p&gt;

&lt;p&gt;My rudimentary implementation required two stacks: one for keys and strings, and the other for values. Other information, such as the sizes of objects and arrays, were obtained from &lt;code&gt;basic_parser&lt;/code&gt;. My implementation, though primitive, gave some promising results on the benchmarks: up to 10% for certain documents. After some brainstorming with Vinnie, he had the idea of storing object keys as values; the last piece of the puzzle we needed to make this thing work.&lt;/p&gt;

&lt;p&gt;His fleshed-out implementation was even faster. In just a week’s time, Boost.JSON’s performance increased by some 15%. I’m still working on the finishing touches, but the results are looking promising.&lt;/p&gt;

&lt;h2 id=&quot;more-utf-8-validation-malarkey&quot;&gt;More UTF-8 validation malarkey&lt;/h2&gt;

&lt;p&gt;Out of all the things I’ve worked on, nothing has proved as frustrating as UTF-8 validation. The validation itself is trivial; but making it work with an incremental parser is remarkably difficult. Shortly after merging the feature, &lt;a href=&quot;https://github.com/CPPAlliance/json/issues/162&quot;&gt;an issue was opened&lt;/a&gt;; while validation worked just fine when a document was parsed without suspending, I neglected to write tests for incremental parsing, and that’s precisely where the bug was. Turns out, if parsing suspended while validating a UTF-8 byte sequence, the handler just would not be called.&lt;/p&gt;

&lt;p&gt;This was… quite a problem to say the least, and required me to reimplement UTF-8 validation from scratch – but with a twist. We don’t want to pass partial UTF-8 sequences because it just transfers the burden of assembling incomplete sequences to the handler. This means that we need to store the sequences, append to them until we get a complete codepoint, and only then can we validate and send it off to the handler. Doing this in an efficient manner proved to be quite challenging, so I ended up with a “fix” that was 50% code and 50% &lt;code&gt;// KRYSTIAN TODO: this can be optimized&lt;/code&gt;. The tests provided in the issue finally passed, so the patch was merged.&lt;/p&gt;

&lt;p&gt;I thought my woes with validation were over, but I was wrong. Just over a week later, a new issue rolled in:&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/CPPAlliance/json/issues/162&quot;&gt;Handler not invoked correctly in multi-byte UTF8 sequences, part 2&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Luckily, fixing this didn’t require another rewrite. This taught me a fine lesson in exhaustive testing.&lt;/p&gt;</content><author><name></name></author><category term="krystian" /><summary type="html">Boost.JSON Boost.JSON is officially scheduled for review! It starts on September 14th, so there isn’t much time left to finish up polishing the library – but it looks like we will make the deadline. Optimize, optimize, optimize Boost.JSON’s performance has significantly increased in the past month. The change to the parsing functions where we pass and return const char* instead of result (detailed in my last post) was merged, bringing large gains across the board. After this, my work on optimizing basic_parser was complete (for now…), save for a few more minor changes: The handler is stored as the first data member as opposed to passing a reference to each parse function. This means that the this pointer for basic_parser is the this pointer for the handler, which eliminates some register spills. The parser’s depth (i.e. nesting level of objects/arrays) is now tracked as max_depth - actual_depth, meaning that we don’t have to read max_depth from memory each time a structure is parsed. parse_string was split into two functions: parse_unescaped and parse_escaped. The former is much cheaper to call as it doesn’t have to store the string within a local buffer, and since unescaped strings are vastly more common in JSON documents, this increases performance considerably. The DOM parser Our old implementation of parser was pretty wasteful. It stored state information (such as whether we were parsing an object or array), keys, and values, all on one stack. This proved to be quite a pain when it came to unwinding it and also required us to align the stack when pushing arrays and objects. Several months ago, Vinnie and I tried to figure out how to make the homogeneous but came to a dead end. I decided to revisit the idea, and after some experimentation, it became apparent that there was a lot of redundancy in the implementation. For example, basic_parser already keeps track of the current object/array/string/key size, so there is no reason to so within parser. The state information we were tracking was also not needed – basic_parser already checks the syntactic correctness of the input. That left one more thing: strings and keys. My rudimentary implementation required two stacks: one for keys and strings, and the other for values. Other information, such as the sizes of objects and arrays, were obtained from basic_parser. My implementation, though primitive, gave some promising results on the benchmarks: up to 10% for certain documents. After some brainstorming with Vinnie, he had the idea of storing object keys as values; the last piece of the puzzle we needed to make this thing work. His fleshed-out implementation was even faster. In just a week’s time, Boost.JSON’s performance increased by some 15%. I’m still working on the finishing touches, but the results are looking promising. More UTF-8 validation malarkey Out of all the things I’ve worked on, nothing has proved as frustrating as UTF-8 validation. The validation itself is trivial; but making it work with an incremental parser is remarkably difficult. Shortly after merging the feature, an issue was opened; while validation worked just fine when a document was parsed without suspending, I neglected to write tests for incremental parsing, and that’s precisely where the bug was. Turns out, if parsing suspended while validating a UTF-8 byte sequence, the handler just would not be called. This was… quite a problem to say the least, and required me to reimplement UTF-8 validation from scratch – but with a twist. We don’t want to pass partial UTF-8 sequences because it just transfers the burden of assembling incomplete sequences to the handler. This means that we need to store the sequences, append to them until we get a complete codepoint, and only then can we validate and send it off to the handler. Doing this in an efficient manner proved to be quite challenging, so I ended up with a “fix” that was 50% code and 50% // KRYSTIAN TODO: this can be optimized. The tests provided in the issue finally passed, so the patch was merged. I thought my woes with validation were over, but I was wrong. Just over a week later, a new issue rolled in: Handler not invoked correctly in multi-byte UTF8 sequences, part 2 Luckily, fixing this didn’t require another rewrite. This taught me a fine lesson in exhaustive testing.</summary></entry><entry><title type="html">Richard’s August Update</title><link href="http://cppalliance.org/richard/2020/09/01/RichardsAugustUpdate.html" rel="alternate" type="text/html" title="Richard’s August Update" /><published>2020-09-01T00:00:00+00:00</published><updated>2020-09-01T00:00:00+00:00</updated><id>http://cppalliance.org/richard/2020/09/01/RichardsAugustUpdate</id><content type="html" xml:base="http://cppalliance.org/richard/2020/09/01/RichardsAugustUpdate.html">&lt;h1 id=&quot;new-debugging-feature-in-asio-and-beast&quot;&gt;New Debugging Feature in Asio and Beast&lt;/h1&gt;

&lt;p&gt;As covered previously, Boost 1.74 brought an implementation of the new unified executors model to Boost.Asio.&lt;/p&gt;

&lt;p&gt;Support for this is not the only thing that is new in Beast.&lt;/p&gt;

&lt;p&gt;Chris Kohlhoff recently submitted a &lt;a href=&quot;https://github.com/boostorg/beast/pull/2053&quot;&gt;PR&lt;/a&gt; to Beast’s repository 
demonstrating how to annotate source code with the &lt;code&gt;BOOST_ASIO_HANDLER_LOCATION&lt;/code&gt; macro. I have since followed up and 
annotated all asynchronous operations in Beast this way.&lt;/p&gt;

&lt;p&gt;In a normal build, there is no effect (and zero extra code generation). However, defining the preprocessor macro 
&lt;code&gt;BOOST_ASIO_ENABLE_HANDLER_TRACKING&lt;/code&gt; will cause these macros to generate code which will emit handler tracking
log data to stdout in a very specific format.&lt;/p&gt;

&lt;p&gt;The output is designed to describe the flow of asynchronous events in a format suitable for generating a visualisation
in linear terms. i.e. the asynchronous events are flattened and linked to show causality.&lt;/p&gt;

&lt;p&gt;Here is an example of the output:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;@asio|1597543084.233257|&amp;gt;33|
@asio|1597543084.233273|33|deadline_timer@0x7fa6cac25218.cancel
@asio|1597543084.233681|33^34|in 'basic_stream::async_write_some' (../../../../../../boost/beast/core/impl/basic_stream.hpp:321)
@asio|1597543084.233681|33^34|called from 'async_write' (../../../../../../boost/asio/impl/write.hpp:331)
@asio|1597543084.233681|33^34|called from 'ssl::stream&amp;lt;&amp;gt;::async_write_some' (../../../../../../boost/asio/ssl/detail/io.hpp:201)
@asio|1597543084.233681|33^34|called from 'http::async_write_some' (../../../../../../boost/beast/http/impl/write.hpp:64)
@asio|1597543084.233681|33^34|called from 'http::async_write' (../../../../../../boost/beast/http/impl/write.hpp:223)
@asio|1597543084.233681|33^34|called from 'http::async_write(msg)' (../../../../../../boost/beast/http/impl/write.hpp:277)
@asio|1597543084.233681|33*34|deadline_timer@0x7fa6cac25298.async_wait
@asio|1597543084.233801|33^35|in 'basic_stream::async_write_some' (../../../../../../boost/beast/core/impl/basic_stream.hpp:373)
@asio|1597543084.233801|33^35|called from 'async_write' (../../../../../../boost/asio/impl/write.hpp:331)
@asio|1597543084.233801|33^35|called from 'ssl::stream&amp;lt;&amp;gt;::async_write_some' (../../../../../../boost/asio/ssl/detail/io.hpp:201)
@asio|1597543084.233801|33^35|called from 'http::async_write_some' (../../../../../../boost/beast/http/impl/write.hpp:64)
@asio|1597543084.233801|33^35|called from 'http::async_write' (../../../../../../boost/beast/http/impl/write.hpp:223)
@asio|1597543084.233801|33^35|called from 'http::async_write(msg)' (../../../../../../boost/beast/http/impl/write.hpp:277)
@asio|1597543084.233801|33*35|socket@0x7fa6cac251c8.async_send
@asio|1597543084.233910|.35|non_blocking_send,ec=system:0,bytes_transferred=103
@asio|1597543084.233949|&amp;lt;33|
@asio|1597543084.233983|&amp;lt;31|
@asio|1597543084.234031|&amp;gt;30|ec=system:89
@asio|1597543084.234045|30*36|strand_executor@0x7fa6cac24bd0.execute
@asio|1597543084.234054|&amp;gt;36|
@asio|1597543084.234064|&amp;lt;36|
@asio|1597543084.234072|&amp;lt;30|
@asio|1597543084.234086|&amp;gt;35|ec=system:0,bytes_transferred=103
@asio|1597543084.234100|35*37|strand_executor@0x7fa6cac24bd0.execute
@asio|1597543084.234109|&amp;gt;37|
@asio|1597543084.234119|37|deadline_timer@0x7fa6cac25298.cancel
@asio|1597543084.234198|37^38|in 'basic_stream::async_read_some' (../../../../../../boost/beast/core/impl/basic_stream.hpp:321)
@asio|1597543084.234198|37^38|called from 'ssl::stream&amp;lt;&amp;gt;::async_read_some' (../../../../../../boost/asio/ssl/detail/io.hpp:168)
@asio|1597543084.234198|37^38|called from 'http::async_read_some' (../../../../../../boost/beast/http/impl/read.hpp:212)
@asio|1597543084.234198|37^38|called from 'http::async_read' (../../../../../../boost/beast/http/impl/read.hpp:297)
@asio|1597543084.234198|37^38|called from 'http::async_read(msg)' (../../../../../../boost/beast/http/impl/read.hpp:101)
@asio|1597543084.234198|37*38|deadline_timer@0x7fa6cac25218.async_wait
@asio|1597543084.234288|37^39|in 'basic_stream::async_read_some' (../../../../../../boost/beast/core/impl/basic_stream.hpp:373)
@asio|1597543084.234288|37^39|called from 'ssl::stream&amp;lt;&amp;gt;::async_read_some' (../../../../../../boost/asio/ssl/detail/io.hpp:168)
@asio|1597543084.234288|37^39|called from 'http::async_read_some' (../../../../../../boost/beast/http/impl/read.hpp:212)
@asio|1597543084.234288|37^39|called from 'http::async_read' (../../../../../../boost/beast/http/impl/read.hpp:297)
@asio|1597543084.234288|37^39|called from 'http::async_read(msg)' (../../../../../../boost/beast/http/impl/read.hpp:101)
@asio|1597543084.234288|37*39|socket@0x7fa6cac251c8.async_receive
@asio|1597543084.234334|.39|non_blocking_recv,ec=system:35,bytes_transferred=0
@asio|1597543084.234353|&amp;lt;37|
@asio|1597543084.234364|&amp;lt;35|
@asio|1597543084.234380|&amp;gt;34|ec=system:89
@asio|1597543084.234392|34*40|strand_executor@0x7fa6cac24bd0.execute
@asio|1597543084.234401|&amp;gt;40|
@asio|1597543084.234408|&amp;lt;40|
@asio|1597543084.234416|&amp;lt;34|
@asio|1597543084.427594|.39|non_blocking_recv,ec=system:0,bytes_transferred=534
@asio|1597543084.427680|&amp;gt;39|ec=system:0,bytes_transferred=534
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;So far, so good. But not very informative or friendly to the native eye.&lt;/p&gt;

&lt;p&gt;Fortunately as of Boost 1.74 there is a tool in the Asio source tree to convert this data into something consumable by the open source
tool dot, which can then output the resulting execution graph in one of a number of common graphical formats such as
PNG, BMP, SVG and many others.&lt;/p&gt;

&lt;p&gt;Here is an example of a visualisation of a simple execution graph:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/posts/richard/2020-09-01-handler-tracking-example.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The tool you need to do this is in the &lt;code&gt;asio&lt;/code&gt; subproject of the Boost repo. The full path is 
&lt;code&gt;libs/asio/tools/handlerviz.pl&lt;/code&gt;. The command is self-documenting but for clarity, the process would be like this:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Compile and link your program with the compiler flag &lt;code&gt;-DBOOST_ASIO_ENABLE_HANDLER_TRACKING&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;run your program, capturing stdout to a file (say &lt;code&gt;mylog.txt&lt;/code&gt;) (or you can pipe it to the next step)&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;handlerviz.pl &amp;lt; mylog.txt | dot -Tpng mygraph.png&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;You should now be able to view your graph in a web browser, editor or picture viewer.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The documentation for dot is &lt;a href=&quot;https://linux.die.net/man/1/dot&quot;&gt;here&lt;/a&gt; dot is usually available in the graphviz package 
of your linux distro/brew cask. Windows users can download an executable suite 
&lt;a href=&quot;https://www.graphviz.org/download/&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;If you have written your own asynchronous operations to compliment Beast or Asio, or indeed you just wish you add your
handler locations to the graph output, you can do so by inserting the &lt;code&gt;BOOST_ASIO_HANDLER_LOCATION&lt;/code&gt; macro just before
each asynchronous suspension point (i.e. just before the call to &lt;code&gt;async_xxx&lt;/code&gt;). If you’re doing this in an Asio 
&lt;code&gt;coroutine&lt;/code&gt; (not to be confused with C++ coroutines) then be sure to place the macro in curly braces after the 
YIELD macro, for example:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;    ...

    // this marks a suspension point of the coroutine
    BOOST_ASIO_CORO_YIELD
    {
        // This macro creates scoped variables so must be in a private scope
        BOOST_ASIO_HANDLER_LOCATION((           // note: double open brackets
            __FILE__, __LINE__,                 // source location
            &quot;websocket::tcp::async_teardown&quot;    // name of the initiating function
        ));

        // this is the initiation of the next inner asynchronous operation
        s_.async_wait(
            net::socket_base::wait_read,
                beast::detail::bind_continuation(std::move(*this)));

        // there is an implied return statement here
    }

    ...
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;When writing applications, people historically have used Continuation Passing Style when calling asynchronous 
operations, capturing a shared_ptr to the connection implementation in each handler (continuation).&lt;/p&gt;

&lt;p&gt;When using this macro in user code with written in continuation passing style, you might do so like this:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;void send_request(http::request&amp;lt;http::string_body&amp;gt; req)
{
    send_queue_.push_back(std::move(req));
    if (!sending_)
    {
        sending_ = true;
        maybe_initiate_send();
    }
}

void my_connection_impl::maybe_initiate_send()
{
    if (send_queue_.empty())
    {
        sending_ = false;
        return;
    }

    // assume request_queue_ is a std::deque so elements will have stable addresses
    auto&amp;amp; current_request = request_queue_.front(); 

    BOOST_ASIO_HANDLER_LOCATION((
        __FILE__, __LINE__,
        &quot;my_connection_impl::maybe_initiate_send&quot;
    ));

    // suspension point

    boost::beast::http::async_write(stream_, current_request_, 
        [self = this-&amp;gt;shared_from_this()](boost::beast::error_code ec, std::size_t)
        {
            // continuation

            if (!ec)
            {
                self-&amp;gt;request_queue_.pop_front();
                self-&amp;gt;maybe_initiate_send();
            }
            else
            {
                // handle error
            }
        });
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;If you’re using c++ coroutines it becomes a little more complicated as you want the lifetime of the tracking
state to be destroyed after the asynchronous initiation function but before the coroutine continuation:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;namespace net = boost::asio;
namespace http = boost::beast::http;

auto connect_and_send(
    boost::asio::ip::tcp::socket&amp;amp; stream, 
    std::string host, 
    std::string port, 
    http::request&amp;lt;http::string_body&amp;gt; req) 
-&amp;gt; net::awaitable&amp;lt;void&amp;gt;
{
    namespace net = boost::asio;
    
    auto resolver = net::ip::tcp::resolver(co_await net::this_coro::executor);

    // suspension point coming up

    auto oresults = std::optional&amp;lt;net::awaitable&amp;lt;net::ip::tcp::resolver::results_type&amp;gt;&amp;gt;();
    {
        BOOST_ASIO_HANDLER_LOCATION((
            __FILE__, __LINE__,
            &quot;my_connection_impl::connect_and_send&quot;
        ));
        oresults.emplace(resolver.async_resolve(host, port, net::use_awaitable));
    }
    auto results = co_await std::move(*oresults);

    auto oconnect = std::optional&amp;lt;net::awaitable&amp;lt;net::ip::tcp::endpoint&amp;gt;&amp;gt;();
    {
        BOOST_ASIO_HANDLER_LOCATION((
            __FILE__, __LINE__,
            &quot;my_connection_impl::connect_and_send&quot;
        ));
        oconnect.emplace(net::async_connect(stream, results, net::use_awaitable));
    }
    auto ep = co_await *std::move(oconnect);

    // ... and so on ...

}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Which might look a little unwieldy compared to the unannotated code, which could look like this:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;auto connect_and_send(
    boost::asio::ip::tcp::socket&amp;amp; stream, 
    std::string host, 
    std::string port, 
    http::request&amp;lt;http::string_body&amp;gt; req) 
-&amp;gt; net::awaitable&amp;lt;void&amp;gt;
{
    namespace net = boost::asio;
    
    auto resolver = net::ip::tcp::resolver(co_await net::this_coro::executor);

    auto ep = co_await net::async_connect(stream, 
                            co_await resolver.async_resolve(host, port, net::use_awaitable), 
                            net::use_awaitable);

    // ... and so on ...

}
&lt;/code&gt;&lt;/pre&gt;</content><author><name></name></author><category term="richard" /><summary type="html">New Debugging Feature in Asio and Beast As covered previously, Boost 1.74 brought an implementation of the new unified executors model to Boost.Asio. Support for this is not the only thing that is new in Beast. Chris Kohlhoff recently submitted a PR to Beast’s repository demonstrating how to annotate source code with the BOOST_ASIO_HANDLER_LOCATION macro. I have since followed up and annotated all asynchronous operations in Beast this way. In a normal build, there is no effect (and zero extra code generation). However, defining the preprocessor macro BOOST_ASIO_ENABLE_HANDLER_TRACKING will cause these macros to generate code which will emit handler tracking log data to stdout in a very specific format. The output is designed to describe the flow of asynchronous events in a format suitable for generating a visualisation in linear terms. i.e. the asynchronous events are flattened and linked to show causality. Here is an example of the output: @asio|1597543084.233257|&amp;gt;33| @asio|1597543084.233273|33|deadline_timer@0x7fa6cac25218.cancel @asio|1597543084.233681|33^34|in 'basic_stream::async_write_some' (../../../../../../boost/beast/core/impl/basic_stream.hpp:321) @asio|1597543084.233681|33^34|called from 'async_write' (../../../../../../boost/asio/impl/write.hpp:331) @asio|1597543084.233681|33^34|called from 'ssl::stream&amp;lt;&amp;gt;::async_write_some' (../../../../../../boost/asio/ssl/detail/io.hpp:201) @asio|1597543084.233681|33^34|called from 'http::async_write_some' (../../../../../../boost/beast/http/impl/write.hpp:64) @asio|1597543084.233681|33^34|called from 'http::async_write' (../../../../../../boost/beast/http/impl/write.hpp:223) @asio|1597543084.233681|33^34|called from 'http::async_write(msg)' (../../../../../../boost/beast/http/impl/write.hpp:277) @asio|1597543084.233681|33*34|deadline_timer@0x7fa6cac25298.async_wait @asio|1597543084.233801|33^35|in 'basic_stream::async_write_some' (../../../../../../boost/beast/core/impl/basic_stream.hpp:373) @asio|1597543084.233801|33^35|called from 'async_write' (../../../../../../boost/asio/impl/write.hpp:331) @asio|1597543084.233801|33^35|called from 'ssl::stream&amp;lt;&amp;gt;::async_write_some' (../../../../../../boost/asio/ssl/detail/io.hpp:201) @asio|1597543084.233801|33^35|called from 'http::async_write_some' (../../../../../../boost/beast/http/impl/write.hpp:64) @asio|1597543084.233801|33^35|called from 'http::async_write' (../../../../../../boost/beast/http/impl/write.hpp:223) @asio|1597543084.233801|33^35|called from 'http::async_write(msg)' (../../../../../../boost/beast/http/impl/write.hpp:277) @asio|1597543084.233801|33*35|socket@0x7fa6cac251c8.async_send @asio|1597543084.233910|.35|non_blocking_send,ec=system:0,bytes_transferred=103 @asio|1597543084.233949|&amp;lt;33| @asio|1597543084.233983|&amp;lt;31| @asio|1597543084.234031|&amp;gt;30|ec=system:89 @asio|1597543084.234045|30*36|strand_executor@0x7fa6cac24bd0.execute @asio|1597543084.234054|&amp;gt;36| @asio|1597543084.234064|&amp;lt;36| @asio|1597543084.234072|&amp;lt;30| @asio|1597543084.234086|&amp;gt;35|ec=system:0,bytes_transferred=103 @asio|1597543084.234100|35*37|strand_executor@0x7fa6cac24bd0.execute @asio|1597543084.234109|&amp;gt;37| @asio|1597543084.234119|37|deadline_timer@0x7fa6cac25298.cancel @asio|1597543084.234198|37^38|in 'basic_stream::async_read_some' (../../../../../../boost/beast/core/impl/basic_stream.hpp:321) @asio|1597543084.234198|37^38|called from 'ssl::stream&amp;lt;&amp;gt;::async_read_some' (../../../../../../boost/asio/ssl/detail/io.hpp:168) @asio|1597543084.234198|37^38|called from 'http::async_read_some' (../../../../../../boost/beast/http/impl/read.hpp:212) @asio|1597543084.234198|37^38|called from 'http::async_read' (../../../../../../boost/beast/http/impl/read.hpp:297) @asio|1597543084.234198|37^38|called from 'http::async_read(msg)' (../../../../../../boost/beast/http/impl/read.hpp:101) @asio|1597543084.234198|37*38|deadline_timer@0x7fa6cac25218.async_wait @asio|1597543084.234288|37^39|in 'basic_stream::async_read_some' (../../../../../../boost/beast/core/impl/basic_stream.hpp:373) @asio|1597543084.234288|37^39|called from 'ssl::stream&amp;lt;&amp;gt;::async_read_some' (../../../../../../boost/asio/ssl/detail/io.hpp:168) @asio|1597543084.234288|37^39|called from 'http::async_read_some' (../../../../../../boost/beast/http/impl/read.hpp:212) @asio|1597543084.234288|37^39|called from 'http::async_read' (../../../../../../boost/beast/http/impl/read.hpp:297) @asio|1597543084.234288|37^39|called from 'http::async_read(msg)' (../../../../../../boost/beast/http/impl/read.hpp:101) @asio|1597543084.234288|37*39|socket@0x7fa6cac251c8.async_receive @asio|1597543084.234334|.39|non_blocking_recv,ec=system:35,bytes_transferred=0 @asio|1597543084.234353|&amp;lt;37| @asio|1597543084.234364|&amp;lt;35| @asio|1597543084.234380|&amp;gt;34|ec=system:89 @asio|1597543084.234392|34*40|strand_executor@0x7fa6cac24bd0.execute @asio|1597543084.234401|&amp;gt;40| @asio|1597543084.234408|&amp;lt;40| @asio|1597543084.234416|&amp;lt;34| @asio|1597543084.427594|.39|non_blocking_recv,ec=system:0,bytes_transferred=534 @asio|1597543084.427680|&amp;gt;39|ec=system:0,bytes_transferred=534 So far, so good. But not very informative or friendly to the native eye. Fortunately as of Boost 1.74 there is a tool in the Asio source tree to convert this data into something consumable by the open source tool dot, which can then output the resulting execution graph in one of a number of common graphical formats such as PNG, BMP, SVG and many others. Here is an example of a visualisation of a simple execution graph: The tool you need to do this is in the asio subproject of the Boost repo. The full path is libs/asio/tools/handlerviz.pl. The command is self-documenting but for clarity, the process would be like this: Compile and link your program with the compiler flag -DBOOST_ASIO_ENABLE_HANDLER_TRACKING run your program, capturing stdout to a file (say mylog.txt) (or you can pipe it to the next step) handlerviz.pl &amp;lt; mylog.txt | dot -Tpng mygraph.png You should now be able to view your graph in a web browser, editor or picture viewer. The documentation for dot is here dot is usually available in the graphviz package of your linux distro/brew cask. Windows users can download an executable suite here. If you have written your own asynchronous operations to compliment Beast or Asio, or indeed you just wish you add your handler locations to the graph output, you can do so by inserting the BOOST_ASIO_HANDLER_LOCATION macro just before each asynchronous suspension point (i.e. just before the call to async_xxx). If you’re doing this in an Asio coroutine (not to be confused with C++ coroutines) then be sure to place the macro in curly braces after the YIELD macro, for example: ... // this marks a suspension point of the coroutine BOOST_ASIO_CORO_YIELD { // This macro creates scoped variables so must be in a private scope BOOST_ASIO_HANDLER_LOCATION(( // note: double open brackets __FILE__, __LINE__, // source location &quot;websocket::tcp::async_teardown&quot; // name of the initiating function )); // this is the initiation of the next inner asynchronous operation s_.async_wait( net::socket_base::wait_read, beast::detail::bind_continuation(std::move(*this))); // there is an implied return statement here } ... When writing applications, people historically have used Continuation Passing Style when calling asynchronous operations, capturing a shared_ptr to the connection implementation in each handler (continuation). When using this macro in user code with written in continuation passing style, you might do so like this: void send_request(http::request&amp;lt;http::string_body&amp;gt; req) { send_queue_.push_back(std::move(req)); if (!sending_) { sending_ = true; maybe_initiate_send(); } } void my_connection_impl::maybe_initiate_send() { if (send_queue_.empty()) { sending_ = false; return; } // assume request_queue_ is a std::deque so elements will have stable addresses auto&amp;amp; current_request = request_queue_.front(); BOOST_ASIO_HANDLER_LOCATION(( __FILE__, __LINE__, &quot;my_connection_impl::maybe_initiate_send&quot; )); // suspension point boost::beast::http::async_write(stream_, current_request_, [self = this-&amp;gt;shared_from_this()](boost::beast::error_code ec, std::size_t) { // continuation if (!ec) { self-&amp;gt;request_queue_.pop_front(); self-&amp;gt;maybe_initiate_send(); } else { // handle error } }); } If you’re using c++ coroutines it becomes a little more complicated as you want the lifetime of the tracking state to be destroyed after the asynchronous initiation function but before the coroutine continuation: namespace net = boost::asio; namespace http = boost::beast::http; auto connect_and_send( boost::asio::ip::tcp::socket&amp;amp; stream, std::string host, std::string port, http::request&amp;lt;http::string_body&amp;gt; req) -&amp;gt; net::awaitable&amp;lt;void&amp;gt; { namespace net = boost::asio; auto resolver = net::ip::tcp::resolver(co_await net::this_coro::executor); // suspension point coming up auto oresults = std::optional&amp;lt;net::awaitable&amp;lt;net::ip::tcp::resolver::results_type&amp;gt;&amp;gt;(); { BOOST_ASIO_HANDLER_LOCATION(( __FILE__, __LINE__, &quot;my_connection_impl::connect_and_send&quot; )); oresults.emplace(resolver.async_resolve(host, port, net::use_awaitable)); } auto results = co_await std::move(*oresults); auto oconnect = std::optional&amp;lt;net::awaitable&amp;lt;net::ip::tcp::endpoint&amp;gt;&amp;gt;(); { BOOST_ASIO_HANDLER_LOCATION(( __FILE__, __LINE__, &quot;my_connection_impl::connect_and_send&quot; )); oconnect.emplace(net::async_connect(stream, results, net::use_awaitable)); } auto ep = co_await *std::move(oconnect); // ... and so on ... } Which might look a little unwieldy compared to the unannotated code, which could look like this: auto connect_and_send( boost::asio::ip::tcp::socket&amp;amp; stream, std::string host, std::string port, http::request&amp;lt;http::string_body&amp;gt; req) -&amp;gt; net::awaitable&amp;lt;void&amp;gt; { namespace net = boost::asio; auto resolver = net::ip::tcp::resolver(co_await net::this_coro::executor); auto ep = co_await net::async_connect(stream, co_await resolver.async_resolve(host, port, net::use_awaitable), net::use_awaitable); // ... and so on ... }</summary></entry><entry><title type="html">Krystian’s July Update</title><link href="http://cppalliance.org/krystian/2020/08/01/KrystiansJulyUpdate.html" rel="alternate" type="text/html" title="Krystian’s July Update" /><published>2020-08-01T00:00:00+00:00</published><updated>2020-08-01T00:00:00+00:00</updated><id>http://cppalliance.org/krystian/2020/08/01/KrystiansJulyUpdate</id><content type="html" xml:base="http://cppalliance.org/krystian/2020/08/01/KrystiansJulyUpdate.html">&lt;h1 id=&quot;what-ive-been-doing&quot;&gt;What I’ve been doing&lt;/h1&gt;

&lt;p&gt;I’ve been spending a &lt;em&gt;lot&lt;/em&gt; of time working on optimizing the parser; perhaps a bit too much. Nevertheless, it’s very enjoyable and in doing so I’ve learned more than I could hope to ever learn in school. In addition to the optimization, comment and trailing comma support finally got merged, and I implemented UTF-8 validation (enabled by default, but it can be disabled).&lt;/p&gt;

&lt;h2 id=&quot;utf-8-validation&quot;&gt;UTF-8 validation&lt;/h2&gt;

&lt;p&gt;Prior to implementing this extension (or rather, feature which can be disabled), the parser considers any character appearing within a string to be valid, so long as it wasn’t a control character or formed an illegal escape. While this is &lt;em&gt;fast&lt;/em&gt;, it technically does not conform to the JSON standard.&lt;/p&gt;

&lt;p&gt;As per Section 2 of the &lt;a href=&quot;http://www.ecma-international.org/publications/files/ECMA-ST/ECMA-404.pdf&quot;&gt;JSON Data Interchange Syntax Standard&lt;/a&gt;:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;A conforming JSON text is a sequence of Unicode code points that strictly conforms to the JSON grammar defined by this specification.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;As with most standardese, this particular requirement for conformance is not outright stated, but rather implied. Anyways, that’s enough standardese talk for this post.&lt;/p&gt;

&lt;p&gt;After working on this parser so much, I’ve pretty much got the suspend/resume idiom we use nailed down, so integrating it with the string parsing function was trivial… the actual validation, not so much. I hadn’t the slightest clue about any of the terminology used in the Unicode standard, so it took a good couple of hours to point myself in the right direction. Anyways, a lot of Googling and a messy python script for generating valid and invalid byte sequences later, I had something functional.&lt;/p&gt;

&lt;p&gt;Then came my favorite part: optimization.&lt;/p&gt;

&lt;p&gt;The first byte within a UTF-8 byte sequence determines how many bytes will follow, as well as the valid ranges for these following bytes. Since this byte has such a large valid range, I settled on using a lookup table to check whether the first byte is valid.&lt;/p&gt;

&lt;p&gt;Luckily, the following bytes have ranges that can be trivially checked using a mask. For example, if the first byte is &lt;code&gt;0xE1&lt;/code&gt;, then the byte sequence will be composed of three bytes, the latter two having a valid range of &lt;code&gt;0x80&lt;/code&gt; to &lt;code&gt;0xBF&lt;/code&gt;. Thus, our fast-path routine to verify this sequence can be written as:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-cpp&quot;&gt;uint32_t v;
// this is reversed on big-endian
std::memcpy(&amp;amp;v, bytes, 4); // 4 bytes load

switch (lookup_table[v &amp;amp; 0x7F]) // mask out the most significant bit
{
...
case 3:
	if ((v &amp;amp; 0x00C0C000) == 0x00808000)
		return result::ok;
	return result::fail;
...
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This works well for all but one byte sequence combination. For whatever reason, UTF-8 byte sequences that start with &lt;code&gt;0xF0&lt;/code&gt; can have a second byte between &lt;code&gt;0x90&lt;/code&gt; and &lt;code&gt;0xBF&lt;/code&gt; which requires the check to be done as:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-cpp&quot;&gt;(v &amp;amp; 0xC0C0FF00) + 0x7F7F7000 &amp;lt;= 0x00002F00
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;It’s a weird little outlier that I spent way too much time trying to figure out.&lt;/p&gt;

&lt;p&gt;Since our parser supports incremental parsing, we only take the fast path if the input stream has four or more bytes remaining. If this condition isn’t met, we have to check each byte individually. It’s slower, but shouldn’t happen often.&lt;/p&gt;

&lt;h2 id=&quot;other-optimizations&quot;&gt;Other optimizations&lt;/h2&gt;

&lt;p&gt;I’ve been trying out a number of different optimizations to squeeze all the performance we can get out of the parser. Most recently, I rewrote the parser functions to take a &lt;code&gt;const char*&lt;/code&gt; parameter indicating the start of the value, and return a pointer to the end of the value (if parsing succeeds) or &lt;code&gt;nullptr&lt;/code&gt; upon failure or partial parsing.&lt;/p&gt;

&lt;p&gt;Since I’m not great at explaining things, here’s the before:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-cpp&quot;&gt;result parse_array(const_stream&amp;amp;);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;and here’s the after:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-cpp&quot;&gt;const char* parse_array(const char*);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This allows us to keep the pointer to the current position in the stream entirely within the registers when parsing a document. Since the value is local to the function, the compiler no longer needs to write it to the &lt;code&gt;const_stream&lt;/code&gt; object at the top of the call stack (created within &lt;code&gt;basic_parser::write_some&lt;/code&gt;), nor read it each time a nested value is parsed. This yields an &lt;em&gt;8%&lt;/em&gt; boost in performance across the board.&lt;/p&gt;

&lt;p&gt;More time was spent optimizing the SSE2 functions used for parsing unescaped strings and whitespace as well. Within &lt;code&gt;count_whitespace&lt;/code&gt;, we were able to get rid of a &lt;code&gt;_mm_cmpeq_epi8&lt;/code&gt; (&lt;code&gt;PCMPEQB&lt;/code&gt;) instruction by performing a bitwise or with 4 after testing for spaces, and then comparing the result with &lt;code&gt;'\r'&lt;/code&gt;, as the ASCII value of tab (&lt;code&gt;'\t'&lt;/code&gt;) only differs from that of the carriage return by the third least significant bit. This was something that clang was doing for us, but it’s nice to implement it for all other compilers.&lt;/p&gt;

&lt;p&gt;For &lt;code&gt;count_unescaped&lt;/code&gt; (used to parse unescaped strings), we were able to again reduce the length of the hot path, this time a bit more significantly. Instead of checking for control characters by means of relational comparison, we can instead check for quotes and backslash first, and once that’s done, the &lt;code&gt;_mm_min_epu8&lt;/code&gt; (&lt;code&gt;PMINUB&lt;/code&gt;) instruction can be used to set all control characters (0 - 31) to 31, and then test for equality. This brought our performance on the &lt;code&gt;strings.json&lt;/code&gt; benchmark past the 8 GB/s mark from around 7.7 GB/s. Combined with the optimization of how the stream pointer is passed around, we now hit just a hair under 8.5 GB/s on this benchmark.&lt;/p&gt;

&lt;h2 id=&quot;the-important-but-boring-stuff&quot;&gt;The important but boring stuff&lt;/h2&gt;

&lt;p&gt;After merging the parser extensions, there was a bunch of housekeeping to do such as improving coverage and writing documentation. Though these are far from being my favorite tasks, they are integral to writing a good library, so it must be done. My initial approach to writing tests for the parser extensions was to run each test on every parser configuration we have, but this soon proved to be a nonoptimal approach when the time taken to run the test suite quadrupled. I ended up doing the right thing by making the tests more surgical in nature, and in doing so we even got 100% coverage on the parser.&lt;/p&gt;</content><author><name></name></author><category term="krystian" /><summary type="html">What I’ve been doing I’ve been spending a lot of time working on optimizing the parser; perhaps a bit too much. Nevertheless, it’s very enjoyable and in doing so I’ve learned more than I could hope to ever learn in school. In addition to the optimization, comment and trailing comma support finally got merged, and I implemented UTF-8 validation (enabled by default, but it can be disabled). UTF-8 validation Prior to implementing this extension (or rather, feature which can be disabled), the parser considers any character appearing within a string to be valid, so long as it wasn’t a control character or formed an illegal escape. While this is fast, it technically does not conform to the JSON standard. As per Section 2 of the JSON Data Interchange Syntax Standard: A conforming JSON text is a sequence of Unicode code points that strictly conforms to the JSON grammar defined by this specification. As with most standardese, this particular requirement for conformance is not outright stated, but rather implied. Anyways, that’s enough standardese talk for this post. After working on this parser so much, I’ve pretty much got the suspend/resume idiom we use nailed down, so integrating it with the string parsing function was trivial… the actual validation, not so much. I hadn’t the slightest clue about any of the terminology used in the Unicode standard, so it took a good couple of hours to point myself in the right direction. Anyways, a lot of Googling and a messy python script for generating valid and invalid byte sequences later, I had something functional. Then came my favorite part: optimization. The first byte within a UTF-8 byte sequence determines how many bytes will follow, as well as the valid ranges for these following bytes. Since this byte has such a large valid range, I settled on using a lookup table to check whether the first byte is valid. Luckily, the following bytes have ranges that can be trivially checked using a mask. For example, if the first byte is 0xE1, then the byte sequence will be composed of three bytes, the latter two having a valid range of 0x80 to 0xBF. Thus, our fast-path routine to verify this sequence can be written as: uint32_t v; // this is reversed on big-endian std::memcpy(&amp;amp;v, bytes, 4); // 4 bytes load switch (lookup_table[v &amp;amp; 0x7F]) // mask out the most significant bit { ... case 3: if ((v &amp;amp; 0x00C0C000) == 0x00808000) return result::ok; return result::fail; ... } This works well for all but one byte sequence combination. For whatever reason, UTF-8 byte sequences that start with 0xF0 can have a second byte between 0x90 and 0xBF which requires the check to be done as: (v &amp;amp; 0xC0C0FF00) + 0x7F7F7000 &amp;lt;= 0x00002F00 It’s a weird little outlier that I spent way too much time trying to figure out. Since our parser supports incremental parsing, we only take the fast path if the input stream has four or more bytes remaining. If this condition isn’t met, we have to check each byte individually. It’s slower, but shouldn’t happen often. Other optimizations I’ve been trying out a number of different optimizations to squeeze all the performance we can get out of the parser. Most recently, I rewrote the parser functions to take a const char* parameter indicating the start of the value, and return a pointer to the end of the value (if parsing succeeds) or nullptr upon failure or partial parsing. Since I’m not great at explaining things, here’s the before: result parse_array(const_stream&amp;amp;); and here’s the after: const char* parse_array(const char*); This allows us to keep the pointer to the current position in the stream entirely within the registers when parsing a document. Since the value is local to the function, the compiler no longer needs to write it to the const_stream object at the top of the call stack (created within basic_parser::write_some), nor read it each time a nested value is parsed. This yields an 8% boost in performance across the board. More time was spent optimizing the SSE2 functions used for parsing unescaped strings and whitespace as well. Within count_whitespace, we were able to get rid of a _mm_cmpeq_epi8 (PCMPEQB) instruction by performing a bitwise or with 4 after testing for spaces, and then comparing the result with '\r', as the ASCII value of tab ('\t') only differs from that of the carriage return by the third least significant bit. This was something that clang was doing for us, but it’s nice to implement it for all other compilers. For count_unescaped (used to parse unescaped strings), we were able to again reduce the length of the hot path, this time a bit more significantly. Instead of checking for control characters by means of relational comparison, we can instead check for quotes and backslash first, and once that’s done, the _mm_min_epu8 (PMINUB) instruction can be used to set all control characters (0 - 31) to 31, and then test for equality. This brought our performance on the strings.json benchmark past the 8 GB/s mark from around 7.7 GB/s. Combined with the optimization of how the stream pointer is passed around, we now hit just a hair under 8.5 GB/s on this benchmark. The important but boring stuff After merging the parser extensions, there was a bunch of housekeeping to do such as improving coverage and writing documentation. Though these are far from being my favorite tasks, they are integral to writing a good library, so it must be done. My initial approach to writing tests for the parser extensions was to run each test on every parser configuration we have, but this soon proved to be a nonoptimal approach when the time taken to run the test suite quadrupled. I ended up doing the right thing by making the tests more surgical in nature, and in doing so we even got 100% coverage on the parser.</summary></entry><entry><title type="html">Richard’s July Update</title><link href="http://cppalliance.org/richard/2020/08/01/RichardsJulyUpdate.html" rel="alternate" type="text/html" title="Richard’s July Update" /><published>2020-08-01T00:00:00+00:00</published><updated>2020-08-01T00:00:00+00:00</updated><id>http://cppalliance.org/richard/2020/08/01/RichardsJulyUpdate</id><content type="html" xml:base="http://cppalliance.org/richard/2020/08/01/RichardsJulyUpdate.html">&lt;h1 id=&quot;boost-174---progress-update&quot;&gt;Boost 1.74 - Progress Update&lt;/h1&gt;

&lt;p&gt;Boost 1.74 beta release has been published and the various maintainers are applying last-minute bug fixes to their 
libraries in readiness for the final release on  12th August.&lt;/p&gt;

&lt;p&gt;For us in the Beast team, a fair amount of attention has been spent monitoring last minutes changes to Asio, as Chris
makes the final tweaks after the Unified Executors update I mentioned in last month’s blog.&lt;/p&gt;

&lt;h2 id=&quot;comprehensive-testing&quot;&gt;Comprehensive Testing&lt;/h2&gt;

&lt;p&gt;Last month I &lt;a href=&quot;https://github.com/boostorg/beast/commit/b84d8ad3d48d173bd78ed6dc2ed8d26d84762af3&quot;&gt;committed&lt;/a&gt; what I hoped 
would be the first of a suite of Dockerfiles which help the mass testing of Beast. The upstream changes to Asio
were a lesson in just how many compilers, hosts and target environments we have to support in order that our user base
is not surprised or impeded as a result of compiler selection or imposition.&lt;/p&gt;

&lt;p&gt;I am not expert is Docker matters. I mean, I can read the manual and follow basic instructions like anyone else,
but I was hoping that someone would come along to help flesh out the suite a little. Particularly for the Windows
builds, since I have no experience in installing software from the command line in Windows, and the greatest respect
for those individuals who have mastered the art.&lt;/p&gt;

&lt;p&gt;Fortunately for me, we’ve had a new addition to the team. Sam Darwin, who has submitted a number of commits which
increase Docker coverage. Of these I was most pleased to see the submission of the 
&lt;a href=&quot;https://github.com/boostorg/beast/commit/3486e9cb18aa39b392e07031a33e65b1792fbccf&quot;&gt;Windows&lt;/a&gt; build matrix which has 
been of enormous value. I think it would be fair to say that Microsoft Visual Studio is nothing short of notorious
for its subtle deviations from the standard. As if it were not difficult enough already to create useful and coherent 
template libraries, supporting (particularly older) versions of MSVC requires extra care and workarounds.&lt;/p&gt;

&lt;p&gt;Hopefully, now that two-phase lookup has been firmly 
&lt;a href=&quot;https://devblogs.microsoft.com/cppblog/two-phase-name-lookup-support-comes-to-msvc/&quot;&gt;adopted&lt;/a&gt; by Microsoft (some two 
decades after its standardisation), this kind of issue will become less of a concern as time moves forward and support 
for older compilers is gradually dropped.&lt;/p&gt;

&lt;p&gt;To be fair to Microsoft, if my memory serves, they were pioneers of bringing the C++ language to the masses back in the 
days of Visual Studio 97 and prior to that, the separate product Visual C++ (which we used to have to pay for!).&lt;/p&gt;

&lt;p&gt;In hindsight a number of errors were made in terms of implementation that had lasting effects on a generation of 
developers and their projects. But arguably, had Microsoft not championed this effort, it is likely that C++ may not
have achieved the penetration and exposure that it did.&lt;/p&gt;

&lt;h1 id=&quot;a-bug-in-asio-resolver-surely-not&quot;&gt;A Bug In Asio Resolver? Surely Not?&lt;/h1&gt;

&lt;p&gt;One of the examples in the Beast repository is a simple 
&lt;a href=&quot;https://github.com/boostorg/beast/tree/develop/example/http/client/crawl&quot;&gt;web crawler&lt;/a&gt;. If you have taken sufficient
interest to read the code, you will have noticed that it follows the model of “multiple threads, &lt;code&gt;one io_context&lt;/code&gt; per 
thread.”&lt;/p&gt;

&lt;p&gt;This may seem an odd decision, since a web crawler spends most of its time idle waiting for asynchronous IO to complete.
However, there is an unfortunate implementation detail in the *nix version of &lt;code&gt;asio::ip::tcp::resolver&lt;/code&gt; which is due to
a limitation of the &lt;a href=&quot;https://linux.die.net/man/3/getaddrinfo&quot;&gt;&lt;code&gt;getaddrinfo&lt;/code&gt;&lt;/a&gt; API upon which it depends.&lt;/p&gt;

&lt;p&gt;For background, &lt;code&gt;getaddrinfo&lt;/code&gt; is a thread-safe, blocking call. This means that Asio has to spawn a background thread
in order to perform the actual name resolution as part of the implementation of &lt;code&gt;async_resolve&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;So with that out of the way, why am I writing this?&lt;/p&gt;

&lt;p&gt;One of the bugs I tackled this month was that this demo, when run with multiple (say 500) threads, can be reliably made 
to hang indefinitely on my Fedora 32 system. At first, I assumed that either we or Asio had introduced a race condition.
However, after digging into the lockup it turned out to almost always lock up while resolving the FQDN &lt;code&gt;secureupload.eu&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Investigating further, it turns out that the nameserver response for this FQDN is too long to fit into a UDP packet.
This means that the DNS client on the Linux host is forced to revert to a TCP connection in order to receive the entire
record. This can be evidenced by using &lt;code&gt;nslookup&lt;/code&gt; on the command line:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ nslookup secureupload.eu
Server: 192.168.0.1                    &amp;lt;&amp;lt;-- address of my local nameserver
Address: 192.168.0.1#53

Non-authoritative answer:
Name: secureupload.eu
Address: 45.87.161.67

... many others ...

Name: secureupload.eu
Address: 45.76.235.58
;; Truncated, retrying in TCP mode.    &amp;lt;&amp;lt;-- indication that nslookup is switching to TCP
Name: secureupload.eu
Address: 2a04:5b82:3:209::2

... many others

Name: secureupload.eu
Address: 2a04:5b82:3:203::2
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Furthermore, whenever I checked the call stack, the thread in question was always stuck in the glibc function 
&lt;a href=&quot;https://code.woboq.org/userspace/glibc/resolv/res_send.c.html#send_vc&quot;&gt;&lt;code&gt;send_vc()&lt;/code&gt;&lt;/a&gt;, which is called by &lt;code&gt;getaddrinfo&lt;/code&gt; 
in response to the condition of a truncated UDP response.&lt;/p&gt;

&lt;p&gt;So despite my initial assumption that there must be a race in user code, the evidence was starting to point to something
interesting about this particular FQDN. Now I’ve been writing software for over three decades on and off and I’ve seen
a lot of bugs in code that the authors were adamant they they had not put there. We are as a rule, our own worst 
critics. So I was reluctant to believe that there could be a data-driven bug in glibc.&lt;/p&gt;

&lt;p&gt;Nevertheless, a scan of the redhat bug tracker by Chris turned up 
&lt;a href=&quot;https://bugzilla.redhat.com/show_bug.cgi?id=1429442&quot;&gt;this little nugget&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;It turns out that what was happening was that the TCP connection to the upstream name server had gone quiet or had 
dropped packets - presumably courtesy my cheap Huawei Home Gateway router which was being swamped by 500 simultaneous
 requests by the 500 threads I had assigned to the crawler. Because the glibc implementation does not implement
a timeout on the request, the failed reception of a response by the nameserver caused the call to &lt;code&gt;gethostinfo&lt;/code&gt; to hang
whenever this FQDN was being resolved.&lt;/p&gt;

&lt;p&gt;So it turns out that there is indeed a bug in glibc, which can affect any *nix (or cygwin) program that performs DNS
address resolution when the requested domain’s response is too long to fit in a UDP response message.&lt;/p&gt;

&lt;p&gt;Until this bug is fixed, I have learned a lesson and you have been warned.&lt;/p&gt;</content><author><name></name></author><category term="richard" /><summary type="html">Boost 1.74 - Progress Update Boost 1.74 beta release has been published and the various maintainers are applying last-minute bug fixes to their libraries in readiness for the final release on 12th August. For us in the Beast team, a fair amount of attention has been spent monitoring last minutes changes to Asio, as Chris makes the final tweaks after the Unified Executors update I mentioned in last month’s blog. Comprehensive Testing Last month I committed what I hoped would be the first of a suite of Dockerfiles which help the mass testing of Beast. The upstream changes to Asio were a lesson in just how many compilers, hosts and target environments we have to support in order that our user base is not surprised or impeded as a result of compiler selection or imposition. I am not expert is Docker matters. I mean, I can read the manual and follow basic instructions like anyone else, but I was hoping that someone would come along to help flesh out the suite a little. Particularly for the Windows builds, since I have no experience in installing software from the command line in Windows, and the greatest respect for those individuals who have mastered the art. Fortunately for me, we’ve had a new addition to the team. Sam Darwin, who has submitted a number of commits which increase Docker coverage. Of these I was most pleased to see the submission of the Windows build matrix which has been of enormous value. I think it would be fair to say that Microsoft Visual Studio is nothing short of notorious for its subtle deviations from the standard. As if it were not difficult enough already to create useful and coherent template libraries, supporting (particularly older) versions of MSVC requires extra care and workarounds. Hopefully, now that two-phase lookup has been firmly adopted by Microsoft (some two decades after its standardisation), this kind of issue will become less of a concern as time moves forward and support for older compilers is gradually dropped. To be fair to Microsoft, if my memory serves, they were pioneers of bringing the C++ language to the masses back in the days of Visual Studio 97 and prior to that, the separate product Visual C++ (which we used to have to pay for!). In hindsight a number of errors were made in terms of implementation that had lasting effects on a generation of developers and their projects. But arguably, had Microsoft not championed this effort, it is likely that C++ may not have achieved the penetration and exposure that it did. A Bug In Asio Resolver? Surely Not? One of the examples in the Beast repository is a simple web crawler. If you have taken sufficient interest to read the code, you will have noticed that it follows the model of “multiple threads, one io_context per thread.” This may seem an odd decision, since a web crawler spends most of its time idle waiting for asynchronous IO to complete. However, there is an unfortunate implementation detail in the *nix version of asio::ip::tcp::resolver which is due to a limitation of the getaddrinfo API upon which it depends. For background, getaddrinfo is a thread-safe, blocking call. This means that Asio has to spawn a background thread in order to perform the actual name resolution as part of the implementation of async_resolve. So with that out of the way, why am I writing this? One of the bugs I tackled this month was that this demo, when run with multiple (say 500) threads, can be reliably made to hang indefinitely on my Fedora 32 system. At first, I assumed that either we or Asio had introduced a race condition. However, after digging into the lockup it turned out to almost always lock up while resolving the FQDN secureupload.eu. Investigating further, it turns out that the nameserver response for this FQDN is too long to fit into a UDP packet. This means that the DNS client on the Linux host is forced to revert to a TCP connection in order to receive the entire record. This can be evidenced by using nslookup on the command line: $ nslookup secureupload.eu Server: 192.168.0.1 &amp;lt;&amp;lt;-- address of my local nameserver Address: 192.168.0.1#53 Non-authoritative answer: Name: secureupload.eu Address: 45.87.161.67 ... many others ... Name: secureupload.eu Address: 45.76.235.58 ;; Truncated, retrying in TCP mode. &amp;lt;&amp;lt;-- indication that nslookup is switching to TCP Name: secureupload.eu Address: 2a04:5b82:3:209::2 ... many others Name: secureupload.eu Address: 2a04:5b82:3:203::2 Furthermore, whenever I checked the call stack, the thread in question was always stuck in the glibc function send_vc(), which is called by getaddrinfo in response to the condition of a truncated UDP response. So despite my initial assumption that there must be a race in user code, the evidence was starting to point to something interesting about this particular FQDN. Now I’ve been writing software for over three decades on and off and I’ve seen a lot of bugs in code that the authors were adamant they they had not put there. We are as a rule, our own worst critics. So I was reluctant to believe that there could be a data-driven bug in glibc. Nevertheless, a scan of the redhat bug tracker by Chris turned up this little nugget. It turns out that what was happening was that the TCP connection to the upstream name server had gone quiet or had dropped packets - presumably courtesy my cheap Huawei Home Gateway router which was being swamped by 500 simultaneous requests by the 500 threads I had assigned to the crawler. Because the glibc implementation does not implement a timeout on the request, the failed reception of a response by the nameserver caused the call to gethostinfo to hang whenever this FQDN was being resolved. So it turns out that there is indeed a bug in glibc, which can affect any *nix (or cygwin) program that performs DNS address resolution when the requested domain’s response is too long to fit in a UDP response message. Until this bug is fixed, I have learned a lesson and you have been warned.</summary></entry><entry><title type="html">Richard’s May/June Update</title><link href="http://cppalliance.org/richard/2020/07/01/RichardsJuneUpdate.html" rel="alternate" type="text/html" title="Richard’s May/June Update" /><published>2020-07-01T00:00:00+00:00</published><updated>2020-07-01T00:00:00+00:00</updated><id>http://cppalliance.org/richard/2020/07/01/RichardsJuneUpdate</id><content type="html" xml:base="http://cppalliance.org/richard/2020/07/01/RichardsJuneUpdate.html">&lt;h1 id=&quot;boost-174---interesting-developments-in-asio&quot;&gt;Boost 1.74 - Interesting Developments in Asio&lt;/h1&gt;

&lt;p&gt;We’re currently beta-testing Boost 1.74, the lead-up to which has seen a flurry of activity in Asio, which has
impacted Beast.&lt;/p&gt;

&lt;p&gt;Recent versions of Asio have moved away from the idea of sequencing completion handlers directly on an &lt;code&gt;io_context&lt;/code&gt;
(which used to be called an &lt;code&gt;io_service&lt;/code&gt;) towards the execution of completion handlers by an Executor.&lt;/p&gt;

&lt;p&gt;The basic idea being that the executor is a lightweight handle to some execution context, which did what the &lt;code&gt;io_context&lt;/code&gt;
always used to do - schedule the execution of completion handlers.&lt;/p&gt;

&lt;p&gt;The changes to Asio have been tracking 
&lt;a href=&quot;http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2018/n4771.pdf&quot;&gt;The Networking TS&lt;/a&gt; which describes a concept
of Executor relevant to asynchronous IO.&lt;/p&gt;

&lt;p&gt;The &lt;a href=&quot;http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2019/p0443r11.html&quot;&gt;Unified Executors&lt;/a&gt; proposal unifies the 
concepts of io execution and the general concept of “a place to execute work” - a somewhat more generic idea than merely
an IO loop or thread pool. Work has been ongoing by the members of WG21 to produce an execution model that 
serves all parties’ needs.&lt;/p&gt;

&lt;p&gt;Courtesy of an incredible effort by Chris Kohlhoff, Latest Asio and Boost.Asio 1.74 has been updated to accommodate both 
models of executors, with the Unified Executors model being the default. It’s important to note that most users won’t 
notice the change in API this time around since by default the Asio in 1.74 also includes the 1.73 interface.&lt;/p&gt;

&lt;p&gt;There are a number of preprocessor macros that can be defined to change this default behaviour:&lt;/p&gt;

&lt;h2 id=&quot;boost_asio_no_ts_executors&quot;&gt;&lt;code&gt;BOOST_ASIO_NO_TS_EXECUTORS&lt;/code&gt;&lt;/h2&gt;

&lt;p&gt;Defining this macro disables the Networking TS executor model. The most immediate thing you’ll notice if you define this
macro is that for some executor &lt;code&gt;e&lt;/code&gt;, the expression &lt;code&gt;e.context()&lt;/code&gt; becomes invalid.&lt;/p&gt;

&lt;p&gt;In the Unified Executors world, this operation is expressed as a query against the executor:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-c++&quot;&gt;auto&amp;amp; ctx = asio::query(e, asio::execution::context);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The idea being that the execution context is a &lt;em&gt;property&lt;/em&gt; of an executor that can be &lt;em&gt;queried&lt;/em&gt; for.&lt;/p&gt;

&lt;p&gt;Another change which users are likely to notice when this macro is defined is that the &lt;code&gt;asio::executor_work_guard&amp;lt;&amp;gt;&lt;/code&gt; 
template corresponding &lt;code&gt;asio::make_work_guard&lt;/code&gt; function is no longer defined.&lt;/p&gt;

&lt;p&gt;You may well ask then, how we would prevent an underlying execution context from running out of work?&lt;/p&gt;

&lt;p&gt;In the Unified Executors world, we can think of Executors as an unbounded set of types with various properties
enabled or disabled. The idea is that the state of the properties define the behaviour of the interaction between the 
executor and its underlying context.&lt;/p&gt;

&lt;p&gt;In the new world, we don’t explicitly create a work guard which references the executor. We ‘simply’ create a new
executor which happens to have the property of ‘tracking work’ (i.e. this executor will in some way ensure that the 
underlying context has outstanding work until the executor’s lifetime ends).&lt;/p&gt;

&lt;p&gt;Again, given that &lt;code&gt;e&lt;/code&gt; is some executor, here’s how we spell this:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-c++&quot;&gt;auto tracked = asio::require(e, asio::execution::outstanding_work.tracked);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;After executing this statement, there are now two executors in play. The first, &lt;code&gt;e&lt;/code&gt; may or may not be “tracking work”
(ensuring that the underlying context does not stop), but &lt;code&gt;tracked&lt;/code&gt; certainly is.&lt;/p&gt;

&lt;p&gt;There is another way to spell this, more useful in a generic programming environment.&lt;/p&gt;

&lt;p&gt;Suppose you were writing generic code and you don’t know the type of the executor presented to you, or even what kind
of execution context it is associated with. However, you do know that &lt;em&gt;if&lt;/em&gt; the underlying context can stop if it runs
out of work, then we want to prevent it from doing so for the duration of some operation.&lt;/p&gt;

&lt;p&gt;In this case, we can’t use &lt;code&gt;require&lt;/code&gt; because this will fail to compile if the given executor does not support the 
&lt;code&gt;outstanding_work::tracked&lt;/code&gt; property. Therefore we would request (or more correctly, &lt;em&gt;prefer&lt;/em&gt;) the capability rather 
than require it:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-c++&quot;&gt;auto maybe_tracked = asio::prefer(e, asio::execution::outstanding_work.tracked);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We can now use &lt;code&gt;maybe_tracked&lt;/code&gt; as the executor for our operation, and it will “do the right thing” regarding the tracking
of work whatever the underlying type of execution context. It is important to note that it &lt;em&gt;is&lt;/em&gt; an executor, not merely
a guard object that contains an executor.&lt;/p&gt;

&lt;h3 id=&quot;post-dispatch-and-defer&quot;&gt;post, dispatch and defer&lt;/h3&gt;

&lt;p&gt;Another notable change in the Asio API when this macro is defined is that models of the Executor concept lose their
&lt;code&gt;post&lt;/code&gt;, &lt;code&gt;dispatch&lt;/code&gt; and &lt;code&gt;defer&lt;/code&gt; member functions.&lt;/p&gt;

&lt;p&gt;The free function versions still remain, so if you have code like this:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-c++&quot;&gt;e.dispatch([]{ /* something */ });
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;you will need to rewrite it as:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-c++&quot;&gt;asio::dispatch(e, []{ /* something */ });
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;or you can be more creative with the underlying property system:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-c++&quot;&gt;asio::execution::execute(
    asio::prefer(
        e, 
        asio::execution::blocking.possibly), 
    []{ /* something */ });
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Which is more-or-less what the implementation of &lt;code&gt;dispatch&lt;/code&gt; does under the covers. It’s actually a little more involved
than that since the completion token’s associated allocator has to be taken into account. There is a 
property for that too: &lt;code&gt;asio::execution::allocator&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;In summary, all previous Asio and Networking TS execution/completion scenarios are now handled by executing a handler
in some executor supporting a set of relevant properties.&lt;/p&gt;

&lt;h2 id=&quot;boost_asio_no_deprecated&quot;&gt;BOOST_ASIO_NO_DEPRECATED&lt;/h2&gt;

&lt;p&gt;Defining this macro will ensure that old asio-style invocation and allocation completion handler customisation
functions will no longer be used. The newer paradigm is to explicitly query or require execution properties at the 
time of scheduling a completion handler for invocation. If you don’t know what any of that means, you’d be in the 
majority and don’t need to worry about it.&lt;/p&gt;

&lt;h2 id=&quot;boost_asio_use_ts_executor_as_default&quot;&gt;BOOST_ASIO_USE_TS_EXECUTOR_AS_DEFAULT&lt;/h2&gt;

&lt;p&gt;As of Boost 1.74, Asio IO objects will be associated with the new &lt;code&gt;asio::any_io_executor&lt;/code&gt; rather than the previous
polymorphic &lt;code&gt;asio::executor&lt;/code&gt;. Defining this macro, undoes this change. It may be useful to you if you have written code
that depends on the use of &lt;code&gt;asio::executor&lt;/code&gt;.&lt;/p&gt;

&lt;h2 id=&quot;other-observations&quot;&gt;Other observations&lt;/h2&gt;

&lt;h3 id=&quot;strands-are-still-a-thing&quot;&gt;Strands are Still a Thing&lt;/h3&gt;

&lt;p&gt;Asio &lt;code&gt;strand&lt;/code&gt; objects still seem to occupy a twilight zone between executors and something other than executors.&lt;/p&gt;

&lt;p&gt;To be honest, when I first saw the property mechanism, I assumed that a strand would be “just another executor” with 
some “sequential execution” property enabled. This turns out not to be the case. A strand has its own distinct execution
context which manages the sequencing of completion handler invocations within it. The strand keeps a copy of the inner
executor, which is the one where the strand’s completion handlers will be invoked in turn.&lt;/p&gt;

&lt;p&gt;However, a strand models the Executor concept, so it also &lt;em&gt;is an&lt;/em&gt; executor.&lt;/p&gt;

&lt;h3 id=&quot;execute-looks-set-to-become-the-new-call&quot;&gt;execute() looks set to become the new call().&lt;/h3&gt;

&lt;p&gt;Reading the Unified Executors paper is an interesting, exciting or horrifying experience - depending on your view of
what you’d like C++ to be.&lt;/p&gt;

&lt;p&gt;My take from the paper, fleshed out a little with the experience of touching the implementation in Asio, is that in the 
new world, the programming thought process will go something like this,
imagine the following situation:&lt;/p&gt;

&lt;p&gt;“I need to execute this set of tasks,&lt;/p&gt;

&lt;p&gt;Ideally I’d like them to execute in parallel,&lt;/p&gt;

&lt;p&gt;I’d like to wait for them to be done”&lt;/p&gt;

&lt;p&gt;As I understand things, the idea behind unified executors is that I will be able to express these desires and mandates
by executing my work function(s) in some executor yielded by a series of calls to &lt;code&gt;prefer&lt;/code&gt; and &lt;code&gt;require&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Something like:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-c++&quot;&gt;    auto eparallel = prefer(e, bulk_guarantee.unsequenced); // prefer parallel execution
    auto eblock = require(eparallel, blocking.always);      // require blocking
    execute(eblock, task1, task2, task3, task...);          // blocking call which will execute in parallel if possible
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Proponents will no doubt think,&lt;/p&gt;

&lt;p&gt;“Great! Programming by expression of intent”.&lt;/p&gt;

&lt;p&gt;Detractors might say,&lt;/p&gt;

&lt;p&gt;“Ugh! Nondeterministic programs. How do I debug this when it goes wrong?”&lt;/p&gt;

&lt;p&gt;To be honest that this stage, I find myself in both camps. No doubt time will tell.&lt;/p&gt;

&lt;h1 id=&quot;adventures-in-b2-boost-build&quot;&gt;Adventures in B2 (Boost Build)&lt;/h1&gt;

&lt;p&gt;Because of the pressure of testing Beast with the new multi-faceted Asio, I wanted a way to bulk compile and test many
different variants of:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Compilers&lt;/li&gt;
  &lt;li&gt;Preprocessor macro definitions&lt;/li&gt;
  &lt;li&gt;C++ standards&lt;/li&gt;
  &lt;li&gt;etc.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;I was dimly aware that the Boost build tool, B2, was capable of doing this from one command-line invocation.&lt;/p&gt;

&lt;p&gt;It’s worth mentioning at this point that I have fairly recently discovered just how powerful B2 is. It’s a shame that
it has never been offered to the world in a neat package with some friendly conversation-style documentation, which
seems to be the norm these days.&lt;/p&gt;

&lt;p&gt;It can actually do anything CMake can do and more. For example, all of the above.&lt;/p&gt;

&lt;p&gt;My thanks to Peter Dimov for teaching me about the existence of B2 &lt;em&gt;features&lt;/em&gt; and how to use them.&lt;/p&gt;

&lt;p&gt;It turns out to be a simple 2-step process:&lt;/p&gt;

&lt;p&gt;First defined a &lt;code&gt;user-config.jam&lt;/code&gt; file to describe the feature and its settings:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-jam&quot;&gt;import feature ;

feature.feature asio.mode : dflt nodep nots ts nodep-nots nodep-ts : propagated composite ;
feature.compose &amp;lt;asio.mode&amp;gt;nodep : &amp;lt;define&amp;gt;&quot;BOOST_ASIO_NO_DEPRECATED&quot; ;
feature.compose &amp;lt;asio.mode&amp;gt;nots : &amp;lt;define&amp;gt;&quot;BOOST_ASIO_NO_TS_EXECUTORS&quot; ;
feature.compose &amp;lt;asio.mode&amp;gt;ts : &amp;lt;define&amp;gt;&quot;BOOST_ASIO_USE_TS_EXECUTOR_AS_DEFAULT&quot; ;
feature.compose &amp;lt;asio.mode&amp;gt;nodep-nots : &amp;lt;define&amp;gt;&quot;BOOST_ASIO_NO_DEPRECATED&quot; &amp;lt;define&amp;gt;&quot;BOOST_ASIO_NO_TS_EXECUTORS&quot; ;
feature.compose &amp;lt;asio.mode&amp;gt;nodep-ts : &amp;lt;define&amp;gt;&quot;BOOST_ASIO_NO_DEPRECATED&quot; &amp;lt;define&amp;gt;&quot;BOOST_ASIO_USE_TS_EXECUTOR_AS_DEFAULT&quot; ;

using clang :   : clang++ : &amp;lt;stdlib&amp;gt;&quot;libc++&quot; &amp;lt;cxxflags&amp;gt;&quot;-Wno-c99-extensions&quot; ;
using gcc :   : g++ : &amp;lt;cxxflags&amp;gt;&quot;-Wno-c99-extensions&quot; ;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Then ask b2 to do the rest:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;./b2 --user-config=./user-config.jam \
  toolset=clang,gcc \
  asio.mode=dflt,nodep,nots,ts,nodep-nots,nodep-ts \
  variant=release \
  cxxstd=2a,17,14,11 \
  -j`grep processor /proc/cpuinfo | wc -l` \
  libs/beast/test libs/beast/example
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This will compile all examples and run all tests in beast on a linux platform for the cross-product of:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;clang and gcc&lt;/li&gt;
  &lt;li&gt;all 6 of the legal combinations of the preprocessor macros BOOST_ASIO_NO_DEPRECATED, BOOST_ASIO_NO_TS_EXECUTORS and 
BOOST_ASIO_USE_TS_EXECUTOR_AS_DEFAULT&lt;/li&gt;
  &lt;li&gt;C++ standards 2a, 17, 14 and 11&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;So that’s 48 separate scenarios.&lt;/p&gt;

&lt;p&gt;It will also:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Build any dependencies.&lt;/li&gt;
  &lt;li&gt;Build each scenario into its own separately named path in the bin.v2 directory.&lt;/li&gt;
  &lt;li&gt;Understand which tests passed and failed so that passing tests are not re-run on subsequent calls to b2 unless a
dependent file has changed.&lt;/li&gt;
  &lt;li&gt;Use as many CPUs as are available on the host (in my case, fortunately that’s 48, otherwise this would take a long time
to run)&lt;/li&gt;
&lt;/ul&gt;</content><author><name></name></author><category term="richard" /><summary type="html">Boost 1.74 - Interesting Developments in Asio We’re currently beta-testing Boost 1.74, the lead-up to which has seen a flurry of activity in Asio, which has impacted Beast. Recent versions of Asio have moved away from the idea of sequencing completion handlers directly on an io_context (which used to be called an io_service) towards the execution of completion handlers by an Executor. The basic idea being that the executor is a lightweight handle to some execution context, which did what the io_context always used to do - schedule the execution of completion handlers. The changes to Asio have been tracking The Networking TS which describes a concept of Executor relevant to asynchronous IO. The Unified Executors proposal unifies the concepts of io execution and the general concept of “a place to execute work” - a somewhat more generic idea than merely an IO loop or thread pool. Work has been ongoing by the members of WG21 to produce an execution model that serves all parties’ needs. Courtesy of an incredible effort by Chris Kohlhoff, Latest Asio and Boost.Asio 1.74 has been updated to accommodate both models of executors, with the Unified Executors model being the default. It’s important to note that most users won’t notice the change in API this time around since by default the Asio in 1.74 also includes the 1.73 interface. There are a number of preprocessor macros that can be defined to change this default behaviour: BOOST_ASIO_NO_TS_EXECUTORS Defining this macro disables the Networking TS executor model. The most immediate thing you’ll notice if you define this macro is that for some executor e, the expression e.context() becomes invalid. In the Unified Executors world, this operation is expressed as a query against the executor: auto&amp;amp; ctx = asio::query(e, asio::execution::context); The idea being that the execution context is a property of an executor that can be queried for. Another change which users are likely to notice when this macro is defined is that the asio::executor_work_guard&amp;lt;&amp;gt; template corresponding asio::make_work_guard function is no longer defined. You may well ask then, how we would prevent an underlying execution context from running out of work? In the Unified Executors world, we can think of Executors as an unbounded set of types with various properties enabled or disabled. The idea is that the state of the properties define the behaviour of the interaction between the executor and its underlying context. In the new world, we don’t explicitly create a work guard which references the executor. We ‘simply’ create a new executor which happens to have the property of ‘tracking work’ (i.e. this executor will in some way ensure that the underlying context has outstanding work until the executor’s lifetime ends). Again, given that e is some executor, here’s how we spell this: auto tracked = asio::require(e, asio::execution::outstanding_work.tracked); After executing this statement, there are now two executors in play. The first, e may or may not be “tracking work” (ensuring that the underlying context does not stop), but tracked certainly is. There is another way to spell this, more useful in a generic programming environment. Suppose you were writing generic code and you don’t know the type of the executor presented to you, or even what kind of execution context it is associated with. However, you do know that if the underlying context can stop if it runs out of work, then we want to prevent it from doing so for the duration of some operation. In this case, we can’t use require because this will fail to compile if the given executor does not support the outstanding_work::tracked property. Therefore we would request (or more correctly, prefer) the capability rather than require it: auto maybe_tracked = asio::prefer(e, asio::execution::outstanding_work.tracked); We can now use maybe_tracked as the executor for our operation, and it will “do the right thing” regarding the tracking of work whatever the underlying type of execution context. It is important to note that it is an executor, not merely a guard object that contains an executor. post, dispatch and defer Another notable change in the Asio API when this macro is defined is that models of the Executor concept lose their post, dispatch and defer member functions. The free function versions still remain, so if you have code like this: e.dispatch([]{ /* something */ }); you will need to rewrite it as: asio::dispatch(e, []{ /* something */ }); or you can be more creative with the underlying property system: asio::execution::execute( asio::prefer( e, asio::execution::blocking.possibly), []{ /* something */ }); Which is more-or-less what the implementation of dispatch does under the covers. It’s actually a little more involved than that since the completion token’s associated allocator has to be taken into account. There is a property for that too: asio::execution::allocator. In summary, all previous Asio and Networking TS execution/completion scenarios are now handled by executing a handler in some executor supporting a set of relevant properties. BOOST_ASIO_NO_DEPRECATED Defining this macro will ensure that old asio-style invocation and allocation completion handler customisation functions will no longer be used. The newer paradigm is to explicitly query or require execution properties at the time of scheduling a completion handler for invocation. If you don’t know what any of that means, you’d be in the majority and don’t need to worry about it. BOOST_ASIO_USE_TS_EXECUTOR_AS_DEFAULT As of Boost 1.74, Asio IO objects will be associated with the new asio::any_io_executor rather than the previous polymorphic asio::executor. Defining this macro, undoes this change. It may be useful to you if you have written code that depends on the use of asio::executor. Other observations Strands are Still a Thing Asio strand objects still seem to occupy a twilight zone between executors and something other than executors. To be honest, when I first saw the property mechanism, I assumed that a strand would be “just another executor” with some “sequential execution” property enabled. This turns out not to be the case. A strand has its own distinct execution context which manages the sequencing of completion handler invocations within it. The strand keeps a copy of the inner executor, which is the one where the strand’s completion handlers will be invoked in turn. However, a strand models the Executor concept, so it also is an executor. execute() looks set to become the new call(). Reading the Unified Executors paper is an interesting, exciting or horrifying experience - depending on your view of what you’d like C++ to be. My take from the paper, fleshed out a little with the experience of touching the implementation in Asio, is that in the new world, the programming thought process will go something like this, imagine the following situation: “I need to execute this set of tasks, Ideally I’d like them to execute in parallel, I’d like to wait for them to be done” As I understand things, the idea behind unified executors is that I will be able to express these desires and mandates by executing my work function(s) in some executor yielded by a series of calls to prefer and require. Something like: auto eparallel = prefer(e, bulk_guarantee.unsequenced); // prefer parallel execution auto eblock = require(eparallel, blocking.always); // require blocking execute(eblock, task1, task2, task3, task...); // blocking call which will execute in parallel if possible Proponents will no doubt think, “Great! Programming by expression of intent”. Detractors might say, “Ugh! Nondeterministic programs. How do I debug this when it goes wrong?” To be honest that this stage, I find myself in both camps. No doubt time will tell. Adventures in B2 (Boost Build) Because of the pressure of testing Beast with the new multi-faceted Asio, I wanted a way to bulk compile and test many different variants of: Compilers Preprocessor macro definitions C++ standards etc. I was dimly aware that the Boost build tool, B2, was capable of doing this from one command-line invocation. It’s worth mentioning at this point that I have fairly recently discovered just how powerful B2 is. It’s a shame that it has never been offered to the world in a neat package with some friendly conversation-style documentation, which seems to be the norm these days. It can actually do anything CMake can do and more. For example, all of the above. My thanks to Peter Dimov for teaching me about the existence of B2 features and how to use them. It turns out to be a simple 2-step process: First defined a user-config.jam file to describe the feature and its settings: import feature ; feature.feature asio.mode : dflt nodep nots ts nodep-nots nodep-ts : propagated composite ; feature.compose &amp;lt;asio.mode&amp;gt;nodep : &amp;lt;define&amp;gt;&quot;BOOST_ASIO_NO_DEPRECATED&quot; ; feature.compose &amp;lt;asio.mode&amp;gt;nots : &amp;lt;define&amp;gt;&quot;BOOST_ASIO_NO_TS_EXECUTORS&quot; ; feature.compose &amp;lt;asio.mode&amp;gt;ts : &amp;lt;define&amp;gt;&quot;BOOST_ASIO_USE_TS_EXECUTOR_AS_DEFAULT&quot; ; feature.compose &amp;lt;asio.mode&amp;gt;nodep-nots : &amp;lt;define&amp;gt;&quot;BOOST_ASIO_NO_DEPRECATED&quot; &amp;lt;define&amp;gt;&quot;BOOST_ASIO_NO_TS_EXECUTORS&quot; ; feature.compose &amp;lt;asio.mode&amp;gt;nodep-ts : &amp;lt;define&amp;gt;&quot;BOOST_ASIO_NO_DEPRECATED&quot; &amp;lt;define&amp;gt;&quot;BOOST_ASIO_USE_TS_EXECUTOR_AS_DEFAULT&quot; ; using clang : : clang++ : &amp;lt;stdlib&amp;gt;&quot;libc++&quot; &amp;lt;cxxflags&amp;gt;&quot;-Wno-c99-extensions&quot; ; using gcc : : g++ : &amp;lt;cxxflags&amp;gt;&quot;-Wno-c99-extensions&quot; ; Then ask b2 to do the rest: ./b2 --user-config=./user-config.jam \ toolset=clang,gcc \ asio.mode=dflt,nodep,nots,ts,nodep-nots,nodep-ts \ variant=release \ cxxstd=2a,17,14,11 \ -j`grep processor /proc/cpuinfo | wc -l` \ libs/beast/test libs/beast/example This will compile all examples and run all tests in beast on a linux platform for the cross-product of: clang and gcc all 6 of the legal combinations of the preprocessor macros BOOST_ASIO_NO_DEPRECATED, BOOST_ASIO_NO_TS_EXECUTORS and BOOST_ASIO_USE_TS_EXECUTOR_AS_DEFAULT C++ standards 2a, 17, 14 and 11 So that’s 48 separate scenarios. It will also: Build any dependencies. Build each scenario into its own separately named path in the bin.v2 directory. Understand which tests passed and failed so that passing tests are not re-run on subsequent calls to b2 unless a dependent file has changed. Use as many CPUs as are available on the host (in my case, fortunately that’s 48, otherwise this would take a long time to run)</summary></entry><entry><title type="html">Krystian’s May &amp;amp; June Update</title><link href="http://cppalliance.org/krystian/2020/07/01/KrystiansMayJuneUpdate.html" rel="alternate" type="text/html" title="Krystian’s May &amp;amp; June Update" /><published>2020-07-01T00:00:00+00:00</published><updated>2020-07-01T00:00:00+00:00</updated><id>http://cppalliance.org/krystian/2020/07/01/KrystiansMayJuneUpdate</id><content type="html" xml:base="http://cppalliance.org/krystian/2020/07/01/KrystiansMayJuneUpdate.html">&lt;h1 id=&quot;overview&quot;&gt;Overview&lt;/h1&gt;

&lt;p&gt;I’ve been very busy these last two months getting Boost.JSON ready for release, hence the combined blog post. Now that things are winding down, I hopefully can get back the normal blog release schedule.&lt;/p&gt;

&lt;h1 id=&quot;boostjson&quot;&gt;Boost.JSON&lt;/h1&gt;

&lt;p&gt;Aside from a couple of personal projects, the vast majority of my time was spent getting Boost.JSON set for release. Breaking it down, this consisted of three main tasks: a &lt;code&gt;tag_invoke&lt;/code&gt; based &lt;code&gt;value&lt;/code&gt; conversion interface, parser optimizations, and support for extended JSON syntax.&lt;/p&gt;

&lt;h2 id=&quot;value-conversion&quot;&gt;Value Conversion&lt;/h2&gt;

&lt;p&gt;Our previous interface that allowed users to specify their own conversions to and from &lt;code&gt;value&lt;/code&gt; proved unsatisfactory, as it required too much boiler-plate when specifying conversions to and from non-class types (e.g. enumeration types). To remedy this, I was tasked with implementing an ADL solution based on &lt;code&gt;tag_invoke&lt;/code&gt; which greatly reduces the amount of boiler-plate and provides a single, straightforward way to implement a custom conversion. For example, consider the following class type:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-cpp&quot;&gt;struct customer
{
	std::string name;
	std::size_t balance;
};
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;To convert an object of type &lt;code&gt;customer&lt;/code&gt; to &lt;code&gt;value&lt;/code&gt;, all you need is to write an overload of &lt;code&gt;tag_invoke&lt;/code&gt;. This can be implemented as an inline &lt;code&gt;friend&lt;/code&gt; function within the class definition (thus making it visible to ADL but not unqualified lookup; see [[basic.lookup.argdep] p4.3]](http://eel.is/c++draft/basic.lookup.argdep#4.3)), or as a free function:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-cpp&quot;&gt;void tag_invoke(value_from_tag, value&amp;amp; jv, const customer&amp;amp; c)
{
	object&amp;amp; obj = jv.emplace_object();
	obj[&quot;name&quot;] = c.name;
	obj[&quot;balance&quot;] = c.balance;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Note that a reference to &lt;code&gt;value&lt;/code&gt; is passed to the function performing the conversion. This ensures that the &lt;code&gt;storage_ptr&lt;/code&gt; passed to the calling function (i.e. &lt;code&gt;value_from(T&amp;amp;&amp;amp;, storage_ptr)&lt;/code&gt;) is correctly propagated to the result.&lt;/p&gt;

&lt;p&gt;Conversions from &lt;code&gt;value&lt;/code&gt; to a type &lt;code&gt;T&lt;/code&gt; are specified in a similar fashion:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-cpp&quot;&gt;customer tag_invoke(value_to_tag&amp;lt;customer&amp;gt;, const value&amp;amp; jv)
{
	return customer{
		value_to&amp;lt;std::string&amp;gt;(jv.at(&quot;name&quot;])), 
		jv.at(&quot;balance&quot;).as_uint64() 
	};
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;In addition to user-provided &lt;code&gt;tag_invoke&lt;/code&gt; overloads, generic conversions are provided for container-like, map-like, and string-like types, with obvious results. In general, if your container works with a range-based for loop, it will work with &lt;code&gt;value_from&lt;/code&gt; and &lt;code&gt;value_to&lt;/code&gt; without you having to write anything.&lt;/p&gt;

&lt;h2 id=&quot;parser-optimizations&quot;&gt;Parser Optimizations&lt;/h2&gt;

&lt;p&gt;Optimizing the parser was a side-project turned obsession for me. While it’s often a painfully tedious process of trying an idea, running benchmarks, and being disappointed with the results, the few times that you get a performance increase makes it all worth it.&lt;/p&gt;

&lt;p&gt;To preface, Boost.JSON is unique in that it can parse incrementally (no other C++ libraries implement this). However, incremental parsing is considerably slower than parsing a JSON document in its entirety, as a stack must be maintained to track which function the parser should resume to once more data is available. In addition to this, the use cases for incremental parsing will often involve bottlenecks much more significant than the speed of the parser. With this in mind, Boost.JSON’s parser is optimized for non-incremental parsing of a valid JSON document. The remainder of this post will be written without consideration for incremental parsing.&lt;/p&gt;

&lt;p&gt;Most of the optimizations were branch eliminations, such as removing branches based on call site preconditions. These yield small performance gains, but once compounded we saw a performance increase of up to 7% on certain benchmarks. The biggest gain in this category came from removing a large switch statement in &lt;code&gt;parse_value&lt;/code&gt; in favor of a manually written jump table. Making this function branchless significantly increases performance as it’s the most called function when parsing. This also makes the function very compact, meaning it can be inlined almost everywhere.&lt;/p&gt;

&lt;p&gt;In addition to benchmark driven optimization, I also optimized based on codegen. Going into it I really had no idea what I was doing, but after staring at it for a long time and watching some videos I got the hang of it. I used this method to optimize &lt;code&gt;parse_array&lt;/code&gt; and &lt;code&gt;parse_object&lt;/code&gt;, aiming to get the most linear hot path possible, with the fewest number of jumps. It took a few hours, but I was able to reach my target. This was done by moving some branches around, removing the &lt;code&gt;local_const_stream&lt;/code&gt; variable, and adding some optimization hints to various branches. In addition to this, the &lt;code&gt;std::size_t&lt;/code&gt; parameter (representing the number of elements) was removed from the &lt;code&gt;on_array_end&lt;/code&gt; and &lt;code&gt;on_object_end&lt;/code&gt; handlers as it didn’t provide any useful information and is not used by &lt;code&gt;parser&lt;/code&gt;. This yielded a performance increase of up to 4% in certain cases.&lt;/p&gt;

&lt;p&gt;The last major optimization was &lt;a href=&quot;https://github.com/CPPAlliance/json/issues/115&quot;&gt;suggested&lt;/a&gt; by &lt;a href=&quot;https://github.com/joaquintides&quot;&gt;Joaquín M López Muñoz&lt;/a&gt;. In essence, integer division is a slow operation, so compilers have all sorts of ways to avoid it; one of which is doing multiplication instead. When dividing by a constant divisor, the compiler is able to convert this to multiplication by the reciprocal of the divisor, which can be up to 20 times faster. Where this is applicable in Boost.JSON is in the calculation used to get the index of the bucket for a &lt;code&gt;object&lt;/code&gt; key. The implementation was pretty straightforward, and it yielded up to a 10% increase in performance for &lt;code&gt;object&lt;/code&gt; heavy benchmarks – a remarkable gain from such a small change. Thank you Joaquín :)&lt;/p&gt;

&lt;h2 id=&quot;parser-extensions&quot;&gt;Parser Extensions&lt;/h2&gt;

&lt;p&gt;The last major thing I worked on for Boost.JSON was implementing support for extended JSON syntaxes. The two supported extensions are: - allowing C and C++ style comments to appear within whitespace, and&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;allowing trailing commas to appear after the last element of an array or object. 
This post isn’t quite in chronological order, but comment support was my introduction into working on the parser (a trial by fire). After a few naive attempts at implementation, the result was comment parsing that did not affect performance at all when not enabled (as it should) and only has a minor impact on performance when enabled. This was done by building off existing branches within &lt;code&gt;parse_array&lt;/code&gt; and &lt;code&gt;parse_object&lt;/code&gt; instead of checking for comments every time whitespace is being parsed. Allowing for trailing commas was done in much the same way. The larger takeaway from implementing these extensions was getting to know the internals of the parser much better, allowing me to implement the aforementioned optimizations, as well as more complex extensions in the future.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;If you want to get in touch with me, you can message me on the &lt;a href=&quot;http://slack.cpp.al/&quot;&gt;Cpplang slack&lt;/a&gt;, or &lt;a href=&quot;mailto:sdkrystian@gmail.com&quot;&gt;shoot me an email&lt;/a&gt;.&lt;/p&gt;</content><author><name></name></author><category term="krystian" /><summary type="html">Overview I’ve been very busy these last two months getting Boost.JSON ready for release, hence the combined blog post. Now that things are winding down, I hopefully can get back the normal blog release schedule. Boost.JSON Aside from a couple of personal projects, the vast majority of my time was spent getting Boost.JSON set for release. Breaking it down, this consisted of three main tasks: a tag_invoke based value conversion interface, parser optimizations, and support for extended JSON syntax. Value Conversion Our previous interface that allowed users to specify their own conversions to and from value proved unsatisfactory, as it required too much boiler-plate when specifying conversions to and from non-class types (e.g. enumeration types). To remedy this, I was tasked with implementing an ADL solution based on tag_invoke which greatly reduces the amount of boiler-plate and provides a single, straightforward way to implement a custom conversion. For example, consider the following class type: struct customer { std::string name; std::size_t balance; }; To convert an object of type customer to value, all you need is to write an overload of tag_invoke. This can be implemented as an inline friend function within the class definition (thus making it visible to ADL but not unqualified lookup; see [[basic.lookup.argdep] p4.3]](http://eel.is/c++draft/basic.lookup.argdep#4.3)), or as a free function: void tag_invoke(value_from_tag, value&amp;amp; jv, const customer&amp;amp; c) { object&amp;amp; obj = jv.emplace_object(); obj[&quot;name&quot;] = c.name; obj[&quot;balance&quot;] = c.balance; } Note that a reference to value is passed to the function performing the conversion. This ensures that the storage_ptr passed to the calling function (i.e. value_from(T&amp;amp;&amp;amp;, storage_ptr)) is correctly propagated to the result. Conversions from value to a type T are specified in a similar fashion: customer tag_invoke(value_to_tag&amp;lt;customer&amp;gt;, const value&amp;amp; jv) { return customer{ value_to&amp;lt;std::string&amp;gt;(jv.at(&quot;name&quot;])), jv.at(&quot;balance&quot;).as_uint64() }; } In addition to user-provided tag_invoke overloads, generic conversions are provided for container-like, map-like, and string-like types, with obvious results. In general, if your container works with a range-based for loop, it will work with value_from and value_to without you having to write anything. Parser Optimizations Optimizing the parser was a side-project turned obsession for me. While it’s often a painfully tedious process of trying an idea, running benchmarks, and being disappointed with the results, the few times that you get a performance increase makes it all worth it. To preface, Boost.JSON is unique in that it can parse incrementally (no other C++ libraries implement this). However, incremental parsing is considerably slower than parsing a JSON document in its entirety, as a stack must be maintained to track which function the parser should resume to once more data is available. In addition to this, the use cases for incremental parsing will often involve bottlenecks much more significant than the speed of the parser. With this in mind, Boost.JSON’s parser is optimized for non-incremental parsing of a valid JSON document. The remainder of this post will be written without consideration for incremental parsing. Most of the optimizations were branch eliminations, such as removing branches based on call site preconditions. These yield small performance gains, but once compounded we saw a performance increase of up to 7% on certain benchmarks. The biggest gain in this category came from removing a large switch statement in parse_value in favor of a manually written jump table. Making this function branchless significantly increases performance as it’s the most called function when parsing. This also makes the function very compact, meaning it can be inlined almost everywhere. In addition to benchmark driven optimization, I also optimized based on codegen. Going into it I really had no idea what I was doing, but after staring at it for a long time and watching some videos I got the hang of it. I used this method to optimize parse_array and parse_object, aiming to get the most linear hot path possible, with the fewest number of jumps. It took a few hours, but I was able to reach my target. This was done by moving some branches around, removing the local_const_stream variable, and adding some optimization hints to various branches. In addition to this, the std::size_t parameter (representing the number of elements) was removed from the on_array_end and on_object_end handlers as it didn’t provide any useful information and is not used by parser. This yielded a performance increase of up to 4% in certain cases. The last major optimization was suggested by Joaquín M López Muñoz. In essence, integer division is a slow operation, so compilers have all sorts of ways to avoid it; one of which is doing multiplication instead. When dividing by a constant divisor, the compiler is able to convert this to multiplication by the reciprocal of the divisor, which can be up to 20 times faster. Where this is applicable in Boost.JSON is in the calculation used to get the index of the bucket for a object key. The implementation was pretty straightforward, and it yielded up to a 10% increase in performance for object heavy benchmarks – a remarkable gain from such a small change. Thank you Joaquín :) Parser Extensions The last major thing I worked on for Boost.JSON was implementing support for extended JSON syntaxes. The two supported extensions are: - allowing C and C++ style comments to appear within whitespace, and allowing trailing commas to appear after the last element of an array or object. This post isn’t quite in chronological order, but comment support was my introduction into working on the parser (a trial by fire). After a few naive attempts at implementation, the result was comment parsing that did not affect performance at all when not enabled (as it should) and only has a minor impact on performance when enabled. This was done by building off existing branches within parse_array and parse_object instead of checking for comments every time whitespace is being parsed. Allowing for trailing commas was done in much the same way. The larger takeaway from implementing these extensions was getting to know the internals of the parser much better, allowing me to implement the aforementioned optimizations, as well as more complex extensions in the future. If you want to get in touch with me, you can message me on the Cpplang slack, or shoot me an email.</summary></entry><entry><title type="html">Automated Documentation Previews</title><link href="http://cppalliance.org/sam/2020/06/04/WebsitePreviews.html" rel="alternate" type="text/html" title="Automated Documentation Previews" /><published>2020-06-04T00:00:00+00:00</published><updated>2020-06-04T00:00:00+00:00</updated><id>http://cppalliance.org/sam/2020/06/04/WebsitePreviews</id><content type="html" xml:base="http://cppalliance.org/sam/2020/06/04/WebsitePreviews.html">&lt;h1 id=&quot;overview&quot;&gt;Overview&lt;/h1&gt;

&lt;p&gt;Greetings, and welcome to my first blog post at The C++ Alliance.&lt;/p&gt;

&lt;p&gt;I’ve recently begun working on an interesting project for the Alliance which might also have more widespread applicability. The same requirement could possibly apply to your organization as well.&lt;/p&gt;

&lt;p&gt;Consider an open-source project that has multiple contributors who are submitting changes via pull-requests in Github. You’d like to have assurances that a pull-request passes all tests before being merged. That is done with continuous integration solutions such as Travis or Circle-CI, which are quite popular and well-known. Similarly, if the submission is &lt;em&gt;documentation&lt;/em&gt;, you would like to be able to view the formatted output in it’s final published format so you can review the layout, the colors, and so on. What would be the best way to build and publish documentation from pull requests?&lt;/p&gt;

&lt;p&gt;Perhaps the first thought would be to include the functionality in Travis or Circle-CI. And that is certainly possible. However, in some cases there may be sensitive passwords, ssh keys, or other tokens in the configuration. Is it safe to allow random pull requests, from conceivably anyone on the whole internet, to trigger a Circle-CI build that contains authentication information? Let’s explore that question, and then present a possible alternative that should be more secure.&lt;/p&gt;

&lt;h1 id=&quot;security&quot;&gt;Security&lt;/h1&gt;

&lt;p&gt;In Circle-CI, you can choose to enable or disable jobs for Pull Requests. It’s clearly safer to leave them disabled, but if the goal is to run automatic tests, this feature must be turned on. Next, you may choose to enable or disable access to sensitive keys for Pull Requests. This sounds like a great feature that will allow the jobs to be run safely. You could build Pull Requests with limited authorization. But what if you’d like to include secret keys in the build, that are needed to publish the documentation to an external server which is going to host the resulting content. After building the docs, they must be transferred to wherever they will be hosted. That means you must either include the secret keys in plain text, or toggle the setting to enable sensitive keys in Circle-CI.&lt;/p&gt;

&lt;p&gt;Let’s briefly think about the latter option. If secret keys are enabled in Circle-CI, they are not outright published or visible to the end-user. The build system obfuscates them. The obfuscation is a good first step. Unfortunately, there’s a file called .circleci/config.yml in the project, which contains all the commands to be run by the build system. A pull request could modify that file so that it prints the secrets in clear text.&lt;/p&gt;

&lt;p&gt;What can be done?&lt;/p&gt;

&lt;p&gt;The answer - which is not overly difficult if you already have some experience - is to run an in-house build server such as Jenkins. This adds multiple layers of security:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Optionally, does &lt;em&gt;not&lt;/em&gt; publicly print the build output.&lt;/li&gt;
  &lt;li&gt;Optionally, does &lt;em&gt;not&lt;/em&gt; run based on a .circleci file or Jenkinsfile, so modifying the configuration file is not an avenue for external attacks.&lt;/li&gt;
  &lt;li&gt;For each build job, it will only include the minimal number of secret keys required for the current task, and nothing more.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;While the new system may not be impregnable, it’s a major improvement compared to the security issues with Circle-CI for this specific requirement.&lt;/p&gt;

&lt;h1 id=&quot;design&quot;&gt;Design&lt;/h1&gt;

&lt;p&gt;Here is a high level overview of how the system operates, before getting into further details.&lt;/p&gt;

&lt;p&gt;A jenkins server is installed.&lt;/p&gt;

&lt;p&gt;It builds the documentation jobs, and then copies the resulting files to AWS S3.&lt;/p&gt;

&lt;p&gt;The job posts a message in the GitHub pull request conversation with a hyperlink to the new docs.&lt;/p&gt;

&lt;p&gt;Each pull request will get it’s own separate “website”. There could be hundreds of versions being simultaneously hosted.&lt;/p&gt;

&lt;p&gt;An nginx proxy server which sits in front of S3 serves the documents with a consistent URL format, and allows multiple repositories to share the same S3 bucket.&lt;/p&gt;

&lt;p&gt;The resulting functionality can be seen in action. On this pull request &lt;a href=&quot;https://github.com/boostorg/beast/pull/1973&quot;&gt;https://github.com/boostorg/beast/pull/1973&lt;/a&gt; a message appears:&lt;/p&gt;

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;An automated preview of the documentation is available at &lt;a href=&quot;http://1973.beastdocs.prtest.cppalliance.org/libs/beast/doc/html/index.html&quot;&gt;http://1973.beastdocs.prtest.cppalliance.org/libs/beast/doc/html/index.html&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;The link takes you to the preview, and will be updated with each new commit to the pull request.&lt;/p&gt;

&lt;h1 id=&quot;more-details&quot;&gt;More Details&lt;/h1&gt;

&lt;p&gt;The Jenkins server polls each configured repository at a 5 minute interval, to see if a new pull request has been added. Alternatively, instead of polling, you may add a webhook in Github.&lt;/p&gt;

&lt;p&gt;Each repository corresponds to a separate Jenkins “project” on the server. A job checks out a copy of the submitted code, runs the specific steps necessary for that codebase, and uploads the resulting website to an AWS S3 bucket.&lt;/p&gt;

&lt;p&gt;The configuration leverages a few Jenkins plugins:&lt;br /&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;“GitHub Pull Request Builder” to launch jobs based on the existence of a new pull request.&lt;/li&gt;
  &lt;li&gt;“S3 Publisher Plugin” for copying files to S3.&lt;/li&gt;
  &lt;li&gt;“CloudBees Docker Custom Build Environment Plugin” to run the build inside an isolated docker container.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;One previews bucket is created in S3 such as s3://example-previews&lt;/p&gt;

&lt;p&gt;The file path in the S3 bucket is formatted to be “repository”/”PR #”. For example, the filepath of pull request #60 for the repo called “website” is s3://example-previews/website/60&lt;/p&gt;

&lt;p&gt;The web URL is generated by inverting this path, so “website/60” becomes “60.website”. The full URL has the format “60.website.prtest.example.com”. This translation is accomplished with an nginx reverse proxy, hosted on the same Jenkins server.&lt;/p&gt;

&lt;p&gt;nginx rule:&lt;br /&gt;
rewrite ^(.*)$ $backendserver/$repo/$pullrequest$1 break;&lt;/p&gt;

&lt;p&gt;A wildcard DNS entry sends the preview visitors to nginx:&lt;br /&gt;
*.prtest.example.com -&amp;gt; jenkins.example.com&lt;/p&gt;

&lt;h1 id=&quot;implementation&quot;&gt;Implementation&lt;/h1&gt;

&lt;p&gt;In this section, we will go over all the steps in detail, as a tutorial.&lt;/p&gt;

&lt;p&gt;In the following code sections,&lt;br /&gt;
Replace “example.com” with your domain.&lt;br /&gt;
Replace “website” with your repository name.&lt;br /&gt;
Replace “example-previews” with your S3 bucket name.&lt;/p&gt;

&lt;h3 id=&quot;general-server-setup&quot;&gt;General Server Setup&lt;/h3&gt;

&lt;p&gt;Install Jenkins - https://www.jenkins.io/doc/book/installing/&lt;/p&gt;

&lt;p&gt;Install SSL certificate for Jenkins (jenkins.example.com):&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;apt install certbot
certbot certonly
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Install nginx.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;apt install nginx
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Create a website, as follows:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;server {
    listen 80;
    listen [::]:80;
    server_name jenkins.example.com;
    location '/.well-known/acme-challenge' {
        default_type &quot;text/plain&quot;;
        root /var/www/letsencrypt;
    }
    location / {
         return 301 https://jenkins.example.com:8443$request_uri;
    }
}

server {
listen 8443 ssl default_server;
listen [::]:8443 ssl default_server;
ssl_certificate /etc/letsencrypt/live/jenkins.example.com/fullchain.pem;
ssl_certificate_key /etc/letsencrypt/live/jenkins.example.com/privkey.pem;
#include snippets/snakeoil.conf;
location / {
include /etc/nginx/proxy_params;
proxy_pass http://localhost:8080;
proxy_read_timeout 90s;
}
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Set the URL inside of Jenkins-&amp;gt;Manage Jenkins-&amp;gt;Configure System to be https://&lt;em&gt;url&lt;/em&gt; , replacing &lt;em&gt;url&lt;/em&gt; with the hostname such as jenkins.example.com.&lt;/p&gt;

&lt;p&gt;Install the plugin “GitHub pull requests builder”
Go to &lt;code&gt;Manage Jenkins&lt;/code&gt; -&amp;gt; &lt;code&gt;Configure System&lt;/code&gt; -&amp;gt; &lt;code&gt;GitHub pull requests builder&lt;/code&gt; section.&lt;/p&gt;

&lt;p&gt;Click “Create API Token”. Log into github.&lt;/p&gt;

&lt;p&gt;Update “Commit Status Build Triggered”, “Commit Status Build Start” to –none–
Create all three types of “Commit Status Build Result” with –none–&lt;/p&gt;

&lt;p&gt;On the server:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;apt install git build-essential
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Install the plugin “CloudBees Docker Custom Build Environment”&lt;/p&gt;

&lt;p&gt;add Jenkins to docker group&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;usermod -a -G docker jenkins
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Restart jenkins.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;systemctl restart jenkins
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Install the “S3 publisher plugin”&lt;/p&gt;

&lt;p&gt;In Manage Jenkins-&amp;gt;Configure System, go to S3 Profiles, create profile. Assuming the IAM user in AWS is called “example-bot”, then create example-bot-profile with the AWS creds. The necessary IAM permissions are covered a bit further down in this document.&lt;/p&gt;

&lt;p&gt;Install the “Post Build Task plugin”&lt;/p&gt;

&lt;h3 id=&quot;nginx-setup&quot;&gt;Nginx Setup&lt;/h3&gt;

&lt;p&gt;Create a wildcard DNS entry at your DNS hosting provider:
*.prtest.website.example.com CNAME to jenkins.example.com&lt;/p&gt;

&lt;p&gt;Create an nginx site for previews:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;server {
    # Listen on port 80 for all IPs associated with your machine
    listen 80 default_server;

    # Catch all other server names
    server_name _;

    if ($host ~* ([0-9]+)\.(.*?)\.(.*)) {
        set $pullrequest $1;
        set $repo $2;
    }

    location / {
        set $backendserver 'http://example-previews.s3-website-us-east-1.amazonaws.com';

        #CUSTOMIZATIONS
        if ($repo = &quot;example&quot; ) {
          rewrite ^(.*)/something$ $1/something.html ;
        }

        #FINAL REWRITE
        rewrite ^(.*)$ $backendserver/$repo/$pullrequest$1 break;

        # The rewritten request is passed to S3
        proxy_pass http://example-previews.s3-website-us-east-1.amazonaws.com;
        #proxy_pass $backendserver;
        include /etc/nginx/proxy_params;
        proxy_redirect /$repo/$pullrequest / ;
    }
}

&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&quot;aws-setup&quot;&gt;AWS Setup&lt;/h3&gt;

&lt;p&gt;Turn on static web hosting on the bucket.
Endpoint is http://example-previews.s3-website-us-east-1.amazonaws.com&lt;/p&gt;

&lt;p&gt;Add bucket policy&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;{
    &quot;Version&quot;: &quot;2012-10-17&quot;,
    &quot;Statement&quot;: [
        {
            &quot;Sid&quot;: &quot;PublicReadGetObject&quot;,
            &quot;Effect&quot;: &quot;Allow&quot;,
            &quot;Principal&quot;: &quot;*&quot;,
            &quot;Action&quot;: &quot;s3:GetObject&quot;,
            &quot;Resource&quot;: &quot;arn:aws:s3:::example-previews/*&quot;
        }
    ]
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Create an IAM user and add these permissions&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;    &quot;Version&quot;: &quot;2012-10-17&quot;,
    &quot;Statement&quot;: [
        {
            &quot;Effect&quot;: &quot;Allow&quot;,
            &quot;Action&quot;: [
                &quot;s3:GetBucketLocation&quot;,
                &quot;s3:ListAllMyBuckets&quot;
            ],
            &quot;Resource&quot;: &quot;*&quot;
        },
        {
            &quot;Effect&quot;: &quot;Allow&quot;,
            &quot;Action&quot;: [
                &quot;s3:ListBucket&quot;
            ],
            &quot;Resource&quot;: [
                &quot;arn:aws:s3:::example-previews&quot;
            ]
        },
        {
            &quot;Effect&quot;: &quot;Allow&quot;,
            &quot;Action&quot;: [
                &quot;s3:PutObject&quot;,
                &quot;s3:GetObject&quot;,
                &quot;s3:DeleteObject&quot;
            ],
            &quot;Resource&quot;: [
                &quot;arn:aws:s3:::example-previews/*&quot;
            ]
        }
    ]
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&quot;jenkins-freestyle-projects&quot;&gt;JENKINS FREESTYLE PROJECTS&lt;/h3&gt;

&lt;p&gt;Create a new Freestyle Project&lt;/p&gt;

&lt;p&gt;Github Project (checked)
Project URL: https://github.com/yourorg/website/&lt;/p&gt;

&lt;p&gt;Source Code Management
Git (checked)
Repositories: https://github.com/yourorg/website
Credentials: github-example-bot (you should add a credential here, that successfully connects to github)
Advanced:
Refspec: +refs/pull/&lt;em&gt;:refs/remotes/origin/pr/&lt;/em&gt;
Branch Specifier: ${ghprbActualCommit}&lt;/p&gt;

&lt;p&gt;Build Triggers
GitHub Pull Request Builder (checked)
GitHub API Credentials: mybot&lt;/p&gt;

&lt;p&gt;#Consider whether to enable the following setting.
#It is optional. You may also approve each PR.
Advanced:
Build every pull request automatically without asking.&lt;/p&gt;

&lt;p&gt;Trigger Setup:
Build Status Message:
&lt;code&gt;An automated preview of this PR is available at [http://$ghprbPullId.website.prtest.example.com](http://$ghprbPullId.website.prtest.example.com)&lt;/code&gt;
Update Commit Message during build:
Commit Status Build Triggered: –none–
Commit Status Build Started: –none–
Commit Status Build Result: create all types of result, with message –none–&lt;/p&gt;

&lt;p&gt;Build Environment:
Build inside a Docker container (checked)
#Note: choose a Docker image that is appropriate for your project
Pull docker image from repository: circleci/ruby:2.4-node-browsers-legacy&lt;/p&gt;

&lt;p&gt;Build:
Execute Shell:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;#Note: whichever build steps your site requires.
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Post-build Actions
Publish artifacts to S3
S3 Profile: example-bot-profile&lt;/p&gt;

&lt;p&gt;Source: _site/** (set this value as necessary for your code)
Destination:  example-previews/example/${ghprbPullId}
Bucket Region: us-east-1
No upload on build failure (checked)&lt;/p&gt;

&lt;p&gt;#The following part is optional. It will post an alert into a Slack channel.
Add Post Build Tasks&lt;/p&gt;

&lt;p&gt;Log Text: GitHub&lt;/p&gt;

&lt;p&gt;Script:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;#!/bin/bash
PREVIEWMESSAGE=&quot;A preview of the example website is available at http://$ghprbPullId.example.prtest.example.com&quot;
curl -X POST -H 'Content-type: application/json' --data &quot;{\&quot;text\&quot;:\&quot;$PREVIEWMESSAGE\&quot;}&quot;  https://hooks.slack.com/services/T21Q22/B0141JT/aPF___
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Check box “Run script only if all previous steps were successful”&lt;/p&gt;

&lt;p&gt;In Slack administration, (not in jenkins), create a Slack app. Create a “webhook” for your channel. That webhook goes into the curl command.&lt;/p&gt;</content><author><name></name></author><category term="sam" /><summary type="html">Overview Greetings, and welcome to my first blog post at The C++ Alliance. I’ve recently begun working on an interesting project for the Alliance which might also have more widespread applicability. The same requirement could possibly apply to your organization as well. Consider an open-source project that has multiple contributors who are submitting changes via pull-requests in Github. You’d like to have assurances that a pull-request passes all tests before being merged. That is done with continuous integration solutions such as Travis or Circle-CI, which are quite popular and well-known. Similarly, if the submission is documentation, you would like to be able to view the formatted output in it’s final published format so you can review the layout, the colors, and so on. What would be the best way to build and publish documentation from pull requests? Perhaps the first thought would be to include the functionality in Travis or Circle-CI. And that is certainly possible. However, in some cases there may be sensitive passwords, ssh keys, or other tokens in the configuration. Is it safe to allow random pull requests, from conceivably anyone on the whole internet, to trigger a Circle-CI build that contains authentication information? Let’s explore that question, and then present a possible alternative that should be more secure. Security In Circle-CI, you can choose to enable or disable jobs for Pull Requests. It’s clearly safer to leave them disabled, but if the goal is to run automatic tests, this feature must be turned on. Next, you may choose to enable or disable access to sensitive keys for Pull Requests. This sounds like a great feature that will allow the jobs to be run safely. You could build Pull Requests with limited authorization. But what if you’d like to include secret keys in the build, that are needed to publish the documentation to an external server which is going to host the resulting content. After building the docs, they must be transferred to wherever they will be hosted. That means you must either include the secret keys in plain text, or toggle the setting to enable sensitive keys in Circle-CI. Let’s briefly think about the latter option. If secret keys are enabled in Circle-CI, they are not outright published or visible to the end-user. The build system obfuscates them. The obfuscation is a good first step. Unfortunately, there’s a file called .circleci/config.yml in the project, which contains all the commands to be run by the build system. A pull request could modify that file so that it prints the secrets in clear text. What can be done? The answer - which is not overly difficult if you already have some experience - is to run an in-house build server such as Jenkins. This adds multiple layers of security: Optionally, does not publicly print the build output. Optionally, does not run based on a .circleci file or Jenkinsfile, so modifying the configuration file is not an avenue for external attacks. For each build job, it will only include the minimal number of secret keys required for the current task, and nothing more. While the new system may not be impregnable, it’s a major improvement compared to the security issues with Circle-CI for this specific requirement. Design Here is a high level overview of how the system operates, before getting into further details. A jenkins server is installed. It builds the documentation jobs, and then copies the resulting files to AWS S3. The job posts a message in the GitHub pull request conversation with a hyperlink to the new docs. Each pull request will get it’s own separate “website”. There could be hundreds of versions being simultaneously hosted. An nginx proxy server which sits in front of S3 serves the documents with a consistent URL format, and allows multiple repositories to share the same S3 bucket. The resulting functionality can be seen in action. On this pull request https://github.com/boostorg/beast/pull/1973 a message appears: An automated preview of the documentation is available at http://1973.beastdocs.prtest.cppalliance.org/libs/beast/doc/html/index.html The link takes you to the preview, and will be updated with each new commit to the pull request. More Details The Jenkins server polls each configured repository at a 5 minute interval, to see if a new pull request has been added. Alternatively, instead of polling, you may add a webhook in Github. Each repository corresponds to a separate Jenkins “project” on the server. A job checks out a copy of the submitted code, runs the specific steps necessary for that codebase, and uploads the resulting website to an AWS S3 bucket. The configuration leverages a few Jenkins plugins: “GitHub Pull Request Builder” to launch jobs based on the existence of a new pull request. “S3 Publisher Plugin” for copying files to S3. “CloudBees Docker Custom Build Environment Plugin” to run the build inside an isolated docker container. One previews bucket is created in S3 such as s3://example-previews The file path in the S3 bucket is formatted to be “repository”/”PR #”. For example, the filepath of pull request #60 for the repo called “website” is s3://example-previews/website/60 The web URL is generated by inverting this path, so “website/60” becomes “60.website”. The full URL has the format “60.website.prtest.example.com”. This translation is accomplished with an nginx reverse proxy, hosted on the same Jenkins server. nginx rule: rewrite ^(.*)$ $backendserver/$repo/$pullrequest$1 break; A wildcard DNS entry sends the preview visitors to nginx: *.prtest.example.com -&amp;gt; jenkins.example.com Implementation In this section, we will go over all the steps in detail, as a tutorial. In the following code sections, Replace “example.com” with your domain. Replace “website” with your repository name. Replace “example-previews” with your S3 bucket name. General Server Setup Install Jenkins - https://www.jenkins.io/doc/book/installing/ Install SSL certificate for Jenkins (jenkins.example.com): apt install certbot certbot certonly Install nginx. apt install nginx Create a website, as follows: server { listen 80; listen [::]:80; server_name jenkins.example.com; location '/.well-known/acme-challenge' { default_type &quot;text/plain&quot;; root /var/www/letsencrypt; } location / { return 301 https://jenkins.example.com:8443$request_uri; } } server { listen 8443 ssl default_server; listen [::]:8443 ssl default_server; ssl_certificate /etc/letsencrypt/live/jenkins.example.com/fullchain.pem; ssl_certificate_key /etc/letsencrypt/live/jenkins.example.com/privkey.pem; #include snippets/snakeoil.conf; location / { include /etc/nginx/proxy_params; proxy_pass http://localhost:8080; proxy_read_timeout 90s; } } Set the URL inside of Jenkins-&amp;gt;Manage Jenkins-&amp;gt;Configure System to be https://url , replacing url with the hostname such as jenkins.example.com. Install the plugin “GitHub pull requests builder” Go to Manage Jenkins -&amp;gt; Configure System -&amp;gt; GitHub pull requests builder section. Click “Create API Token”. Log into github. Update “Commit Status Build Triggered”, “Commit Status Build Start” to –none– Create all three types of “Commit Status Build Result” with –none– On the server: apt install git build-essential Install the plugin “CloudBees Docker Custom Build Environment” add Jenkins to docker group usermod -a -G docker jenkins Restart jenkins. systemctl restart jenkins Install the “S3 publisher plugin” In Manage Jenkins-&amp;gt;Configure System, go to S3 Profiles, create profile. Assuming the IAM user in AWS is called “example-bot”, then create example-bot-profile with the AWS creds. The necessary IAM permissions are covered a bit further down in this document. Install the “Post Build Task plugin” Nginx Setup Create a wildcard DNS entry at your DNS hosting provider: *.prtest.website.example.com CNAME to jenkins.example.com Create an nginx site for previews: server { # Listen on port 80 for all IPs associated with your machine listen 80 default_server; # Catch all other server names server_name _; if ($host ~* ([0-9]+)\.(.*?)\.(.*)) { set $pullrequest $1; set $repo $2; } location / { set $backendserver 'http://example-previews.s3-website-us-east-1.amazonaws.com'; #CUSTOMIZATIONS if ($repo = &quot;example&quot; ) { rewrite ^(.*)/something$ $1/something.html ; } #FINAL REWRITE rewrite ^(.*)$ $backendserver/$repo/$pullrequest$1 break; # The rewritten request is passed to S3 proxy_pass http://example-previews.s3-website-us-east-1.amazonaws.com; #proxy_pass $backendserver; include /etc/nginx/proxy_params; proxy_redirect /$repo/$pullrequest / ; } } AWS Setup Turn on static web hosting on the bucket. Endpoint is http://example-previews.s3-website-us-east-1.amazonaws.com Add bucket policy { &quot;Version&quot;: &quot;2012-10-17&quot;, &quot;Statement&quot;: [ { &quot;Sid&quot;: &quot;PublicReadGetObject&quot;, &quot;Effect&quot;: &quot;Allow&quot;, &quot;Principal&quot;: &quot;*&quot;, &quot;Action&quot;: &quot;s3:GetObject&quot;, &quot;Resource&quot;: &quot;arn:aws:s3:::example-previews/*&quot; } ] } Create an IAM user and add these permissions &quot;Version&quot;: &quot;2012-10-17&quot;, &quot;Statement&quot;: [ { &quot;Effect&quot;: &quot;Allow&quot;, &quot;Action&quot;: [ &quot;s3:GetBucketLocation&quot;, &quot;s3:ListAllMyBuckets&quot; ], &quot;Resource&quot;: &quot;*&quot; }, { &quot;Effect&quot;: &quot;Allow&quot;, &quot;Action&quot;: [ &quot;s3:ListBucket&quot; ], &quot;Resource&quot;: [ &quot;arn:aws:s3:::example-previews&quot; ] }, { &quot;Effect&quot;: &quot;Allow&quot;, &quot;Action&quot;: [ &quot;s3:PutObject&quot;, &quot;s3:GetObject&quot;, &quot;s3:DeleteObject&quot; ], &quot;Resource&quot;: [ &quot;arn:aws:s3:::example-previews/*&quot; ] } ] } JENKINS FREESTYLE PROJECTS Create a new Freestyle Project Github Project (checked) Project URL: https://github.com/yourorg/website/ Source Code Management Git (checked) Repositories: https://github.com/yourorg/website Credentials: github-example-bot (you should add a credential here, that successfully connects to github) Advanced: Refspec: +refs/pull/:refs/remotes/origin/pr/ Branch Specifier: ${ghprbActualCommit} Build Triggers GitHub Pull Request Builder (checked) GitHub API Credentials: mybot #Consider whether to enable the following setting. #It is optional. You may also approve each PR. Advanced: Build every pull request automatically without asking. Trigger Setup: Build Status Message: An automated preview of this PR is available at [http://$ghprbPullId.website.prtest.example.com](http://$ghprbPullId.website.prtest.example.com) Update Commit Message during build: Commit Status Build Triggered: –none– Commit Status Build Started: –none– Commit Status Build Result: create all types of result, with message –none– Build Environment: Build inside a Docker container (checked) #Note: choose a Docker image that is appropriate for your project Pull docker image from repository: circleci/ruby:2.4-node-browsers-legacy Build: Execute Shell: #Note: whichever build steps your site requires. Post-build Actions Publish artifacts to S3 S3 Profile: example-bot-profile Source: _site/** (set this value as necessary for your code) Destination: example-previews/example/${ghprbPullId} Bucket Region: us-east-1 No upload on build failure (checked) #The following part is optional. It will post an alert into a Slack channel. Add Post Build Tasks Log Text: GitHub Script: #!/bin/bash PREVIEWMESSAGE=&quot;A preview of the example website is available at http://$ghprbPullId.example.prtest.example.com&quot; curl -X POST -H 'Content-type: application/json' --data &quot;{\&quot;text\&quot;:\&quot;$PREVIEWMESSAGE\&quot;}&quot; https://hooks.slack.com/services/T21Q22/B0141JT/aPF___ Check box “Run script only if all previous steps were successful” In Slack administration, (not in jenkins), create a Slack app. Create a “webhook” for your channel. That webhook goes into the curl command.</summary></entry></feed>